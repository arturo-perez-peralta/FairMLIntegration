{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of multistage and binary processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the idea of multistage and logical processors.\n",
    "\n",
    "A multistage processor is a fairness processor that modifies several steps of the algorithm making process. In particular, we will investigate a hybrid approach in which we combine known processors that affect different stages of the machine learning pipeline.\n",
    "\n",
    "A logical processor is a tool used when dealing with multiple sensitive attributes or multilabel sensitive attributes which allows us to transform the prottected information into a binary variable for which many more fairness methods are available.\n",
    "\n",
    "This note\n",
    "\n",
    "1. \n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.AUTO_REUSE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# aif360\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "\n",
    "# Custom imports\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345\n",
      "424242\n",
      "777\n",
      "32768\n",
      "45234\n"
     ]
    }
   ],
   "source": [
    "seeds = [12345, 424242, 777, 32768, 45234]\n",
    "seed = 12345\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "\n",
    "#=========================================================================\n",
    "#                          SIMULATION DATASET\n",
    "#=========================================================================\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          One variable\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def simul1V(seed = 12345, N = 5000, p1 = 0.5, p2 = 0.5):\n",
    "    \"\"\"\n",
    "    Obtain a simulated dataset from the toy model for the case of one sensitive variable\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        N (int): number of individuals in the dataset.\n",
    "        p1 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the first sensitive variable.\n",
    "        p2 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the second sensitive variable.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the simulation.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the simulation.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the simulation.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create variables\n",
    "    vars = dict()\n",
    "\n",
    "    # Sensitive variables (drawn from a binomial distribution)\n",
    "    vars['sens1'] = np.random.binomial(n = 1, p = p1, size = N)\n",
    "    vars['sens2'] = np.random.binomial(n = 1, p = p2, size = N)\n",
    "\n",
    "    # v1, v2 (noisy measurements of the sensitive variables) and their sum\n",
    "    vars['v1'] = np.random.normal(loc = vars['sens1'], scale = 1.0, size = N)\n",
    "    vars['v2'] = np.random.normal(loc = vars['sens2'], scale = 1.0, size = N)\n",
    "    vars['mean'] = np.mean(vars['v1'] + vars['v2'])\n",
    "\n",
    "    # Noisy measurements of the sum of v1 and v2\n",
    "    vars['indirect'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "    vars['weight_response'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "\n",
    "    # Response variable\n",
    "    vars['response'] = vars['weight_response'] > 0.0\n",
    "\n",
    "    # Create the dataset with the correct variables\n",
    "    final_vars = ['sens1', 'sens2', 'indirect', 'response']\n",
    "    df = dict()\n",
    "    for name in final_vars:\n",
    "        df[name] = vars[name]\n",
    "    \n",
    "    # Transform the sensitive variables to boolean\n",
    "    df['sens1'] = df['sens1'] == 1\n",
    "    df['sens2'] = df['sens2'] == 1\n",
    "\n",
    "    # Create the dataset from the dictionary\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Convert to standard dataset\n",
    "    data = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['sens1'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = ['sens2']\n",
    "    )\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = data.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = data.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(data)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Two variables\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def simul2V(seed = 12345, operation = \"OR\", N = 5000, p1 = 0.6, p2 = 0.6):\n",
    "    \"\"\"\n",
    "    Obtain a simulated dataset from the toy model for the case of two sensitive variables\n",
    "    =====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        N (int): number of individuals in the dataset.\n",
    "        p1 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the first sensitive variable.\n",
    "        p2 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the second sensitive variable.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute.\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create variables\n",
    "    vars = dict()\n",
    "\n",
    "    # Sensitive variables (drawn from a binomial distribution)\n",
    "    vars['sens1'] = np.random.binomial(n = 1, p = p1, size = N)\n",
    "    vars['sens2'] = np.random.binomial(n = 1, p = p2, size = N)\n",
    "\n",
    "    # v1, v2 (noisy measurements of the sensitive variables) and their sum\n",
    "    vars['v1'] = np.random.normal(loc = vars['sens1'], scale = 1.0, size = N)\n",
    "    vars['v2'] = np.random.normal(loc = vars['sens2'], scale = 1.0, size = N)\n",
    "    vars['mean'] = np.mean(vars['v1'] + vars['v2'])\n",
    "\n",
    "    # Noisy measurements of the sum of v1 and v2\n",
    "    vars['indirect'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "    vars['weight_response'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "\n",
    "    # Response variable\n",
    "    vars['response'] = vars['weight_response'] > 0.0\n",
    "\n",
    "    # Create the dataset with the correct variables\n",
    "    final_vars = ['sens1', 'sens2', 'indirect', 'response']\n",
    "    df = dict()\n",
    "    for name in final_vars:\n",
    "        df[name] = vars[name]\n",
    "    \n",
    "    df['sens1'] = df['sens1'] == 1\n",
    "    df['sens2'] = df['sens2'] == 1\n",
    "\n",
    "    # Apply bitwise operation\n",
    "    if operation == 'OR':\n",
    "        df['prot_attr'] = np.logical_or(df['sens1'], df['sens2'])\n",
    "\n",
    "    elif operation == 'AND':\n",
    "        df['prot_attr'] = np.logical_and(df['sens1'], df['sens2'])\n",
    "\n",
    "    elif operation == 'XOR':\n",
    "        df['prot_attr'] = np.logical_xor(df['sens1'], df['sens2'])\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Convert to standard datasets\n",
    "    data_single = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['sens1'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    data = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['prot_attr'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = data.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    _, vt_single = data_single.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt_single.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = data.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(data)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single\n",
    "\n",
    "\n",
    "#=========================================================================\n",
    "#                          GERMAN DATASET\n",
    "#=========================================================================\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          One variable\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "def GermanDataset1V(seed = 12345):\n",
    "    \"\"\"\n",
    "    Read and preprocess the German dataset for the case of one sensitive variable\n",
    "    (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the German dataset.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the German dataset.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the German dataset.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Read the data\n",
    "    dataset_german = GermanDataset(\n",
    "            protected_attribute_names=['age'],            \n",
    "            privileged_classes=[lambda x: x >= 25],      \n",
    "            features_to_drop=['personal_status', 'sex'] \n",
    "        )\n",
    "        \n",
    "    # xgboost requires labels to start at zero\n",
    "    dataset_german.labels[dataset_german.labels.ravel() == 2] =  dataset_german.labels[dataset_german.labels.ravel() == 2] - 2\n",
    "    dataset_german.unfavorable_label = dataset_german.unfavorable_label - 2\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = dataset_german.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We obtain sensitive attribute\n",
    "    sensitive_attribute = dataset_german.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Two variables\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def GermanDataset2V(seed = 12345, operation = \"OR\"):\n",
    "    \"\"\"\n",
    "    Read and preprocess the German dataset for the case of two sensitive variables\n",
    "    (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Read the data\n",
    "    dataset = GermanDataset(\n",
    "        protected_attribute_names=['age'],            \n",
    "        privileged_classes=[lambda x: x >= 25],      \n",
    "        features_to_drop=['personal_status', 'sex'] \n",
    "    )\n",
    "\n",
    "    # load the german dataset and update the data with the OR sum of sex and age\n",
    "    dataset_german_upd = utils.update_german_dataset_from_multiple_protected_attributes(dataset, operation)\n",
    "\n",
    "    # change favorable/unfavorable labels to 1: good; 0: bad\n",
    "    dataset_german_upd.labels[dataset_german_upd.labels.ravel() == 2] =  dataset_german_upd.labels[dataset_german_upd.labels.ravel() == 2] - 2\n",
    "    dataset_german_upd.unfavorable_label = dataset_german_upd.unfavorable_label - 2\n",
    "\n",
    "    # For the single dataset as well\n",
    "    dataset.labels[dataset.labels.ravel() == 2] =  dataset.labels[dataset.labels.ravel() == 2] - 2\n",
    "    dataset.unfavorable_label = dataset.unfavorable_label - 2\n",
    "\n",
    "    # Train, val, test split\n",
    "    data_train, vt = dataset_german_upd.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We do the same on the single variable dataset\n",
    "    _, vt = dataset.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = dataset_german_upd.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german_upd)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single\n",
    "\n",
    "\n",
    "\n",
    "#=========================================================================\n",
    "#                          HOMECREDIT DATASET\n",
    "#=========================================================================\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Data handling\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "def LoadHomecredit(\n",
    "        seed: int = 12345,\n",
    "        sample_size: int = 5000\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Reads the homecredit dataset, obtains a sample and store it in the 'data/' folder\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        sample_size (int): size of the sample \n",
    "    Outputs:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # We set a seed    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # We download the data\n",
    "    homecredit = pd.read_csv('data/homecredit.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "    nrows = homecredit.shape[0]\n",
    "\n",
    "    # We sample the dataset to make it more maneagable\n",
    "    ssample = np.random.choice(nrows, size = sample_size, replace = False)\n",
    "    homecredit = homecredit.iloc[ssample, :]\n",
    "    homecredit = homecredit.reset_index(drop=True)\n",
    "    \n",
    "    # We store the homecredit dataset in the data folder\n",
    "    path = 'data/'\n",
    "    with open(path + 'homecredit.pickle', 'wb') as handle:\n",
    "        pickle.dump(homecredit, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "    \n",
    "def ReadHomecredit():\n",
    "    \"\"\"\n",
    "    Reads the sample from the homecredit dataset that we stored in the 'data/' folder\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        None\n",
    "    Outputs:\n",
    "        homecredit (pd.DataFrame): dataframe that contains a subsample from the homecredit dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    homecredit = pd.read_pickle('data/homecredit.pickle')\n",
    "    return homecredit\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          One variable\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Homecredit1V(seed = 12345):\n",
    "    \"\"\"\n",
    "    Read and preprocess the Homecredit dataset for the case of one sensitive variable\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        dataset_homecredit (pd.DataFrame): subsample of the homecredit dataset\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the Homecredit dataset.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the Homecredit dataset.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the Homecredit dataset.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Read the data\n",
    "    dataset_homecredit = ReadHomecredit()\n",
    "\n",
    "    # Make a copy of the dataset\n",
    "    homecredit = dataset_homecredit.copy(deep = True)\n",
    "\n",
    "    # Pre process\n",
    "    homecredit = utils.preprocess_homecredit(homecredit)\n",
    "\n",
    "    # Transform to standard dataset\n",
    "    dataset_homecredit_aif = utils.convert_to_standard_dataset(\n",
    "            df=homecredit,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute='AGE',\n",
    "            priviledged_classes=[lambda x: x >= 25],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "    \n",
    "    # Perform train, test, val split\n",
    "    data_train, vt = dataset_homecredit_aif.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = dataset_homecredit_aif.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_homecredit_aif)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Two variables\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Homecredit2V(seed = 12345, operation = \"OR\"):\n",
    "    \"\"\"\n",
    "    Read and preprocess the Homecredit dataset for the case of one sensitive variable\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        dataset_homecredit (pd.DataFrame): subsample of the homecredit dataset\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute.\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Read the data\n",
    "    dataset_homecredit = ReadHomecredit()\n",
    "\n",
    "    # Copy the dataset\n",
    "    homecredit = dataset_homecredit.copy(deep = True)\n",
    "\n",
    "    # Pre process the data\n",
    "    homecredit = utils.preprocess_homecredit_mult(homecredit, operation = operation)\n",
    "    homecredit_single = homecredit.copy(deep = True)\n",
    "    \n",
    "    # Transform both datasets to aif360 format\n",
    "    homecredit = utils.convert_to_standard_dataset(\n",
    "            df=homecredit,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute=['PROT_ATTR'],\n",
    "            priviledged_classes=[lambda x: x == 1],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    homecredit_single = utils.convert_to_standard_dataset(\n",
    "            df=homecredit_single,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute=['AGE'],\n",
    "            priviledged_classes=[lambda x: x >= 25],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = homecredit.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    _, vt_single = homecredit.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt_single.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = homecredit.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(homecredit)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos diccionarios\n",
    "methods = dict()\n",
    "\n",
    "# RRange of thresholds\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsNames = [\n",
    "    'logreg',\n",
    "    'xgboost'\n",
    "#    'adversarial',\n",
    "#    'metafair',\n",
    "#    'piremover'\n",
    "]\n",
    "\n",
    "modelsTrain = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'xgboost': XGBClassifier\n",
    "#    'adversarial': AdversarialDebiasing,\n",
    "#    'metafair': MetaFairClassifier,\n",
    "#    'piremover': PrejudiceRemover\n",
    "}\n",
    "\n",
    "modelsArgs = {\n",
    "    'logreg': {\n",
    "        'solver': 'liblinear',\n",
    "        'random_state': seed\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'eval_metric': 'error',\n",
    "        'eta':0.1,\n",
    "        'max_depth':6,\n",
    "        'subsample':0.8\n",
    "    }\n",
    "#    'adversarial': {\n",
    "#        'privileged_groups': privileged_groups,\n",
    "#        'unprivileged_groups': unprivileged_groups,\n",
    "#        'scope_name': 'debiased_classifier',\n",
    "#        'debias': True,\n",
    "#        'sess': tf.session(), # Mirar esto de la sesion\n",
    "#        'num_epochs': 80\n",
    "#    },\n",
    "#    'metafair_sr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'sr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "#    'metafair_fdr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'fdr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "#    'pir': {\n",
    "#        'sensitive_attr': sensitive_attribute,\n",
    "#        'eta': 50.0\n",
    "#    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtainPrelDataSingle():\n",
    "    \n",
    "    modelsNames = [\n",
    "        'logreg',\n",
    "        'xgboost'\n",
    "    ]\n",
    "\n",
    "    modelsTrain = {\n",
    "        'logreg': LogisticRegression,\n",
    "        'xgboost': XGBClassifier\n",
    "    }\n",
    "\n",
    "    modelsArgs = {\n",
    "        'logreg': {\n",
    "            'solver': 'liblinear',\n",
    "            'random_state': seed\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'eval_metric': 'error',\n",
    "            'eta':0.1,\n",
    "            'max_depth':6,\n",
    "            'subsample':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return modelsNames, modelsTrain, modelsArgs\n",
    "\n",
    "def ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups):\n",
    "    modelsNames = [\n",
    "        'logreg',\n",
    "        'xgboost',\n",
    "        'adversarial',\n",
    "        'metafair',\n",
    "        'pir'\n",
    "    ]\n",
    "\n",
    "\n",
    "    modelsBenchmark = [\n",
    "        'logreg',\n",
    "        'xgboost'\n",
    "    ]\n",
    "\n",
    "    modelsFair = [\n",
    "        'adversarial',\n",
    "        'metafair_sr',\n",
    "        'metafair_fdr',\n",
    "        'pir'\n",
    "    ]\n",
    "\n",
    "    modelsPre = [\n",
    "        prefix + '_' + model_name for prefix in ['RW', 'DI'] for model_name in modelsBenchmark\n",
    "    ]\n",
    "\n",
    "\n",
    "    modelsPost = modelsPre + modelsFair\n",
    "\n",
    "\n",
    "    modelsTrain = {\n",
    "        'logreg': LogisticRegression,\n",
    "        'xgboost': XGBClassifier,\n",
    "        'adversarial': AdversarialDebiasing,\n",
    "        'metafair': MetaFairClassifier,\n",
    "        'pir': PrejudiceRemover\n",
    "    }\n",
    "\n",
    "    modelsArgs = {\n",
    "        'logreg': {\n",
    "            'solver': 'liblinear',\n",
    "            'random_state': seed\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'eval_metric': 'error',\n",
    "            'eta':0.1,\n",
    "            'max_depth':6,\n",
    "            'subsample':0.8\n",
    "        },\n",
    "        'adversarial': {\n",
    "            'privileged_groups': privileged_groups,\n",
    "            'unprivileged_groups': unprivileged_groups,\n",
    "            'scope_name': 'debiased_classifier',\n",
    "            'debias': True,\n",
    "            'num_epochs': 80\n",
    "        },\n",
    "        'metafair': {\n",
    "            'tau': 0.8,\n",
    "            'sensitive_attr': sensitive_attribute,\n",
    "            'type': 'sr',\n",
    "            'seed': seed\n",
    "        },\n",
    "    #    'metafair_fdr': {\n",
    "    #        'tau': 0.8,\n",
    "    #        'sensitive_attribute': sensitive_attribute,\n",
    "    #        'type': 'fdr',\n",
    "    #        'seed': seed\n",
    "    #    },\n",
    "        'pir': {\n",
    "            'sensitive_attr': sensitive_attribute,\n",
    "            'eta': 50.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return modelsNames, modelsBenchmark, modelsPost, modelsTrain, modelsArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(val, test, method):\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep(\n",
    "        dataset=val,\n",
    "        model=methods[method],\n",
    "        thresh_arr=thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(\n",
    "        metrics_sweep[method],\n",
    "        measurement,\n",
    "        combination\n",
    "        )\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics(\n",
    "        dataset=test, \n",
    "        model=methods[method], \n",
    "        threshold=metrics_best_thresh_validate[method]['best_threshold'])\n",
    "    \n",
    "\n",
    "\n",
    "def results_mult(val, val_single, test, test_single, method):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep_mult(\n",
    "        dataset = val,\n",
    "        dataset_single = val_single,\n",
    "        model = methods[method],\n",
    "        thresh_arr = thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(metrics_sweep[method])\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics_mult(\n",
    "        dataset = test, \n",
    "        dataset_single = test_single,\n",
    "        model = methods[method], \n",
    "        threshold = metrics_best_thresh_validate[method]['best_threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BenchmarkLogistic(data_train, data_val, data_test):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global nvar\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'logreg'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'sample_weight': train.instance_weights}\n",
    "\n",
    "    # Introduce the model in the model dict\n",
    "    methods[model_name] = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel(), **fit_params)\n",
    "\n",
    "    # Obtain results\n",
    "    if nvar == 1:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "    elif nvar == 2:\n",
    "        global data_val_single\n",
    "        global data_test_single\n",
    "        val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "        results_mult(val, val_single, test, test_single, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BenchmarkXGB(data_train, data_val, data_test):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global nvar\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'xgboost'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "    # Assign the correct dict\n",
    "    methods[model_name] = XGBClassifier(**fit_params)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel())\n",
    "\n",
    "    # Obtain results\n",
    "    if nvar == 1:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "    elif nvar == 2:\n",
    "        global data_val_single\n",
    "        global data_test_single\n",
    "        val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "        results_mult(val, val_single, test, test_single, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True):\n",
    "    #Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"RW\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Call the processor\n",
    "    PreProcessor = Reweighing(\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "    # Transform the data\n",
    "    PreProcessor.fit(train)\n",
    "    trainRW = PreProcessor.transform(train)\n",
    "    valRW = PreProcessor.transform(test)\n",
    "    testRW = PreProcessor.transform(val)\n",
    "\n",
    "    # Train the model\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        modelsArgs[model]['sess'] = tf.Session()\n",
    "\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model in modelsBenchmark:\n",
    "        if model == 'logreg':\n",
    "            fit_params = {'sample_weight': trainRW.instance_weights}\n",
    "            methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel(), **fit_params)\n",
    "        else:\n",
    "            methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel())\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainRW)\n",
    "            \n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(valRW, testRW, model_name)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(valRW, val_single, testRW, test_single, model_name)\n",
    "\n",
    "    if model == 'adversarial':\n",
    "        modelsArgs[model]['sess'].close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True):\n",
    "    #Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"DI\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Initialize the processor\n",
    "    PreProcessor = DisparateImpactRemover(\n",
    "        repair_level=repair_level,\n",
    "        sensitive_attribute=sensitive_attribute\n",
    "    )\n",
    "    # Transform the data\n",
    "    PreProcessor.fit_transform(train)\n",
    "    trainDI = PreProcessor.fit_transform(train)\n",
    "    valDI = PreProcessor.fit_transform(val)\n",
    "    testDI = PreProcessor.fit_transform(test)\n",
    "\n",
    "    # Train the model\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        modelsArgs[model]['sess'] = tf.Session()\n",
    "\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model in modelsBenchmark:\n",
    "        if model == 'logreg':\n",
    "            fit_params = {'sample_weight': trainDI.instance_weights}\n",
    "            methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel(), **fit_params)\n",
    "        else:\n",
    "            methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel())\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainDI)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(valDI, testDI, model_name)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            global data_val_single\n",
    "            global data_test_single\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(valDI, val_single, testDI, test_single, model_name)\n",
    "\n",
    "    if model == 'adversarial':\n",
    "        modelsArgs[model]['sess'].close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality,  tau = 0.8, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # assign the correct name\n",
    "    model_name = \"metafair\"\n",
    "    model_name_quality = '{}_{}'.format(model_name, quality)\n",
    "\n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name_quality] = MetaFairClassifier(\n",
    "        tau=tau,\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        type=quality,\n",
    "        seed=seed\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_quality] = methods[model_name_quality].fit(train)\n",
    "\n",
    "    # Obtain scores\n",
    "    methods[model_name_quality].scores_train = methods[model_name_quality].predict(train).scores\n",
    "    methods[model_name_quality].scores_val = methods[model_name_quality].predict(val).scores\n",
    "    methods[model_name_quality].scores_test = methods[model_name_quality].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(val, test, model_name_quality)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            global data_val_single\n",
    "            global data_test_single\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(val, val_single, test, test_single, model_name_quality)\n",
    "\n",
    "\n",
    "\n",
    "def InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'pir'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name] = PrejudiceRemover(\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        eta=eta\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train)\n",
    "    \n",
    "    # Obtain scores\n",
    "    methods[model_name].scores_train = methods[model_name].predict(train).scores\n",
    "    methods[model_name].scores_val = methods[model_name].predict(val).scores\n",
    "    methods[model_name].scores_test = methods[model_name].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "\n",
    "\n",
    "def InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global sess\n",
    "    \n",
    "    # Assign the correct name\n",
    "    model_name = 'adversarial'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    #We train the model\n",
    "    methods[model_name] = AdversarialDebiasing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups,\n",
    "        scope_name = 'debiased_classifier',\n",
    "        debias=True,\n",
    "        sess=sess,\n",
    "        num_epochs=80\n",
    "    )    \n",
    "    methods[model_name].fit(train)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PosprocPlatt(data_train, data_val, data_test, privileged_groups, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "    \n",
    "    # Assign the correct name\n",
    "    fairness_method = '_Platt'\n",
    "\n",
    "    # Validation\n",
    "    #---------------\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy = True), data_val.copy(deepcopy = True), data_test.copy(deepcopy = True)\n",
    "\n",
    "    # Copy the predictions\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Platt Scaling:\n",
    "    #---------------\n",
    "    #1. Split training data on sensitive attribute\n",
    "    val_preds_priv, val_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = val_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    \n",
    "    #2. Copy validation data predictions\n",
    "    val_preds2 = val_preds.copy(deepcopy = True)\n",
    "    \n",
    "    #3. Make one model for each group\n",
    "    sensitive_groups_data = {'priv': [val_preds_priv, priv_indices],\n",
    "                             'unpriv': [val_preds_unpriv, unpriv_indices]}\n",
    "    for group, data_group_list in sensitive_groups_data.items():\n",
    "        # Assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "        # Initialize the model, store it in the dict\n",
    "        methods[model_name_group] = LogisticRegression()\n",
    "        # Train the model using the validation data divided by group\n",
    "        methods[ model_name_group ] = methods[model_name_group].fit(\n",
    "            data_group_list[0].scores,   # data_group_list[0] -> data_val_preds_priv or data_val_preds_unpriv\n",
    "            val.subset(data_group_list[1]).labels.ravel()\n",
    "        ) # data_group_list[1] -> priv_indices or unpriv_indices\n",
    "\n",
    "        # predict group probabilities, store in val_preds2\n",
    "        # Platt scores are given by the predictions of the posterior probabilities\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        val_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "   \n",
    "    # Evaluate the model in a range of values\n",
    "    thresh_sweep_platt = np.linspace(np.min(val_preds2.scores.ravel()),\n",
    "                                     np.max(val_preds2.scores.ravel()),\n",
    "                                     50)\n",
    "\n",
    "    # Obtain the metrics for the val set\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep_from_scores(\n",
    "            dataset_true = val,\n",
    "            dataset_preds = val_preds,\n",
    "            thresh_arr = thresh_sweep_platt\n",
    "        )\n",
    "\n",
    "    # Evaluate metrics and obtain the best thresh\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    # Test\n",
    "    #---------------\n",
    "\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Plat Scaling:\n",
    "    #---------------\n",
    "    \n",
    "    # 1. Divide test set using sensitive varaible's groups\n",
    "    test_preds_priv, test_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = test_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    # 2. Copy test data\n",
    "    if nvar == 1:\n",
    "        test_preds2 = test_preds.copy(deepcopy = True)\n",
    "    elif nvar == 2:\n",
    "        test_single = data_test.copy(deepcopy = True)\n",
    "        test_preds2 = data_test.copy(deepcopy = True)\n",
    "        test_single.scores = np.zeros_like(test_single.labels)\n",
    "\n",
    "    # 3. Predict for each group\n",
    "    sensitive_groups_data_test = {'priv': [test_preds_priv, priv_indices],\n",
    "                                  'unpriv': [test_preds_unpriv, unpriv_indices]}\n",
    "    \n",
    "\n",
    "    for group, data_group_list in sensitive_groups_data_test.items():    \n",
    "        # We assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "\n",
    "        # Predict in each group, store the result in data_val_preds2\n",
    "        # The probabilities are the Platt scores\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        test_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "\n",
    "\n",
    "    if nvar == 1:    \n",
    "        # Obtain metrics\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores(\n",
    "            dataset_true = test,\n",
    "            dataset_pred = test_preds2,\n",
    "            threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold']\n",
    "        )\n",
    "\n",
    "    elif nvar == 2:\n",
    "        # Obtain metrics\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores(\n",
    "            dataset_true = test_single,\n",
    "            dataset_pred = test_preds2,\n",
    "            threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold']\n",
    "        )\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate    \n",
    "    \n",
    "    # Assign the correct name\n",
    "    fairness_method = '_eqOdds' \n",
    "\n",
    "    # Copy the dataset\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the predictions of the base model\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Initialize the model and store the predictions\n",
    "    methods[model_name+fairness_method] = EqOddsPostprocessing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups, \n",
    "        seed = seed)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name+fairness_method] = methods[model_name+fairness_method].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true=val,\n",
    "        dataset_preds=val_preds,\n",
    "        model=methods[model_name+fairness_method],\n",
    "        thresh_arr=thresh_sweep,\n",
    "        scores_or_labels='labels'\n",
    "    )\n",
    "\n",
    "    # Evaluate the model for the best threshold\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    if nvar == 1:\n",
    "\n",
    "        # We use the best threshold to obtain predicitions for test\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            model=methods[model_name+fairness_method], \n",
    "            threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "            scores_or_labels='labels'\n",
    "        )\n",
    "\n",
    "    elif nvar == 2:\n",
    "\n",
    "        test_single = data_test_single.copy(deepcopy=True)\n",
    "        # We use the best threshold to obtain predicitions for test\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            dataset_true_single = test_single,\n",
    "            model=methods[model_name+fairness_method], \n",
    "            threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "            scores_or_labels='labels'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name, quality):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate\n",
    "\n",
    "     # Assign the correct name\n",
    "    fairness_method = '_eqOdds'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the model's predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name_metric = model_name + fairness_method + '_' + quality\n",
    "    \n",
    "    # Initialize the model \n",
    "    methods[model_name_metric] = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        cost_constraint=quality,\n",
    "        seed=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model for a range of thresholds\n",
    "    metrics_sweep[model_name_metric] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true = val,\n",
    "        dataset_preds = val_preds,\n",
    "        model = methods[model_name_metric],\n",
    "        thresh_arr = thresh_sweep,\n",
    "        scores_or_labels = 'scores'\n",
    "    )\n",
    "\n",
    "    # Evaluate in best thresh\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.describe_metrics(metrics_sweep[model_name_metric])\n",
    "\n",
    "    if nvar == 1:\n",
    "\n",
    "        # Using the best thresh, evaluate in test\n",
    "        metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            model=methods[model_name_metric], \n",
    "            threshold=metrics_best_thresh_validate[model_name_metric]['best_threshold'], \n",
    "            scores_or_labels='scores'\n",
    "        )\n",
    "\n",
    "    elif nvar == 2:\n",
    "        test_single = data_test_single.copy(deepcopy=True)\n",
    "\n",
    "        # We use the best threshold to obtain predicitions for test\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            dataset_true_single = test_single,\n",
    "            model=methods[model_name+fairness_method], \n",
    "            threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "            scores_or_labels='labels'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name, key_metric):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate\n",
    "    global fair_metrics_optrej\n",
    "\n",
    "    # Assign the correct name\n",
    "    fairness_method = '_RejOpt'\n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = RejectOptionClassification(\n",
    "        unprivileged_groups=unprivileged_groups, \n",
    "        privileged_groups=privileged_groups, \n",
    "        metric_name=fair_metrics_optrej[key_metric],\n",
    "        metric_lb=-0.01,\n",
    "        metric_ub=0.01\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "\n",
    "\n",
    "    if nvar == 1:\n",
    "        # Obtain best threshold in val\n",
    "        metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=val, \n",
    "            dataset_preds=val_preds, \n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)\n",
    "        \n",
    "        # Obtain it in test\n",
    "        metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=test, \n",
    "            dataset_preds=test_preds, \n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)\n",
    "        \n",
    "    elif nvar == 2:\n",
    "        val_single, test_single = data_val_single.copy(deepcopy=True), data_test_single.copy(deepcopy=True)\n",
    "        # Obtain best threshold in val\n",
    "        metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=val, \n",
    "            dataset_preds=val_preds,\n",
    "            dataset_true_single=val_single, \n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)\n",
    "        \n",
    "        # Obtain it in test\n",
    "        metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=test, \n",
    "            dataset_preds=test_preds, \n",
    "            dataset_true_single=val_single,\n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DI remover\n",
    "repair_level = 0.5                      \n",
    "dir_grid = {                           \n",
    "    'repair_level': [0.25, 0.5, 0.75]\n",
    "}                \n",
    "\n",
    "\n",
    "# MetaFair classifier\n",
    "quality_constraints_meta = ['sr', 'fdr']\n",
    "tau = 0.8   \n",
    "metafair_grid = {\n",
    "    'tau': [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Prejudice index regularizer\n",
    "pir_grid = {\n",
    "    'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "}\n",
    "\n",
    "# Adversarial learning\n",
    "pir_grid = {\n",
    "    'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "}\n",
    "\n",
    "# Equal odds\n",
    "# Quality constraints\n",
    "quality_constraints_eqodds = [\"weighted\", 'fnr', 'fpr']\n",
    "\n",
    "# Option rejection\n",
    "# Fairness metrics\n",
    "fair_metrics_optrej = {\n",
    "    'spd': \"Statistical parity difference\",\n",
    "    'aod': \"Average odds difference\",\n",
    "    'eod': \"Equal opportunity difference\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\utils.py:992: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['AGE'] = -df['DAYS_BIRTH'].astype('float') / 365\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 19599.070312; batch adversarial loss: 0.478175\n",
      "epoch 1; iter: 0; batch classifier loss: 7672.593262; batch adversarial loss: 0.637683\n",
      "epoch 2; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 3; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 4; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 5; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 6; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 7; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 8; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 9; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 10; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 11; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 12; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 13; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 14; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 15; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 16; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 17; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 18; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 19; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 20; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 21; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 22; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 23; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 24; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 25; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 26; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 27; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 28; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 29; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 30; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 31; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 32; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 33; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 34; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 35; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 36; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 37; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 38; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 39; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 40; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 41; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 42; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 43; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 44; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 45; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 46; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 47; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 48; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 49; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 50; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 51; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 52; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 53; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 54; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 55; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 56; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 57; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 58; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 59; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 60; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 61; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 62; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 63; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 64; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 65; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 66; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 67; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 68; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 69; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 70; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 71; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 72; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 73; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 74; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 75; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 76; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 77; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 78; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 79; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 50881.707031; batch adversarial loss: 0.271140\n",
      "epoch 1; iter: 0; batch classifier loss: 8102.904297; batch adversarial loss: 0.670772\n",
      "epoch 2; iter: 0; batch classifier loss: 2803.181396; batch adversarial loss: 0.626958\n",
      "epoch 3; iter: 0; batch classifier loss: 5260.230469; batch adversarial loss: 0.576814\n",
      "epoch 4; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 5; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 6; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 7; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 8; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 9; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 10; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 11; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 12; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 13; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 14; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 15; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 16; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 17; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 18; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 19; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 20; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 21; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 22; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 23; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 24; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 25; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 26; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 27; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 28; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 29; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 30; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 31; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 32; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 33; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 34; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 35; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 36; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 37; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 38; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 39; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 40; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 41; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 42; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 43; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 44; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 45; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 46; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 47; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 48; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 49; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 50; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 51; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 52; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 53; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 54; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 55; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 56; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 57; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 58; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 59; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 60; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 61; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 62; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 63; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 64; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 65; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 66; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 67; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 68; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 69; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 70; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 71; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 72; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 73; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 74; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 75; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 76; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 77; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 78; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 79; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 0; iter: 0; batch classifier loss: 10652.617188; batch adversarial loss: 0.480055\n",
      "epoch 1; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 2; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 3; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 4; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 5; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 6; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 7; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 8; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 9; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 10; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 11; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 12; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 13; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 14; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 15; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 16; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 17; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 18; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 19; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 20; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 21; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 22; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 23; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 24; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 25; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 26; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 27; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 28; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 29; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 30; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 31; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 32; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 33; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 34; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 35; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 36; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 37; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 38; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 39; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 40; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 41; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 42; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 43; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 44; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 45; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 46; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 47; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 48; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 49; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 50; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 51; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 52; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 53; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 54; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 55; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 56; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 57; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 58; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 59; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 60; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 61; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 62; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 63; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 64; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 65; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 66; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 67; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 68; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 69; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 70; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 71; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 72; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 73; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 74; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 75; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 76; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 77; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 78; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 79; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:653: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  out = random_state.multivariate_normal(mean, cov, size)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 25287.775391; batch adversarial loss: 0.708744\n",
      "epoch 1; iter: 0; batch classifier loss: 7809.575684; batch adversarial loss: 0.673612\n",
      "epoch 2; iter: 0; batch classifier loss: 5610.402832; batch adversarial loss: 0.668349\n",
      "epoch 3; iter: 0; batch classifier loss: 5290.039551; batch adversarial loss: 0.648790\n",
      "epoch 4; iter: 0; batch classifier loss: 1486.434692; batch adversarial loss: 0.648592\n",
      "epoch 5; iter: 0; batch classifier loss: 3229.215332; batch adversarial loss: 0.628925\n",
      "epoch 6; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 7; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 8; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 9; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 10; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 11; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 12; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 13; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 14; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 15; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 16; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 17; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 18; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 19; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 20; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 21; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 22; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 23; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 24; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 25; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 26; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 27; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 28; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 29; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 30; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 31; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 32; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 33; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 34; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 35; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 36; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 37; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 38; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 39; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 40; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 41; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 42; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 43; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 44; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 45; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 46; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 47; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 48; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 49; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 50; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 51; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 52; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 53; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 54; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 55; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 56; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 57; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 58; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 59; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 60; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 61; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 62; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 63; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 64; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 65; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 66; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 67; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 68; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 69; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 70; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 71; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 72; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 73; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 74; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 75; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 76; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 77; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 78; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n",
      "epoch 79; iter: 0; batch classifier loss: nan; batch adversarial loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 149\u001b[0m\n\u001b[0;32m    146\u001b[0m InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m modelsPost:\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mPosprocPlatt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m quality \u001b[38;5;129;01min\u001b[39;00m quality_constraints_eqodds:\n",
      "Cell \u001b[1;32mIn[11], line 41\u001b[0m, in \u001b[0;36mPosprocPlatt\u001b[1;34m(data_train, data_val, data_test, privileged_groups, model_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m methods[model_name_group] \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Train the model using the validation data divided by group\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m methods[ model_name_group ] \u001b[38;5;241m=\u001b[39m \u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name_group\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_group_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# data_group_list[0] -> data_val_preds_priv or data_val_preds_unpriv\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_group_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# data_group_list[1] -> priv_indices or unpriv_indices\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# predict group probabilities, store in val_preds2\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Platt scores are given by the predictions of the posterior probabilities\u001b[39;00m\n\u001b[0;32m     48\u001b[0m scores_group \u001b[38;5;241m=\u001b[39m methods[model_name_group]\u001b[38;5;241m.\u001b[39mpredict_proba(data_group_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mscores)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "seeds = [12345, 424242, 777, 32768, 45234]\n",
    "seeds = [12345]\n",
    "datasets = ['Simulation', 'German', 'Homecredit']\n",
    "datasets = ['Homecredit']\n",
    "nvars = ['1', '2']\n",
    "operations = ['OR', 'AND', 'XOR']\n",
    "cases = ['ind', 'com']\n",
    "\n",
    "loadDatasets = {\n",
    "    'Simulation1V': simul1V,\n",
    "    'Simulation2V': simul2V,\n",
    "    'German1V': GermanDataset1V,\n",
    "    'German2V': GermanDataset2V,\n",
    "    'Homecredit1V': Homecredit1V,\n",
    "    'Homecredit2V': Homecredit2V\n",
    "}\n",
    "\n",
    "measurement = 'bal_acc'\n",
    "combination = []\n",
    "\n",
    "resultsDict = dict()\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    i += 1\n",
    "    for data in datasets:\n",
    "        for nvar in nvars:\n",
    "\n",
    "            dataset = data + nvar + 'V'\n",
    "            \n",
    "            if nvar == '1':\n",
    "                # Arguments for the iteration\n",
    "                argumentsLoadData = {\n",
    "                    'seed': seed\n",
    "                }\n",
    "                nvar = 1\n",
    "\n",
    "                # Load data\n",
    "                data_train, data_val, data_test, \\\n",
    "                sensitive_attribute, privileged_groups, \\\n",
    "                unprivileged_groups = loadDatasets[dataset](**argumentsLoadData)\n",
    "\n",
    "                for case in cases:\n",
    "                    if case == 'ind': \n",
    "\n",
    "                        # Obtain benchmarks\n",
    "                        modelsNames, modelsTrain, modelsArgs = ObtainPrelDataSingle()\n",
    "                        modelsBenchmark = modelsNames\n",
    "\n",
    "\n",
    "                        # Initialize dicts\n",
    "                        methods = dict()\n",
    "\n",
    "                        # Range of thresholds to evaluate our models\n",
    "                        thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                        metrics_sweep = dict()\n",
    "\n",
    "                        # Store results from validation and test\n",
    "                        metrics_best_thresh_validate = dict()\n",
    "                        metrics_best_thresh_test = dict()\n",
    "\n",
    "                        # Benchmarks\n",
    "                        BenchmarkLogistic(data_train, data_val, data_test)\n",
    "                        BenchmarkXGB(data_train, data_val, data_test)\n",
    "                        \n",
    "                        # Pre processing\n",
    "                        for model in modelsNames:\n",
    "                            PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "                            PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "                        \n",
    "                        # In processing\n",
    "                        for quality in quality_constraints_meta:\n",
    "                            InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "                        InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "                        \n",
    "                        tf.compat.v1.reset_default_graph()\n",
    "                        sess = tf.compat.v1.Session()\n",
    "                        InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "                        sess.close()\n",
    "                        \n",
    "                        # Post processing\n",
    "                        for model in modelsNames:\n",
    "                            PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "                            PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "                            for quality in quality_constraints_eqodds:\n",
    "                                PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "                            for key_metric in fair_metrics_optrej:\n",
    "                                PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "                        file = dataset + '_' + case + '_' + str(i)\n",
    "                        \n",
    "                        resultsDict[file] = dict()\n",
    "                        resultsDict[file]['methods'] = methods\n",
    "                        resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                        resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "                        with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                            pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                            pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "                    elif case == 'com':\n",
    "                        # Obtain benchmarks and in proncessing models\n",
    "                        modelsNames, modelsBenchmark, modelsPost, \\\n",
    "                        modelsTrain, modelsArgs = ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups)\n",
    "\n",
    "                        # Initialize dicts\n",
    "                        methods = dict()\n",
    "\n",
    "                        # Range of thresholds to evaluate our models\n",
    "                        thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                        metrics_sweep = dict()\n",
    "\n",
    "                        # Store results from validation and test\n",
    "                        metrics_best_thresh_validate = dict()\n",
    "                        metrics_best_thresh_test = dict()\n",
    "\n",
    "                        # Benchmarks\n",
    "                        BenchmarkLogistic(data_train, data_val, data_test)\n",
    "                        BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "                        # Pre processing + In processing\n",
    "                        for model in modelsNames:\n",
    "                            if model == 'adversarial':\n",
    "                                tf.compat.v1.reset_default_graph()\n",
    "                                sess = tf.compat.v1.Session()\n",
    "                            PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "                            if model == 'adversarial':\n",
    "                                sess.close()\n",
    "                                tf.compat.v1.reset_default_graph()\n",
    "                                sess = tf.compat.v1.Session()\n",
    "                            PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "                            \n",
    "                            if model == 'adversarial':\n",
    "                                sess.close()\n",
    "\n",
    "                        # Pre/In processing + Post processing\n",
    "                        for quality in quality_constraints_meta:\n",
    "                            InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "                        InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "\n",
    "                        tf.compat.v1.reset_default_graph()\n",
    "                        sess = tf.compat.v1.Session()\n",
    "                        InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "                        for model in modelsPost:\n",
    "                            PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "                            PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "                            for quality in quality_constraints_eqodds:\n",
    "                                PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "                            for key_metric in fair_metrics_optrej:\n",
    "                                PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "                                \n",
    "                        sess.close()\n",
    "                        \n",
    "                        file = dataset + '_' + case + '_' + str(i)\n",
    "\n",
    "                        resultsDict[file] = dict()\n",
    "                        resultsDict[file]['methods'] = methods\n",
    "                        resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                        resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "                        \n",
    "\n",
    "                        with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                            pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                            pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            \n",
    "\n",
    "            elif nvar == '2':\n",
    "                for operation in operations:\n",
    "                        argumentsLoadData = {\n",
    "                            'seed': seed,\n",
    "                            'operation': operation\n",
    "                        }\n",
    "                        nvar = 2\n",
    "\n",
    "                        resultsDict[dataset + '_' + operation] = dict()\n",
    "\n",
    "                        data_train, data_val, data_test, \\\n",
    "                        sensitive_attribute, privileged_groups, unprivileged_groups, \\\n",
    "                        data_val_single, data_test_single = loadDatasets[dataset](**argumentsLoadData)\n",
    "            \n",
    "                        for case in cases:\n",
    "                            if case == 'ind': \n",
    "\n",
    "                                # Initialize dicts\n",
    "                                methods = dict()\n",
    "\n",
    "                                # Obtain benchmarks\n",
    "                                modelsNames, modelsTrain, modelsArgs = ObtainPrelDataSingle()\n",
    "                                modelsBenchmark = modelsNames\n",
    "\n",
    "                                # Range of thresholds to evaluate our models\n",
    "                                thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                                metrics_sweep = dict()\n",
    "\n",
    "                                # Store results from validation and test\n",
    "                                metrics_best_thresh_validate = dict()\n",
    "                                metrics_best_thresh_test = dict()\n",
    "\n",
    "                                # Benchmarks\n",
    "                                BenchmarkLogistic(data_train, data_val, data_test)\n",
    "                                BenchmarkXGB(data_train, data_val, data_test)\n",
    "                                \n",
    "                                # Pre processing\n",
    "                                for model in modelsNames:\n",
    "                                    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "                                    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "                                \n",
    "                                # In processing\n",
    "                                for quality in quality_constraints_meta:\n",
    "                                    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "                                InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "                                \n",
    "                                tf.compat.v1.reset_default_graph()\n",
    "                                sess = tf.compat.v1.Session()\n",
    "                                InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "                                sess.close()\n",
    "                                \n",
    "                                # Post processing\n",
    "                                for model in modelsNames:\n",
    "                                    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "                                    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "                                    for quality in quality_constraints_eqodds:\n",
    "                                        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "                                    for key_metric in fair_metrics_optrej:\n",
    "                                        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "                                file =  dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "\n",
    "                                resultsDict[file] = dict()\n",
    "                                resultsDict[file]['methods'] = methods\n",
    "                                resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                                resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "                                with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                                    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                                    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "                            elif case == 'com':\n",
    "                                # Obtain benchmarks and in proncessing models\n",
    "                                modelsNames, modelsBenchmark, modelsPost, \\\n",
    "                                modelsTrain, modelsArgs = ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups)\n",
    "\n",
    "                                # Initialize dicts\n",
    "                                methods = dict()\n",
    "\n",
    "                                # Range of thresholds to evaluate our models\n",
    "                                thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                                metrics_sweep = dict()\n",
    "\n",
    "                                # Store results from validation and test\n",
    "                                metrics_best_thresh_validate = dict()\n",
    "                                metrics_best_thresh_test = dict()\n",
    "\n",
    "                                # Benchmarks\n",
    "                                BenchmarkLogistic(data_train, data_val, data_test)\n",
    "                                BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "                                # Pre processing + In processing\n",
    "                                for model in modelsNames:\n",
    "                                    if model == 'adversarial':\n",
    "                                        tf.compat.v1.reset_default_graph()\n",
    "                                        sess = tf.compat.v1.Session()\n",
    "                                    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "                                    if model == 'adversarial':\n",
    "                                        sess.close()\n",
    "                                        tf.compat.v1.reset_default_graph()\n",
    "                                        sess = tf.compat.v1.Session()\n",
    "                                    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "                                    \n",
    "                                    if model == 'adversarial':\n",
    "                                        sess.close()\n",
    "\n",
    "                                # Pre/In processing + Post processing\n",
    "                                for quality in quality_constraints_meta:\n",
    "                                    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "                                InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "\n",
    "                                tf.compat.v1.reset_default_graph()\n",
    "                                sess = tf.compat.v1.Session()\n",
    "                                InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "                                for model in modelsPost:\n",
    "                                    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "                                    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "                                    for quality in quality_constraints_eqodds:\n",
    "                                        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "                                    for key_metric in fair_metrics_optrej:\n",
    "                                        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "                                        \n",
    "                                sess.close()\n",
    "\n",
    "                                file = dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "                                \n",
    "                                resultsDict[file] = dict()\n",
    "                                resultsDict[file]['methods'] = methods\n",
    "                                resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                                resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "                            \n",
    "                                with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                                    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                                    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_train\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for the iteration\n",
    "argumentsLoadData = {\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "nvar = 1\n",
    "\n",
    "datasets = ['Simulation', 'German', 'Homecredit']\n",
    "data = 'Homecredit'\n",
    "\n",
    "dataset = data + nvar + 'V'\n",
    "\n",
    "# Load data\n",
    "data_train, data_val, data_test, \\\n",
    "sensitive_attribute, privileged_groups, \\\n",
    "unprivileged_groups = loadDatasets[dataset](**argumentsLoadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'ind'\n",
    "\n",
    "# Obtain benchmarks\n",
    "modelsNames, modelsTrain, modelsArgs = ObtainPrelDataSingle()\n",
    "modelsBenchmark = modelsNames\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "# In processing\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "sess.close()\n",
    "\n",
    "# Post processing\n",
    "for model in modelsNames:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "file = dataset + '_' + case + '_' + str(i)\n",
    "\n",
    "resultsDict[file] = dict()\n",
    "resultsDict[file]['methods'] = methods\n",
    "resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "\n",
    "with open('results/' + file + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'com'\n",
    "\n",
    "# Obtain benchmarks and in proncessing models\n",
    "modelsNames, modelsBenchmark, modelsPost, \\\n",
    "modelsTrain, modelsArgs = ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups)\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing + In processing\n",
    "for model in modelsNames:\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    if model == 'adversarial':\n",
    "        sess.close()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "    \n",
    "    if model == 'adversarial':\n",
    "        sess.close()\n",
    "\n",
    "# Pre/In processing + Post processing\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "for model in modelsPost:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "        \n",
    "sess.close()\n",
    "\n",
    "# Save the file\n",
    "file = dataset + '_' + case + '_' + str(i)\n",
    "\n",
    "resultsDict[file] = dict()\n",
    "resultsDict[file]['methods'] = methods\n",
    "resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "with open('results/' + file + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    }
   ],
   "source": [
    "nvar = 2\n",
    "\n",
    "operations = ['OR', 'AND', 'XOR']\n",
    "operation = 'XOR'\n",
    "\n",
    "datasets = ['Simulation', 'German', 'Homecredit']\n",
    "data = 'Simulation'\n",
    "\n",
    "loadDatasets = {\n",
    "    'Simulation1V': simul1V,\n",
    "    'Simulation2V': simul2V,\n",
    "    'German1V': GermanDataset1V,\n",
    "    'German2V': GermanDataset2V,\n",
    "    'Homecredit1V': Homecredit1V,\n",
    "    'Homecredit2V': Homecredit2V\n",
    "}\n",
    "\n",
    "dataset = data + str(nvar) + 'V'\n",
    "\n",
    "argumentsLoadData = {\n",
    "    'seed': seed,\n",
    "    'operation': operation\n",
    "}\n",
    "\n",
    "data_train, data_val, data_test, \\\n",
    "sensitive_attribute, privileged_groups, unprivileged_groups, \\\n",
    "data_val_single, data_test_single = loadDatasets[dataset](**argumentsLoadData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# In processing\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m quality \u001b[38;5;129;01min\u001b[39;00m quality_constraints_meta:\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mInprocMeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m InprocPI(data_train, data_val, data_test, sensitive_attribute, eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50.0\u001b[39m, do_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mreset_default_graph()\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mInprocMeta\u001b[1;34m(data_train, data_val, data_test, sensitive_attribute, quality, tau, do_results)\u001b[0m\n\u001b[0;32m     13\u001b[0m methods[model_name_quality] \u001b[38;5;241m=\u001b[39m MetaFairClassifier(\n\u001b[0;32m     14\u001b[0m     tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[0;32m     15\u001b[0m     sensitive_attr\u001b[38;5;241m=\u001b[39msensitive_attribute,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mquality,\n\u001b[0;32m     17\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m methods[model_name_quality] \u001b[38;5;241m=\u001b[39m \u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name_quality\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Obtain scores\u001b[39;00m\n\u001b[0;32m     24\u001b[0m methods[model_name_quality]\u001b[38;5;241m.\u001b[39mscores_train \u001b[38;5;241m=\u001b[39m methods[model_name_quality]\u001b[38;5;241m.\u001b[39mpredict(train)\u001b[38;5;241m.\u001b[39mscores\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\meta_fair_classifier.py:66\u001b[0m, in \u001b[0;36mMetaFairClassifier.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     59\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(dataset\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m==\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfavorable_label,\n\u001b[0;32m     60\u001b[0m                    \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m x_control_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m     62\u001b[0m         np\u001b[38;5;241m.\u001b[39misin(dataset\u001b[38;5;241m.\u001b[39mprotected_attributes[:, sens_idx],\n\u001b[0;32m     63\u001b[0m                 dataset\u001b[38;5;241m.\u001b[39mprivileged_protected_attributes[sens_idx]),\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_control_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\General.py:99\u001b[0m, in \u001b[0;36mGeneral.getModel\u001b[1;34m(self, tau, X, y, sens, random_state)\u001b[0m\n\u001b[0;32m     96\u001b[0m samples \u001b[38;5;241m=\u001b[39m dist_x\u001b[38;5;241m.\u001b[39mrvs(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# TODO: why 20?\u001b[39;00m\n\u001b[0;32m     97\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradientDescent(dist, a, b, samples, z_1)\n\u001b[1;32m---> 99\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetValueForX\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    102\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\StatisticalRate.py:30\u001b[0m, in \u001b[0;36mStatisticalRate.getValueForX\u001b[1;34m(self, dist, a, b, params, z_prior, x, return_cs)\u001b[0m\n\u001b[0;32m     28\u001b[0m prob_m1_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob(dist, np\u001b[38;5;241m.\u001b[39mc_[x, \u001b[38;5;241m-\u001b[39mpos, pos])\n\u001b[0;32m     29\u001b[0m prob_1_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob(dist, np\u001b[38;5;241m.\u001b[39mc_[x, pos, np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x))])\n\u001b[1;32m---> 30\u001b[0m prob_m1_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m total \u001b[38;5;241m=\u001b[39m prob_1_1 \u001b[38;5;241m+\u001b[39m prob_1_0 \u001b[38;5;241m+\u001b[39m prob_m1_0 \u001b[38;5;241m+\u001b[39m prob_m1_1\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# if total == 0:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#     return 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\General.py:75\u001b[0m, in \u001b[0;36mGeneral.prob\u001b[1;34m(self, dist, x)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprob\u001b[39m(\u001b[38;5;28mself\u001b[39m, dist, x):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:744\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.pdf\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:739\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.logpdf\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogpdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    738\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist\u001b[38;5;241m.\u001b[39m_process_quantiles(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m--> 739\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_pdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _squeeze_output(out)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\scipy\\stats\\_multivariate.py:467\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._logpdf\u001b[1;34m(self, x, mean, prec_U, log_det_cov, rank)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Log of the multivariate normal probability density function.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    466\u001b[0m dev \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m mean\n\u001b[1;32m--> 467\u001b[0m maha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_U\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (rank \u001b[38;5;241m*\u001b[39m _LOG_2PI \u001b[38;5;241m+\u001b[39m log_det_cov \u001b[38;5;241m+\u001b[39m maha)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "case = 'ind'\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Obtain benchmarks\n",
    "modelsNames, modelsTrain, modelsArgs = ObtainPrelDataSingle()\n",
    "modelsBenchmark = modelsNames\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "# In processing\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# Post processing\n",
    "for model in modelsNames:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "# Save the file\n",
    "file =  dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "\n",
    "resultsDict[file] = dict()\n",
    "resultsDict[file]['methods'] = methods\n",
    "resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.655120; batch adversarial loss: 0.734380\n",
      "epoch 1; iter: 0; batch classifier loss: 0.499045; batch adversarial loss: 0.705493\n",
      "epoch 2; iter: 0; batch classifier loss: 0.430724; batch adversarial loss: 0.805806\n",
      "epoch 3; iter: 0; batch classifier loss: 0.461004; batch adversarial loss: 0.803801\n",
      "epoch 4; iter: 0; batch classifier loss: 0.378912; batch adversarial loss: 0.884379\n",
      "epoch 5; iter: 0; batch classifier loss: 0.439532; batch adversarial loss: 0.814672\n",
      "epoch 6; iter: 0; batch classifier loss: 0.372998; batch adversarial loss: 0.860546\n",
      "epoch 7; iter: 0; batch classifier loss: 0.439463; batch adversarial loss: 0.737609\n",
      "epoch 8; iter: 0; batch classifier loss: 0.361792; batch adversarial loss: 0.773696\n",
      "epoch 9; iter: 0; batch classifier loss: 0.355298; batch adversarial loss: 0.768550\n",
      "epoch 10; iter: 0; batch classifier loss: 0.282246; batch adversarial loss: 0.739926\n",
      "epoch 11; iter: 0; batch classifier loss: 0.356643; batch adversarial loss: 0.768663\n",
      "epoch 12; iter: 0; batch classifier loss: 0.262501; batch adversarial loss: 0.751766\n",
      "epoch 13; iter: 0; batch classifier loss: 0.406016; batch adversarial loss: 0.694926\n",
      "epoch 14; iter: 0; batch classifier loss: 0.369994; batch adversarial loss: 0.735714\n",
      "epoch 15; iter: 0; batch classifier loss: 0.338135; batch adversarial loss: 0.732283\n",
      "epoch 16; iter: 0; batch classifier loss: 0.411680; batch adversarial loss: 0.692062\n",
      "epoch 17; iter: 0; batch classifier loss: 0.400695; batch adversarial loss: 0.693117\n",
      "epoch 18; iter: 0; batch classifier loss: 0.297215; batch adversarial loss: 0.711241\n",
      "epoch 19; iter: 0; batch classifier loss: 0.467453; batch adversarial loss: 0.698857\n",
      "epoch 20; iter: 0; batch classifier loss: 0.442025; batch adversarial loss: 0.691647\n",
      "epoch 21; iter: 0; batch classifier loss: 0.348574; batch adversarial loss: 0.719030\n",
      "epoch 22; iter: 0; batch classifier loss: 0.448361; batch adversarial loss: 0.710242\n",
      "epoch 23; iter: 0; batch classifier loss: 0.338627; batch adversarial loss: 0.712355\n",
      "epoch 24; iter: 0; batch classifier loss: 0.400649; batch adversarial loss: 0.717366\n",
      "epoch 25; iter: 0; batch classifier loss: 0.442457; batch adversarial loss: 0.718306\n",
      "epoch 26; iter: 0; batch classifier loss: 0.407911; batch adversarial loss: 0.710300\n",
      "epoch 27; iter: 0; batch classifier loss: 0.541127; batch adversarial loss: 0.701208\n",
      "epoch 28; iter: 0; batch classifier loss: 0.358189; batch adversarial loss: 0.697498\n",
      "epoch 29; iter: 0; batch classifier loss: 0.450915; batch adversarial loss: 0.702131\n",
      "epoch 30; iter: 0; batch classifier loss: 0.385331; batch adversarial loss: 0.718872\n",
      "epoch 31; iter: 0; batch classifier loss: 0.500532; batch adversarial loss: 0.715604\n",
      "epoch 32; iter: 0; batch classifier loss: 0.506622; batch adversarial loss: 0.709672\n",
      "epoch 33; iter: 0; batch classifier loss: 0.553210; batch adversarial loss: 0.729999\n",
      "epoch 34; iter: 0; batch classifier loss: 0.642104; batch adversarial loss: 0.736737\n",
      "epoch 35; iter: 0; batch classifier loss: 0.589136; batch adversarial loss: 0.740893\n",
      "epoch 36; iter: 0; batch classifier loss: 0.512712; batch adversarial loss: 0.735766\n",
      "epoch 37; iter: 0; batch classifier loss: 0.679087; batch adversarial loss: 0.724098\n",
      "epoch 38; iter: 0; batch classifier loss: 0.624363; batch adversarial loss: 0.706952\n",
      "epoch 39; iter: 0; batch classifier loss: 0.508451; batch adversarial loss: 0.700546\n",
      "epoch 40; iter: 0; batch classifier loss: 0.434018; batch adversarial loss: 0.701834\n",
      "epoch 41; iter: 0; batch classifier loss: 0.319393; batch adversarial loss: 0.688937\n",
      "epoch 42; iter: 0; batch classifier loss: 0.400025; batch adversarial loss: 0.683998\n",
      "epoch 43; iter: 0; batch classifier loss: 0.459177; batch adversarial loss: 0.702438\n",
      "epoch 44; iter: 0; batch classifier loss: 0.357663; batch adversarial loss: 0.698816\n",
      "epoch 45; iter: 0; batch classifier loss: 0.345363; batch adversarial loss: 0.693442\n",
      "epoch 46; iter: 0; batch classifier loss: 0.430890; batch adversarial loss: 0.697868\n",
      "epoch 47; iter: 0; batch classifier loss: 0.456325; batch adversarial loss: 0.688659\n",
      "epoch 48; iter: 0; batch classifier loss: 0.419212; batch adversarial loss: 0.698534\n",
      "epoch 49; iter: 0; batch classifier loss: 0.399103; batch adversarial loss: 0.698819\n",
      "epoch 50; iter: 0; batch classifier loss: 0.352778; batch adversarial loss: 0.695938\n",
      "epoch 51; iter: 0; batch classifier loss: 0.440363; batch adversarial loss: 0.697463\n",
      "epoch 52; iter: 0; batch classifier loss: 0.426522; batch adversarial loss: 0.697119\n",
      "epoch 53; iter: 0; batch classifier loss: 0.424639; batch adversarial loss: 0.698842\n",
      "epoch 54; iter: 0; batch classifier loss: 0.417625; batch adversarial loss: 0.695987\n",
      "epoch 55; iter: 0; batch classifier loss: 0.500902; batch adversarial loss: 0.695600\n",
      "epoch 56; iter: 0; batch classifier loss: 0.537873; batch adversarial loss: 0.690700\n",
      "epoch 57; iter: 0; batch classifier loss: 0.379142; batch adversarial loss: 0.703618\n",
      "epoch 58; iter: 0; batch classifier loss: 0.358542; batch adversarial loss: 0.691861\n",
      "epoch 59; iter: 0; batch classifier loss: 0.398672; batch adversarial loss: 0.695420\n",
      "epoch 60; iter: 0; batch classifier loss: 0.308892; batch adversarial loss: 0.693367\n",
      "epoch 61; iter: 0; batch classifier loss: 0.525586; batch adversarial loss: 0.696572\n",
      "epoch 62; iter: 0; batch classifier loss: 0.441535; batch adversarial loss: 0.691520\n",
      "epoch 63; iter: 0; batch classifier loss: 0.485318; batch adversarial loss: 0.692892\n",
      "epoch 64; iter: 0; batch classifier loss: 0.337754; batch adversarial loss: 0.691746\n",
      "epoch 65; iter: 0; batch classifier loss: 0.412559; batch adversarial loss: 0.691805\n",
      "epoch 66; iter: 0; batch classifier loss: 0.374050; batch adversarial loss: 0.692772\n",
      "epoch 67; iter: 0; batch classifier loss: 0.433061; batch adversarial loss: 0.692193\n",
      "epoch 68; iter: 0; batch classifier loss: 0.422698; batch adversarial loss: 0.694725\n",
      "epoch 69; iter: 0; batch classifier loss: 0.435862; batch adversarial loss: 0.692048\n",
      "epoch 70; iter: 0; batch classifier loss: 0.373819; batch adversarial loss: 0.692395\n",
      "epoch 71; iter: 0; batch classifier loss: 0.413957; batch adversarial loss: 0.694137\n",
      "epoch 72; iter: 0; batch classifier loss: 0.378877; batch adversarial loss: 0.694844\n",
      "epoch 73; iter: 0; batch classifier loss: 0.412104; batch adversarial loss: 0.692440\n",
      "epoch 74; iter: 0; batch classifier loss: 0.444426; batch adversarial loss: 0.690290\n",
      "epoch 75; iter: 0; batch classifier loss: 0.430486; batch adversarial loss: 0.694746\n",
      "epoch 76; iter: 0; batch classifier loss: 0.250101; batch adversarial loss: 0.698016\n",
      "epoch 77; iter: 0; batch classifier loss: 0.346542; batch adversarial loss: 0.690988\n",
      "epoch 78; iter: 0; batch classifier loss: 0.368277; batch adversarial loss: 0.692345\n",
      "epoch 79; iter: 0; batch classifier loss: 0.535687; batch adversarial loss: 0.693781\n",
      "epoch 0; iter: 0; batch classifier loss: 0.663850; batch adversarial loss: 0.698582\n",
      "epoch 1; iter: 0; batch classifier loss: 0.459021; batch adversarial loss: 0.683980\n",
      "epoch 2; iter: 0; batch classifier loss: 0.512225; batch adversarial loss: 0.742429\n",
      "epoch 3; iter: 0; batch classifier loss: 0.432716; batch adversarial loss: 0.722641\n",
      "epoch 4; iter: 0; batch classifier loss: 0.414858; batch adversarial loss: 0.714477\n",
      "epoch 5; iter: 0; batch classifier loss: 0.396002; batch adversarial loss: 0.685946\n",
      "epoch 6; iter: 0; batch classifier loss: 0.401101; batch adversarial loss: 0.720447\n",
      "epoch 7; iter: 0; batch classifier loss: 0.394098; batch adversarial loss: 0.694907\n",
      "epoch 8; iter: 0; batch classifier loss: 0.414266; batch adversarial loss: 0.717515\n",
      "epoch 9; iter: 0; batch classifier loss: 0.404194; batch adversarial loss: 0.698753\n",
      "epoch 10; iter: 0; batch classifier loss: 0.328816; batch adversarial loss: 0.699677\n",
      "epoch 11; iter: 0; batch classifier loss: 0.363250; batch adversarial loss: 0.693871\n",
      "epoch 12; iter: 0; batch classifier loss: 0.443573; batch adversarial loss: 0.699415\n",
      "epoch 13; iter: 0; batch classifier loss: 0.315830; batch adversarial loss: 0.698940\n",
      "epoch 14; iter: 0; batch classifier loss: 0.371028; batch adversarial loss: 0.692941\n",
      "epoch 15; iter: 0; batch classifier loss: 0.425736; batch adversarial loss: 0.681483\n",
      "epoch 16; iter: 0; batch classifier loss: 0.309741; batch adversarial loss: 0.709828\n",
      "epoch 17; iter: 0; batch classifier loss: 0.293023; batch adversarial loss: 0.694764\n",
      "epoch 18; iter: 0; batch classifier loss: 0.424452; batch adversarial loss: 0.704816\n",
      "epoch 19; iter: 0; batch classifier loss: 0.304985; batch adversarial loss: 0.689799\n",
      "epoch 20; iter: 0; batch classifier loss: 0.383964; batch adversarial loss: 0.701566\n",
      "epoch 21; iter: 0; batch classifier loss: 0.392401; batch adversarial loss: 0.692397\n",
      "epoch 22; iter: 0; batch classifier loss: 0.393839; batch adversarial loss: 0.694856\n",
      "epoch 23; iter: 0; batch classifier loss: 0.375129; batch adversarial loss: 0.692835\n",
      "epoch 24; iter: 0; batch classifier loss: 0.355167; batch adversarial loss: 0.691324\n",
      "epoch 25; iter: 0; batch classifier loss: 0.479012; batch adversarial loss: 0.691144\n",
      "epoch 26; iter: 0; batch classifier loss: 0.378939; batch adversarial loss: 0.694367\n",
      "epoch 27; iter: 0; batch classifier loss: 0.326043; batch adversarial loss: 0.696989\n",
      "epoch 28; iter: 0; batch classifier loss: 0.383076; batch adversarial loss: 0.692111\n",
      "epoch 29; iter: 0; batch classifier loss: 0.373769; batch adversarial loss: 0.688764\n",
      "epoch 30; iter: 0; batch classifier loss: 0.387899; batch adversarial loss: 0.692558\n",
      "epoch 31; iter: 0; batch classifier loss: 0.485884; batch adversarial loss: 0.692373\n",
      "epoch 32; iter: 0; batch classifier loss: 0.344972; batch adversarial loss: 0.688102\n",
      "epoch 33; iter: 0; batch classifier loss: 0.336065; batch adversarial loss: 0.697917\n",
      "epoch 34; iter: 0; batch classifier loss: 0.419133; batch adversarial loss: 0.690878\n",
      "epoch 35; iter: 0; batch classifier loss: 0.408150; batch adversarial loss: 0.694324\n",
      "epoch 36; iter: 0; batch classifier loss: 0.357198; batch adversarial loss: 0.690469\n",
      "epoch 37; iter: 0; batch classifier loss: 0.419508; batch adversarial loss: 0.693162\n",
      "epoch 38; iter: 0; batch classifier loss: 0.392267; batch adversarial loss: 0.693421\n",
      "epoch 39; iter: 0; batch classifier loss: 0.362788; batch adversarial loss: 0.697956\n",
      "epoch 40; iter: 0; batch classifier loss: 0.417361; batch adversarial loss: 0.692929\n",
      "epoch 41; iter: 0; batch classifier loss: 0.363533; batch adversarial loss: 0.695007\n",
      "epoch 42; iter: 0; batch classifier loss: 0.382050; batch adversarial loss: 0.695465\n",
      "epoch 43; iter: 0; batch classifier loss: 0.453802; batch adversarial loss: 0.697006\n",
      "epoch 44; iter: 0; batch classifier loss: 0.380055; batch adversarial loss: 0.695740\n",
      "epoch 45; iter: 0; batch classifier loss: 0.403888; batch adversarial loss: 0.692578\n",
      "epoch 46; iter: 0; batch classifier loss: 0.404589; batch adversarial loss: 0.693837\n",
      "epoch 47; iter: 0; batch classifier loss: 0.409520; batch adversarial loss: 0.692775\n",
      "epoch 48; iter: 0; batch classifier loss: 0.402129; batch adversarial loss: 0.692288\n",
      "epoch 49; iter: 0; batch classifier loss: 0.406036; batch adversarial loss: 0.695502\n",
      "epoch 50; iter: 0; batch classifier loss: 0.376830; batch adversarial loss: 0.694068\n",
      "epoch 51; iter: 0; batch classifier loss: 0.398617; batch adversarial loss: 0.694810\n",
      "epoch 52; iter: 0; batch classifier loss: 0.437963; batch adversarial loss: 0.693741\n",
      "epoch 53; iter: 0; batch classifier loss: 0.380995; batch adversarial loss: 0.694882\n",
      "epoch 54; iter: 0; batch classifier loss: 0.296635; batch adversarial loss: 0.693933\n",
      "epoch 55; iter: 0; batch classifier loss: 0.397327; batch adversarial loss: 0.692489\n",
      "epoch 56; iter: 0; batch classifier loss: 0.349393; batch adversarial loss: 0.695306\n",
      "epoch 57; iter: 0; batch classifier loss: 0.422747; batch adversarial loss: 0.692928\n",
      "epoch 58; iter: 0; batch classifier loss: 0.373946; batch adversarial loss: 0.696781\n",
      "epoch 59; iter: 0; batch classifier loss: 0.376794; batch adversarial loss: 0.691771\n",
      "epoch 60; iter: 0; batch classifier loss: 0.393188; batch adversarial loss: 0.694601\n",
      "epoch 61; iter: 0; batch classifier loss: 0.359782; batch adversarial loss: 0.694421\n",
      "epoch 62; iter: 0; batch classifier loss: 0.397246; batch adversarial loss: 0.691772\n",
      "epoch 63; iter: 0; batch classifier loss: 0.504902; batch adversarial loss: 0.692992\n",
      "epoch 64; iter: 0; batch classifier loss: 0.412079; batch adversarial loss: 0.696199\n",
      "epoch 65; iter: 0; batch classifier loss: 0.374319; batch adversarial loss: 0.699123\n",
      "epoch 66; iter: 0; batch classifier loss: 0.545248; batch adversarial loss: 0.693439\n",
      "epoch 67; iter: 0; batch classifier loss: 0.351601; batch adversarial loss: 0.694495\n",
      "epoch 68; iter: 0; batch classifier loss: 0.371575; batch adversarial loss: 0.695786\n",
      "epoch 69; iter: 0; batch classifier loss: 0.394813; batch adversarial loss: 0.696872\n",
      "epoch 70; iter: 0; batch classifier loss: 0.387973; batch adversarial loss: 0.693201\n",
      "epoch 71; iter: 0; batch classifier loss: 0.357388; batch adversarial loss: 0.696316\n",
      "epoch 72; iter: 0; batch classifier loss: 0.413424; batch adversarial loss: 0.695419\n",
      "epoch 73; iter: 0; batch classifier loss: 0.400021; batch adversarial loss: 0.698820\n",
      "epoch 74; iter: 0; batch classifier loss: 0.435868; batch adversarial loss: 0.698192\n",
      "epoch 75; iter: 0; batch classifier loss: 0.417441; batch adversarial loss: 0.696255\n",
      "epoch 76; iter: 0; batch classifier loss: 0.431176; batch adversarial loss: 0.693737\n",
      "epoch 77; iter: 0; batch classifier loss: 0.286196; batch adversarial loss: 0.695294\n",
      "epoch 78; iter: 0; batch classifier loss: 0.378056; batch adversarial loss: 0.693831\n",
      "epoch 79; iter: 0; batch classifier loss: 0.410560; batch adversarial loss: 0.695836\n",
      "epoch 0; iter: 0; batch classifier loss: 0.639393; batch adversarial loss: 0.722191\n",
      "epoch 1; iter: 0; batch classifier loss: 0.509555; batch adversarial loss: 0.708988\n",
      "epoch 2; iter: 0; batch classifier loss: 0.470905; batch adversarial loss: 0.700209\n",
      "epoch 3; iter: 0; batch classifier loss: 0.424625; batch adversarial loss: 0.712099\n",
      "epoch 4; iter: 0; batch classifier loss: 0.417640; batch adversarial loss: 0.703129\n",
      "epoch 5; iter: 0; batch classifier loss: 0.388131; batch adversarial loss: 0.723070\n",
      "epoch 6; iter: 0; batch classifier loss: 0.342299; batch adversarial loss: 0.700268\n",
      "epoch 7; iter: 0; batch classifier loss: 0.372245; batch adversarial loss: 0.718225\n",
      "epoch 8; iter: 0; batch classifier loss: 0.393790; batch adversarial loss: 0.681851\n",
      "epoch 9; iter: 0; batch classifier loss: 0.489409; batch adversarial loss: 0.695801\n",
      "epoch 10; iter: 0; batch classifier loss: 0.447236; batch adversarial loss: 0.699344\n",
      "epoch 11; iter: 0; batch classifier loss: 0.334694; batch adversarial loss: 0.702853\n",
      "epoch 12; iter: 0; batch classifier loss: 0.406375; batch adversarial loss: 0.698295\n",
      "epoch 13; iter: 0; batch classifier loss: 0.464126; batch adversarial loss: 0.711511\n",
      "epoch 14; iter: 0; batch classifier loss: 0.402057; batch adversarial loss: 0.708369\n",
      "epoch 15; iter: 0; batch classifier loss: 0.304324; batch adversarial loss: 0.693689\n",
      "epoch 16; iter: 0; batch classifier loss: 0.391826; batch adversarial loss: 0.697649\n",
      "epoch 17; iter: 0; batch classifier loss: 0.369958; batch adversarial loss: 0.696421\n",
      "epoch 18; iter: 0; batch classifier loss: 0.430370; batch adversarial loss: 0.688534\n",
      "epoch 19; iter: 0; batch classifier loss: 0.396270; batch adversarial loss: 0.687794\n",
      "epoch 20; iter: 0; batch classifier loss: 0.395764; batch adversarial loss: 0.712107\n",
      "epoch 21; iter: 0; batch classifier loss: 0.402953; batch adversarial loss: 0.703704\n",
      "epoch 22; iter: 0; batch classifier loss: 0.431689; batch adversarial loss: 0.707440\n",
      "epoch 23; iter: 0; batch classifier loss: 0.413525; batch adversarial loss: 0.692625\n",
      "epoch 24; iter: 0; batch classifier loss: 0.473253; batch adversarial loss: 0.690227\n",
      "epoch 25; iter: 0; batch classifier loss: 0.342772; batch adversarial loss: 0.691293\n",
      "epoch 26; iter: 0; batch classifier loss: 0.360267; batch adversarial loss: 0.688518\n",
      "epoch 27; iter: 0; batch classifier loss: 0.342470; batch adversarial loss: 0.695900\n",
      "epoch 28; iter: 0; batch classifier loss: 0.470298; batch adversarial loss: 0.700044\n",
      "epoch 29; iter: 0; batch classifier loss: 0.412810; batch adversarial loss: 0.693418\n",
      "epoch 30; iter: 0; batch classifier loss: 0.342027; batch adversarial loss: 0.696795\n",
      "epoch 31; iter: 0; batch classifier loss: 0.350384; batch adversarial loss: 0.698479\n",
      "epoch 32; iter: 0; batch classifier loss: 0.371459; batch adversarial loss: 0.693479\n",
      "epoch 33; iter: 0; batch classifier loss: 0.509540; batch adversarial loss: 0.690149\n",
      "epoch 34; iter: 0; batch classifier loss: 0.393742; batch adversarial loss: 0.696911\n",
      "epoch 35; iter: 0; batch classifier loss: 0.362938; batch adversarial loss: 0.701804\n",
      "epoch 36; iter: 0; batch classifier loss: 0.400424; batch adversarial loss: 0.690471\n",
      "epoch 37; iter: 0; batch classifier loss: 0.372607; batch adversarial loss: 0.700011\n",
      "epoch 38; iter: 0; batch classifier loss: 0.486590; batch adversarial loss: 0.684811\n",
      "epoch 39; iter: 0; batch classifier loss: 0.369482; batch adversarial loss: 0.693960\n",
      "epoch 40; iter: 0; batch classifier loss: 0.377815; batch adversarial loss: 0.689896\n",
      "epoch 41; iter: 0; batch classifier loss: 0.392669; batch adversarial loss: 0.694950\n",
      "epoch 42; iter: 0; batch classifier loss: 0.400226; batch adversarial loss: 0.685340\n",
      "epoch 43; iter: 0; batch classifier loss: 0.326868; batch adversarial loss: 0.690111\n",
      "epoch 44; iter: 0; batch classifier loss: 0.342217; batch adversarial loss: 0.694635\n",
      "epoch 45; iter: 0; batch classifier loss: 0.361300; batch adversarial loss: 0.694402\n",
      "epoch 46; iter: 0; batch classifier loss: 0.465495; batch adversarial loss: 0.694790\n",
      "epoch 47; iter: 0; batch classifier loss: 0.373569; batch adversarial loss: 0.686068\n",
      "epoch 48; iter: 0; batch classifier loss: 0.340358; batch adversarial loss: 0.690567\n",
      "epoch 49; iter: 0; batch classifier loss: 0.405590; batch adversarial loss: 0.691961\n",
      "epoch 50; iter: 0; batch classifier loss: 0.386088; batch adversarial loss: 0.695845\n",
      "epoch 51; iter: 0; batch classifier loss: 0.383593; batch adversarial loss: 0.699984\n",
      "epoch 52; iter: 0; batch classifier loss: 0.363234; batch adversarial loss: 0.695789\n",
      "epoch 53; iter: 0; batch classifier loss: 0.490510; batch adversarial loss: 0.697944\n",
      "epoch 54; iter: 0; batch classifier loss: 0.426645; batch adversarial loss: 0.697421\n",
      "epoch 55; iter: 0; batch classifier loss: 0.347918; batch adversarial loss: 0.692920\n",
      "epoch 56; iter: 0; batch classifier loss: 0.464935; batch adversarial loss: 0.688326\n",
      "epoch 57; iter: 0; batch classifier loss: 0.359282; batch adversarial loss: 0.696255\n",
      "epoch 58; iter: 0; batch classifier loss: 0.412001; batch adversarial loss: 0.694388\n",
      "epoch 59; iter: 0; batch classifier loss: 0.450336; batch adversarial loss: 0.692899\n",
      "epoch 60; iter: 0; batch classifier loss: 0.498798; batch adversarial loss: 0.699214\n",
      "epoch 61; iter: 0; batch classifier loss: 0.404041; batch adversarial loss: 0.687685\n",
      "epoch 62; iter: 0; batch classifier loss: 0.325899; batch adversarial loss: 0.699121\n",
      "epoch 63; iter: 0; batch classifier loss: 0.364094; batch adversarial loss: 0.694861\n",
      "epoch 64; iter: 0; batch classifier loss: 0.415461; batch adversarial loss: 0.688876\n",
      "epoch 65; iter: 0; batch classifier loss: 0.506910; batch adversarial loss: 0.690605\n",
      "epoch 66; iter: 0; batch classifier loss: 0.435785; batch adversarial loss: 0.693473\n",
      "epoch 67; iter: 0; batch classifier loss: 0.397344; batch adversarial loss: 0.694813\n",
      "epoch 68; iter: 0; batch classifier loss: 0.446878; batch adversarial loss: 0.694474\n",
      "epoch 69; iter: 0; batch classifier loss: 0.284692; batch adversarial loss: 0.690743\n",
      "epoch 70; iter: 0; batch classifier loss: 0.390867; batch adversarial loss: 0.691160\n",
      "epoch 71; iter: 0; batch classifier loss: 0.311725; batch adversarial loss: 0.693312\n",
      "epoch 72; iter: 0; batch classifier loss: 0.428452; batch adversarial loss: 0.688969\n",
      "epoch 73; iter: 0; batch classifier loss: 0.329913; batch adversarial loss: 0.688227\n",
      "epoch 74; iter: 0; batch classifier loss: 0.328271; batch adversarial loss: 0.691640\n",
      "epoch 75; iter: 0; batch classifier loss: 0.550333; batch adversarial loss: 0.696440\n",
      "epoch 76; iter: 0; batch classifier loss: 0.351840; batch adversarial loss: 0.691489\n",
      "epoch 77; iter: 0; batch classifier loss: 0.408629; batch adversarial loss: 0.694908\n",
      "epoch 78; iter: 0; batch classifier loss: 0.263995; batch adversarial loss: 0.694638\n",
      "epoch 79; iter: 0; batch classifier loss: 0.390703; batch adversarial loss: 0.689744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 60\u001b[0m\n\u001b[0;32m     56\u001b[0m         PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n\u001b[0;32m     58\u001b[0m sess\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m---> 60\u001b[0m file \u001b[38;5;241m=\u001b[39m dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m operation \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m case \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mi\u001b[49m)\n\u001b[0;32m     62\u001b[0m resultsDict[file] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     63\u001b[0m resultsDict[file][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethods\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m methods\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "case = 'com'\n",
    "data = 'Simulation'\n",
    "i = 0\n",
    "\n",
    "# Obtain benchmarks and in proncessing models\n",
    "modelsNames, modelsBenchmark, modelsPost, \\\n",
    "modelsTrain, modelsArgs = ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups)\n",
    "\n",
    "measurement = 'bal_acc'\n",
    "combination = []\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing + In processing\n",
    "for model in modelsNames:\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    if model == 'adversarial':\n",
    "        sess.close()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "    \n",
    "    if model == 'adversarial':\n",
    "        sess.close()\n",
    "\n",
    "# Pre/In processing + Post processing\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "for model in modelsPost:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "        \n",
    "sess.close()\n",
    "\n",
    "file = dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "\n",
    "resultsDict[file] = dict()\n",
    "resultsDict[file]['methods'] = methods\n",
    "resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "file = dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "\n",
    "resultsDict = dict()\n",
    "resultsDict[file] = dict()\n",
    "resultsDict[file]['methods'] = methods\n",
    "resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultsDict = dict()\n",
    "sweepsDict = dict()\n",
    "\n",
    "for file in os.listdir('results/best'):\n",
    "    name = file[:-12]\n",
    "    with open('results/best/' + name + '_best.pickle', 'rb') as handle:\n",
    "        resultsDict[name] = pickle.load(handle)\n",
    "    with open('results/sweep/' + name + '_sweep.pickle', 'rb') as handle:\n",
    "        sweepsDict[name] = pickle.load(handle)\n",
    "\n",
    "names = resultsDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_dataset(dataset):\n",
    "    new_table = dataset.copy(deep = True)\n",
    "    \n",
    "    colNames = dataset.columns\n",
    "    subset = [column for column in colNames if column not in ['best_threshold']]\n",
    "    new_table.loc[:, subset] =  (new_table.loc[:, subset] - new_table.loc['logreg', subset])/new_table.loc['logreg', subset]*100\n",
    "    return new_table\n",
    "\n",
    "def compare_tables(dataset1, dataset2):\n",
    "    new_table = dataset1.copy(deep = True) \n",
    "    new_table =  (dataset2 - dataset1)/dataset1*100\n",
    "    return new_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tables = dict()\n",
    "\n",
    "for name in names:\n",
    "    new_tables[name] = tidy_dataset(resultsDict[name])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'float_format', \"{:,.2f}\".format):  # more options can be specified also\n",
    "    display(new_tables['German1V_com_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(new_tables['German1V_com_1']['separation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_comp = compare_tables(resultsDict['German1V_com_1'], resultsDict['German2V_AND_com_1'])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'float_format', \"{:,.2f}\".format):  # more options can be specified also\n",
    "    display(tabla_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweepsDict['German1V_com_1']['DI_pir']['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install empirical-attainment-func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sweep = sweepsDict['German2V_XOR_ind_1']\n",
    "method = 'adversarial'\n",
    "metric1 = sweep[method]['separation']\n",
    "metric2 = sweep[method]['acc']\n",
    "\n",
    "total_dict = dict()\n",
    "for method in sweep.keys():\n",
    "    try:\n",
    "        total_dict['separation'] += sweep[method]['separation']\n",
    "        total_dict['independence'] += sweep[method]['independence']\n",
    "        total_dict['sufficiency'] += sweep[method]['sufficiency']\n",
    "        total_dict['acc'] += sweep[method]['acc']\n",
    "    except:\n",
    "        total_dict['separation'] = sweep[method]['separation']\n",
    "        total_dict['independence'] = sweep[method]['independence']\n",
    "        total_dict['sufficiency'] = sweep[method]['sufficiency']\n",
    "        total_dict['acc'] = sweep[method]['acc']\n",
    "\n",
    "def plot_pareto(sweepDict, metric1_name, metric2_name):\n",
    "\n",
    "    metric1 = sweepDict[metric1_name]\n",
    "    metric2 = sweepDict[metric2_name]\n",
    "\n",
    "    metric1_np = np.array(metric1)\n",
    "    metric2_np = np.array(metric2)\n",
    "    \n",
    "    frontier = np.zeros(np.shape(metric1_np)) \n",
    "\n",
    "    for value in metric1:\n",
    "        index = np.where(metric1_np == value)\n",
    "        maxim = max(metric2_np[index])\n",
    "        frontier[index] = maxim \n",
    "\n",
    "    sorted_frontier = np.array([x for _,x in sorted(zip(metric1, frontier))])\n",
    "    cum_sorted_frontier = np.maximum.accumulate(sorted_frontier)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Trade offs')\n",
    "    ax.set_xlabel(metric1_name)\n",
    "    ax.set_ylabel(metric2_name)\n",
    "    ax.step(np.sort(metric1_np), cum_sorted_frontier)\n",
    "    return fig, ax\n",
    "\n",
    "#sweep[method]\n",
    "fig, ax = plot_pareto(total_dict, 'separation', 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where([1,2,3,1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(np.array(metric2)[np.array(metric1) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep['DI_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openWrapper(fun):\n",
    "\n",
    "    def sessWrap(*args, **kwargs):\n",
    "\n",
    "        # Open \n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "\n",
    "        fun(*args, **kwargs)\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "\n",
    "    return sessWrap\n",
    "\n",
    "\n",
    "\n",
    "class Multiprocessor():\n",
    "    \n",
    "\n",
    "    def __init__(self, preproc = None, inproc = None, postproc = None, results = None, dataset = None):\n",
    "    \n",
    "        self.data = dataset\n",
    "        self.preproc = preproc\n",
    "        self.inproc = inproc\n",
    "        self.postproc = postproc\n",
    "        self.results = results\n",
    "\n",
    "        if self.inproc.__name__ == AdversarialDebiasing.__name__:\n",
    "            self.isNN = True\n",
    "        else:\n",
    "            self.isNN = False\n",
    "\n",
    "        return\n",
    "    \n",
    "\n",
    "    def obtain_data(self, **kwargs):\n",
    "        \n",
    "        if not self.dataset:\n",
    "            return\n",
    "        \n",
    "        self.datasetName = dataset.__name__\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def Preprocess(self, **kwargs):\n",
    "        \n",
    "        if not self.preproc:\n",
    "            return\n",
    "        \n",
    "        self.preprocName = preproc.__name__\n",
    "        preproc(**kwargs)\n",
    "\n",
    "        return\n",
    "        \n",
    "\n",
    "\n",
    "    def Inprocess(self, **kwargs):\n",
    "\n",
    "        if not self.inproc:\n",
    "            return\n",
    "\n",
    "        self.inprocName = inproc.__name__\n",
    "\n",
    "        self.inprocess(kwargs)\n",
    "\n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "    def Postprocess(self, **kwargs):\n",
    "\n",
    "        if not self.postproc:\n",
    "            return\n",
    "        \n",
    "        self.postprocName = postproc.__name__\n",
    "\n",
    "        return\n",
    "    \n",
    "    def Results(self, **kwargs):\n",
    "\n",
    "        self.resultsDict = dict()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiprocessor(Reweighing, None, EqOddsPostprocessing, results, GermanDataset1V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operation1(a,b,c):\n",
    "    return a+b/c\n",
    "\n",
    "def operation2(a=1,b=2):\n",
    "    return a/b\n",
    "\n",
    "def operation3(func, **kwargs):\n",
    "    return 1 + func(**kwargs)\n",
    "\n",
    "args = {\n",
    "    'a': 1,\n",
    "    'b': 2\n",
    "}\n",
    "operation3(operation2, a=1, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100000\n",
    "n = T - 13\n",
    "N = 1000\n",
    "\n",
    "p = 1\n",
    "for i in range(1000):\n",
    "    p *= (n - i)/(T - i)\n",
    "\n",
    "q = 1 - p\n",
    "print(q*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaFair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
