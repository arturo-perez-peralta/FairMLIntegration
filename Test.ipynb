{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arturo\\AppData\\Local\\Temp\\ipykernel_13760\\1338218607.py:20: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "\n",
    "# TF\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.AUTO_REUSE\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345\n",
      "424242\n",
      "777\n",
      "32768\n",
      "45234\n"
     ]
    }
   ],
   "source": [
    "seeds = [12345, 424242, 777, 32768, 45234]\n",
    "\n",
    "for seed in seeds:\n",
    "    print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "\n",
    "def GermanDataset1V(seed = 12345):\n",
    "    \"\"\"\n",
    "    Read and preprocess the German dataset for the case of one sensitive variable\n",
    "    (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the German dataset.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the German dataset.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the German dataset.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the data\n",
    "    dataset_german = GermanDataset(\n",
    "            protected_attribute_names=['age'],            \n",
    "            privileged_classes=[lambda x: x >= 25],      \n",
    "            features_to_drop=['personal_status', 'sex'] \n",
    "        )\n",
    "        \n",
    "    # xgboost requires labels to start at zero\n",
    "    dataset_german.labels[dataset_german.labels.ravel() == 2] =  dataset_german.labels[dataset_german.labels.ravel() == 2] - 2\n",
    "    dataset_german.unfavorable_label = dataset_german.unfavorable_label - 2\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = dataset_german.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We obtain sensitive attribute\n",
    "    sensitive_attribute = dataset_german.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GermanDataset2V(seed = 12345, operation = \"OR\"):\n",
    "    \"\"\"\n",
    "    Read and preprocess the German dataset for the case of two sensitive variables\n",
    "    (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the data\n",
    "    dataset = GermanDataset(\n",
    "        protected_attribute_names=['age'],            \n",
    "        privileged_classes=[lambda x: x >= 25],      \n",
    "        features_to_drop=['personal_status', 'sex'] \n",
    "    )\n",
    "\n",
    "    # load the german dataset and update the data with the OR sum of sex and age\n",
    "    dataset_german_upd = utils.update_german_dataset_from_multiple_protected_attributes(dataset, operation)\n",
    "\n",
    "    # change favorable/unfavorable labels to 1: good; 0: bad\n",
    "    dataset_german_upd.labels[dataset_german_upd.labels.ravel() == 2] =  dataset_german_upd.labels[dataset_german_upd.labels.ravel() == 2] - 2\n",
    "    dataset_german_upd.unfavorable_label = dataset_german_upd.unfavorable_label - 2\n",
    "\n",
    "    # For the single dataset as well\n",
    "    dataset.labels[dataset.labels.ravel() == 2] =  dataset.labels[dataset.labels.ravel() == 2] - 2\n",
    "    dataset.unfavorable_label = dataset.unfavorable_label - 2\n",
    "\n",
    "    # Train, val, test split\n",
    "    data_train, vt = dataset_german_upd.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We do the same on the single variable dataset\n",
    "    _, vt = dataset.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = dataset_german_upd.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german_upd)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Homecredit1V(seed = 12345):\n",
    "    \"\"\"\n",
    "    Read and preprocess the Homecredit dataset for the case of one sensitive variable\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the Homecredit dataset.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the Homecredit dataset.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the Homecredit dataset.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the data\n",
    "    dataset_homecredit = pd.read_csv('data/homecredit.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "    dataset_homecredit = utils.preprocess_homecredit(dataset_homecredit)\n",
    "\n",
    "    dataset_homecredit_aif = utils.convert_to_standard_dataset(\n",
    "            df=dataset_homecredit,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute='AGE',\n",
    "            priviledged_classes=[lambda x: x >= 25],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    # We make a subsample\n",
    "    sample_size_hc = 5000\n",
    "    dataset_homecredit_aif = dataset_homecredit_aif.subset(np.random.randint(0, 307511+1, size=(sample_size_hc)))\n",
    "    # train, val, test split\n",
    "    data_train, vt = dataset_homecredit_aif.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = dataset_homecredit_aif.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_homecredit_aif)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "\n",
    "def Homecredit2V(seed = 12345, operation = \"OR\"):\n",
    "    \"\"\"\n",
    "    Read and preprocess the Homecredit dataset for the case of one sensitive variable\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute.\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    homecredit = pd.read_csv('data/homecredit.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "    homecredit = utils.preprocess_homecredit_mult(homecredit, operation = operation)\n",
    "    homecredit_single = homecredit.copy(deep = True)\n",
    "    \n",
    "    # Transform both datasets to aif360 format\n",
    "    homecredit = utils.convert_to_standard_dataset(\n",
    "            df=homecredit,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute=['PROT_ATTR'],\n",
    "            priviledged_classes=[lambda x: x == 1],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    homecredit_single = utils.convert_to_standard_dataset(\n",
    "            df=homecredit_single,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute=['AGE'],\n",
    "            priviledged_classes=[lambda x: x >= 25],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    # We make a subsample\n",
    "    sample_size_hc = 5000\n",
    "    ssample = np.random.randint(0, 307511+1, size=(sample_size_hc))\n",
    "    homecredit = homecredit.subset(ssample)\n",
    "    homecredit_single = homecredit_single.subset(ssample)\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = homecredit.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    _, vt_single = homecredit.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt_single.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = homecredit.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(homecredit)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single\n",
    "\n",
    "\n",
    "def simul1V(seed = 12345, N = 1000, p1 = 0.5, p2 = 0.5):\n",
    "    \"\"\"\n",
    "    Obtain a simulated dataset from the toy model for the case of one sensitive variable\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        N (int): number of individuals in the dataset.\n",
    "        p1 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the first sensitive variable.\n",
    "        p2 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the second sensitive variable.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the simulation.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the simulation.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the simulation.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create variables\n",
    "    vars = dict()\n",
    "\n",
    "    # Sensitive variables (drawn from a binomial distribution)\n",
    "    vars['sens1'] = np.random.binomial(n = 1, p = p1, size = N)\n",
    "    vars['sens2'] = np.random.binomial(n = 1, p = p2, size = N)\n",
    "\n",
    "    # v1, v2 (noisy measurements of the sensitive variables) and their sum\n",
    "    vars['v1'] = np.random.normal(loc = vars['sens1'], scale = 1.0, size = N)\n",
    "    vars['v2'] = np.random.normal(loc = vars['sens2'], scale = 1.0, size = N)\n",
    "    vars['sum'] = vars['v1'] + vars['v2']\n",
    "\n",
    "    # Noisy measurements of the sum of v1 and v2\n",
    "    vars['indirect'] = np.random.normal(loc = vars['sum'], scale = 1.0, size = N)\n",
    "    vars['weight_response'] = np.random.normal(loc = vars['sum'], scale = 1.0, size = N)\n",
    "\n",
    "    # Response variable\n",
    "    vars['response'] = vars['weight_response'] > 0.0\n",
    "\n",
    "    # Create the dataset with the correct variables\n",
    "    final_vars = ['sens1', 'sens2', 'indirect', 'response']\n",
    "    df = dict()\n",
    "    for name in final_vars:\n",
    "        df[name] = vars[name]\n",
    "    \n",
    "    # Transform the sensitive variables to boolean\n",
    "    df['sens1'] = df['sens1'] == 1\n",
    "    df['sens2'] = df['sens2'] == 1\n",
    "\n",
    "    # Create the dataset from the dictionary\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Convert to standard dataset\n",
    "    data = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['sens1'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = data.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = data.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(data)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "def simul2V(seed = 12345, operation = \"OR\", N = 1000, p1 = 0.5, p2 = 0.5):\n",
    "    \"\"\"\n",
    "    Obtain a simulated dataset from the toy model for the case of two sensitive variables\n",
    "    =====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        N (int): number of individuals in the dataset.\n",
    "        p1 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the first sensitive variable.\n",
    "        p2 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the second sensitive variable.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute.\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create variables\n",
    "    vars = dict()\n",
    "\n",
    "    # Sensitive variables (drawn from a binomial distribution)\n",
    "    vars['sens1'] = np.random.binomial(n = 1, p = p1, size = N)\n",
    "    vars['sens2'] = np.random.binomial(n = 1, p = p2, size = N)\n",
    "\n",
    "    # v1, v2 (noisy measurements of the sensitive variables) and their sum\n",
    "    vars['v1'] = np.random.normal(loc = vars['sens1'], scale = 1.0, size = N)\n",
    "    vars['v2'] = np.random.normal(loc = vars['sens2'], scale = 1.0, size = N)\n",
    "    vars['sum'] = vars['v1'] + vars['v2']\n",
    "\n",
    "    # Noisy measurements of the sum of v1 and v2\n",
    "    vars['indirect'] = np.random.normal(loc = vars['sum'], scale = 1.0, size = N)\n",
    "    vars['weight_response'] = np.random.normal(loc = vars['sum'], scale = 1.0, size = N)\n",
    "\n",
    "    # Response variable\n",
    "    vars['response'] = vars['weight_response'] > 0.0\n",
    "\n",
    "    # Create the dataset with the correct variables\n",
    "    final_vars = ['sens1', 'sens2', 'indirect', 'response']\n",
    "    df = dict()\n",
    "    for name in final_vars:\n",
    "        df[name] = vars[name]\n",
    "    \n",
    "    df['sens1'] = df['sens1'] == 1\n",
    "    df['sens2'] = df['sens2'] == 1\n",
    "\n",
    "    # Apply bitwise operation\n",
    "    if operation == 'OR':\n",
    "        df['prot_attr'] = np.logical_or(df['sens1'], df['sens2'])\n",
    "\n",
    "    elif operation == 'AND':\n",
    "        df['prot_attr'] = np.logical_and(df['sens1'], df['sens2'])\n",
    "\n",
    "    elif operation == 'XOR':\n",
    "        df['prot_attr'] = np.logical_xor(df['sens1'], df['sens2'])\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Convert to standard datasets\n",
    "    data_single = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['sens1'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    data = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['prot_attr'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = data.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    _, vt_single = data_single.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt_single.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = data.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(data)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos diccionarios\n",
    "methods = dict()\n",
    "\n",
    "# RRange of thresholds\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsNames = [\n",
    "    'logreg',\n",
    "    'xgboost'\n",
    "#    'adversarial',\n",
    "#    'metafair',\n",
    "#    'piremover'\n",
    "]\n",
    "\n",
    "modelsTrain = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'xgboost': XGBClassifier\n",
    "#    'adversarial': AdversarialDebiasing,\n",
    "#    'metafair': MetaFairClassifier,\n",
    "#    'piremover': PrejudiceRemover\n",
    "}\n",
    "\n",
    "modelsArgs = {\n",
    "    'logreg': {\n",
    "        'solver': 'liblinear',\n",
    "        'random_state': seed\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'eval_metric': 'error',\n",
    "        'eta':0.1,\n",
    "        'max_depth':6,\n",
    "        'subsample':0.8\n",
    "    }\n",
    "#    'adversarial': {\n",
    "#        'privileged_groups': privileged_groups,\n",
    "#        'unprivileged_groups': unprivileged_groups,\n",
    "#        'scope_name': 'debiased_classifier',\n",
    "#        'debias': True,\n",
    "#        'sess': tf.session(), # Mirar esto de la sesion\n",
    "#        'num_epochs': 80\n",
    "#    },\n",
    "#    'metafair_sr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'sr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "#    'metafair_fdr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'fdr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "#    'pir': {\n",
    "#        'sensitive_attr': sensitive_attribute,\n",
    "#        'eta': 50.0\n",
    "#    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_privileged_groups(dataset, sens_attr_ix=0):\n",
    "    \"\"\"\n",
    "    Helper function to privileged and unprivileged group dictionaries from `dataset`\n",
    "    Args:\n",
    "        dataset (StandardDataset): dataset in the aif360 format.\n",
    "        sens_attr_ix (int, optional): Index of the dataset.privileged_protected_attributes pointing to the sensitive attribute. Defaults to 0.\n",
    "    Returns:\n",
    "        tuple(list, list): privileged group and unprivileged group\n",
    "    \"\"\"\n",
    "\n",
    "    sens_attr = dataset.protected_attribute_names[sens_attr_ix]\n",
    "    priviledged_groups = [{sens_attr: v} for v in dataset.privileged_protected_attributes[sens_attr_ix]]\n",
    "    unpriviledged_groups = [{sens_attr: v} for v in dataset.unprivileged_protected_attributes[sens_attr_ix]]\n",
    "\n",
    "    return priviledged_groups, unpriviledged_groups\n",
    "\n",
    "\n",
    "\n",
    "def results(val, test, method):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep(\n",
    "        dataset=val,\n",
    "        model=methods[method],\n",
    "        thresh_arr=thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(\n",
    "        metrics_sweep[method],\n",
    "        measurement,\n",
    "        combination\n",
    "        )\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics(\n",
    "        dataset=test, \n",
    "        model=methods[method], \n",
    "        threshold=metrics_best_thresh_validate[method]['best_threshold'])\n",
    "    \n",
    "\n",
    "\n",
    "def results_mult(val, val_single, test, test_single, method):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep_mult(\n",
    "        dataset = val,\n",
    "        dataset_single = val_single,\n",
    "        model = methods[method],\n",
    "        thresh_arr = thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(metrics_sweep[method])\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics_mult(\n",
    "        dataset = test, \n",
    "        dataset_single = test_single,\n",
    "        model = methods[method], \n",
    "        threshold = metrics_best_thresh_validate[method]['best_threshold'])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def resultsPost(val, val_preds, test, test_preds, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "    \n",
    "    # Compute the metrics on the validation set\n",
    "    metrics_best_thresh_validate[model_name] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=val, \n",
    "        dataset_preds=val_preds, \n",
    "        model=methods[model_name], \n",
    "        required_threshold=False)\n",
    "    \n",
    "    # Compute the metrics on the test set\n",
    "    metrics_best_thresh_test[model_name] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test, \n",
    "        dataset_preds=test_preds, \n",
    "        model=methods[model_name], \n",
    "        required_threshold=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BenchmarkLogistic(data_train, data_val, data_test):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global nvar\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'logreg'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'sample_weight': train.instance_weights}\n",
    "\n",
    "    # Introduce the model in the model dict\n",
    "    methods[model_name] = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel(), **fit_params)\n",
    "\n",
    "    # Obtain results\n",
    "    if nvar == 1:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "    elif nvar == 2:\n",
    "        global data_val_single\n",
    "        global data_test_single\n",
    "        val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "        results_mult(val, val_single, test, test_single, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BenchmarkXGB(data_train, data_val, data_test):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global nvar\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'xgboost'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "    # Assign the correct dict\n",
    "    methods[model_name] = XGBClassifier(**fit_params)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel())\n",
    "\n",
    "    # Obtain results\n",
    "    if nvar == 1:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "    elif nvar == 2:\n",
    "        global data_val_single\n",
    "        global data_test_single\n",
    "        val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "        results_mult(val, val_single, test, test_single, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True):\n",
    "    #Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"RW\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Call the processor\n",
    "    PreProcessor = Reweighing(\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "    # Transform the data\n",
    "    PreProcessor.fit(train)\n",
    "    trainRW = PreProcessor.transform(train)\n",
    "    valRW = PreProcessor.transform(test)\n",
    "    testRW = PreProcessor.transform(val)\n",
    "\n",
    "    # Train the model\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        modelsArgs[model]['sess'] = tf.Session()\n",
    "\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model in modelsBenchmark:\n",
    "        if model == 'logreg':\n",
    "            fit_params = {'sample_weight': trainRW.instance_weights}\n",
    "            methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel(), **fit_params)\n",
    "        else:\n",
    "            methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel())\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainRW)\n",
    "            \n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(valRW, testRW, model_name)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            global data_val_single\n",
    "            global data_test_single\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(valRW, val_single, testRW, test_single, model_name)\n",
    "\n",
    "    if model == 'adversarial':\n",
    "        modelsArgs[model]['sess'].close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True):\n",
    "    #Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"DI\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Initialize the processor\n",
    "    PreProcessor = DisparateImpactRemover(\n",
    "        repair_level=repair_level,\n",
    "        sensitive_attribute=sensitive_attribute\n",
    "    )\n",
    "    # Transform the data\n",
    "    PreProcessor.fit_transform(train)\n",
    "    trainDI = PreProcessor.fit_transform(train)\n",
    "    valDI = PreProcessor.fit_transform(val)\n",
    "    testDI = PreProcessor.fit_transform(test)\n",
    "\n",
    "    # Train the model\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        modelsArgs[model]['sess'] = tf.Session()\n",
    "\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model in modelsBenchmark:\n",
    "        if model == 'logreg':\n",
    "            fit_params = {'sample_weight': trainDI.instance_weights}\n",
    "            methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel(), **fit_params)\n",
    "        else:\n",
    "            methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel())\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainDI)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(valDI, testDI, model_name)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            global data_val_single\n",
    "            global data_test_single\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(valDI, val_single, testDI, test_single, model_name)\n",
    "\n",
    "    if model == 'adversarial':\n",
    "        modelsArgs[model]['sess'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality,  tau = 0.8, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # assign the correct name\n",
    "    model_name = \"metafair\"\n",
    "    model_name_quality = '{}_{}'.format(model_name, quality)\n",
    "\n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name_quality] = MetaFairClassifier(\n",
    "        tau=tau,\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        type=quality,\n",
    "        seed=seed\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_quality] = methods[model_name_quality].fit(train)\n",
    "\n",
    "    # Obtain scores\n",
    "    methods[model_name_quality].scores_train = methods[model_name_quality].predict(train).scores\n",
    "    methods[model_name_quality].scores_val = methods[model_name_quality].predict(val).scores\n",
    "    methods[model_name_quality].scores_test = methods[model_name_quality].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(val, test, model_name_quality)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            global data_val_single\n",
    "            global data_test_single\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(val, val_single, test, test_single, model_name_quality)\n",
    "\n",
    "\n",
    "\n",
    "def InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'pir'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name] = PrejudiceRemover(\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        eta=eta\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train)\n",
    "    \n",
    "    # Obtain scores\n",
    "    methods[model_name].scores_train = methods[model_name].predict(train).scores\n",
    "    methods[model_name].scores_val = methods[model_name].predict(val).scores\n",
    "    methods[model_name].scores_test = methods[model_name].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "\n",
    "\n",
    "def InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global sess\n",
    "    \n",
    "    # Assign the correct name\n",
    "    model_name = 'adversarial'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    #We train the model\n",
    "    methods[model_name] = AdversarialDebiasing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups,\n",
    "        scope_name = 'debiased_classifier',\n",
    "        debias=True,\n",
    "        sess=sess,\n",
    "        num_epochs=80\n",
    "    )    \n",
    "    methods[model_name].fit(train)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PosprocPlatt(data_train, data_val, data_test, privileged_groups, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "    \n",
    "    # Assign the correct name\n",
    "    fairness_method = '_Platt'\n",
    "\n",
    "    # Validation\n",
    "    #---------------\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy = True), data_val.copy(deepcopy = True), data_test.copy(deepcopy = True)\n",
    "\n",
    "    # Copy the predictions\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Platt Scaling:\n",
    "    #---------------\n",
    "    #1. Split training data on sensitive attribute\n",
    "    val_preds_priv, val_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = val_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    \n",
    "    #2. Copy validation data predictions\n",
    "    val_preds2 = val_preds.copy(deepcopy = True)\n",
    "    \n",
    "    #3. Make one model for each group\n",
    "    sensitive_groups_data = {'priv': [val_preds_priv, priv_indices],\n",
    "                             'unpriv': [val_preds_unpriv, unpriv_indices]}\n",
    "    for group, data_group_list in sensitive_groups_data.items():\n",
    "        # Assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "        # Initialize the model, store it in the dict\n",
    "        methods[model_name_group] = LogisticRegression()\n",
    "        # Train the model using the validation data divided by group\n",
    "        methods[ model_name_group ] = methods[model_name_group].fit(\n",
    "            data_group_list[0].scores,   # data_group_list[0] -> data_val_preds_priv or data_val_preds_unpriv\n",
    "            val.subset(data_group_list[1]).labels.ravel()\n",
    "        ) # data_group_list[1] -> priv_indices or unpriv_indices\n",
    "\n",
    "        # predict group probabilities, store in val_preds2\n",
    "        # Platt scores are given by the predictions of the posterior probabilities\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        val_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "   \n",
    "    # Evaluate the model in a range of values\n",
    "    thresh_sweep_platt = np.linspace(np.min(val_preds2.scores.ravel()),\n",
    "                                     np.max(val_preds2.scores.ravel()),\n",
    "                                     50)\n",
    "\n",
    "    # Obtain the metrics for the val set\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep_from_scores(\n",
    "            dataset_true = val,\n",
    "            dataset_preds = val_preds,\n",
    "            thresh_arr = thresh_sweep_platt\n",
    "        )\n",
    "\n",
    "    # Evaluate metrics and obtain the best thresh\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    # Test\n",
    "    #---------------\n",
    "\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Plat Scaling:\n",
    "    #---------------\n",
    "    \n",
    "    # 1. Divide test set using sensitive varaible's groups\n",
    "    test_preds_priv, test_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = test_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    # 2. Copy test data\n",
    "    test_preds2 = test_preds.copy(deepcopy = True)\n",
    "    \n",
    "    # 3. Predict for each group\n",
    "    sensitive_groups_data_test = {'priv': [test_preds_priv, priv_indices],\n",
    "                                  'unpriv': [test_preds_unpriv, unpriv_indices]}\n",
    "\n",
    "    for group, data_group_list in sensitive_groups_data_test.items():    \n",
    "        # We assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "\n",
    "        # Predict in each group, store the result in data_val_preds2\n",
    "        # The probabilities are the Platt scores\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        test_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "    \n",
    "    # Obtain metrics\n",
    "    metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores(\n",
    "        dataset_true = test,\n",
    "        dataset_pred = test_preds2,\n",
    "        threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold']\n",
    "    )\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate    \n",
    "    \n",
    "    # Assign the correct name\n",
    "    fairness_method = '_eqOdds' \n",
    "\n",
    "    # Copy the dataset\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the predictions of the base model\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Initialize the model and store the predictions\n",
    "    methods[model_name+fairness_method] = EqOddsPostprocessing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups, \n",
    "        seed = seed)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name+fairness_method] = methods[model_name+fairness_method].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true=val,\n",
    "        dataset_preds=val_preds,\n",
    "        model=methods[model_name+fairness_method],\n",
    "        thresh_arr=thresh_sweep,\n",
    "        scores_or_labels='labels'\n",
    "    )\n",
    "\n",
    "    # Evaluate the model for the best threshold\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    # We use the best threshold to obtain predicitions for test\n",
    "    metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test,\n",
    "        dataset_preds=test_preds,\n",
    "        model=methods[model_name+fairness_method], \n",
    "        threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "        scores_or_labels='labels'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name, quality):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate\n",
    "\n",
    "     # Assign the correct name\n",
    "    fairness_method = '_eqOdds'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the model's predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name_metric = model_name + fairness_method + '_' + quality\n",
    "    \n",
    "    # Initialize the model \n",
    "    methods[model_name_metric] = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        cost_constraint=quality,\n",
    "        seed=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model for a range of thresholds\n",
    "    metrics_sweep[model_name_metric] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true = val,\n",
    "        dataset_preds = val_preds,\n",
    "        model = methods[model_name_metric],\n",
    "        thresh_arr = thresh_sweep,\n",
    "        scores_or_labels = 'scores'\n",
    "    )\n",
    "\n",
    "    # Evaluate in best thresh\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.describe_metrics(metrics_sweep[model_name_metric])\n",
    "\n",
    "    # Using the best thresh, evaluate in test\n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test,\n",
    "        dataset_preds=test_preds,\n",
    "        model=methods[model_name_metric], \n",
    "        threshold=metrics_best_thresh_validate[model_name_metric]['best_threshold'], \n",
    "        scores_or_labels='scores'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name, key_metric):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate\n",
    "    global fair_metrics_optrej\n",
    "\n",
    "    # Assign the correct name\n",
    "    fairness_method = '_RejOpt'\n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = RejectOptionClassification(\n",
    "        unprivileged_groups=unprivileged_groups, \n",
    "        privileged_groups=privileged_groups, \n",
    "        metric_name=fair_metrics_optrej[key_metric],\n",
    "        metric_lb=-0.01,\n",
    "        metric_ub=0.01\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "    \n",
    "    # Obtain best threshold in val\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=val, \n",
    "        dataset_preds=val_preds, \n",
    "        model=methods[model_name_metric], \n",
    "        required_threshold=False)\n",
    "    \n",
    "    # Obtain it in test\n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test, \n",
    "        dataset_preds=test_preds, \n",
    "        model=methods[model_name_metric], \n",
    "        required_threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DI remover\n",
    "repair_level = 0.5                      \n",
    "dir_grid = {                           \n",
    "    'repair_level': [0.25, 0.5, 0.75]\n",
    "}                \n",
    "\n",
    "\n",
    "# MetaFair classifier\n",
    "quality_constraints_meta = ['sr', 'fdr']\n",
    "tau = 0.8   \n",
    "metafair_grid = {\n",
    "    'tau': [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Prejudice index regularizer\n",
    "pir_grid = {\n",
    "    'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "}\n",
    "\n",
    "# Adversarial learning\n",
    "pir_grid = {\n",
    "    'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "}\n",
    "\n",
    "# Equal odds\n",
    "# Quality constraints\n",
    "quality_constraints_eqodds = [\"weighted\", 'fnr', 'fpr']\n",
    "\n",
    "# Option rejection\n",
    "# Fairness metrics\n",
    "fair_metrics_optrej = {\n",
    "    'spd': \"Statistical parity difference\",\n",
    "    'aod': \"Average odds difference\",\n",
    "    'eod': \"Equal opportunity difference\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arturo\\AppData\\Local\\Temp\\ipykernel_39696\\2889977420.py:100: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 78.328941; batch adversarial loss: 0.603363\n",
      "epoch 1; iter: 0; batch classifier loss: 65.281548; batch adversarial loss: 0.541025\n",
      "epoch 2; iter: 0; batch classifier loss: 32.843430; batch adversarial loss: 0.521141\n",
      "epoch 3; iter: 0; batch classifier loss: 38.578865; batch adversarial loss: 0.482325\n",
      "epoch 4; iter: 0; batch classifier loss: 36.352013; batch adversarial loss: 0.504410\n",
      "epoch 5; iter: 0; batch classifier loss: 51.872528; batch adversarial loss: 0.501763\n",
      "epoch 6; iter: 0; batch classifier loss: 50.874722; batch adversarial loss: 0.522069\n",
      "epoch 7; iter: 0; batch classifier loss: 40.608467; batch adversarial loss: 0.498124\n",
      "epoch 8; iter: 0; batch classifier loss: 43.048775; batch adversarial loss: 0.531246\n",
      "epoch 9; iter: 0; batch classifier loss: 43.378441; batch adversarial loss: 0.467275\n",
      "epoch 10; iter: 0; batch classifier loss: 42.046997; batch adversarial loss: 0.495165\n",
      "epoch 11; iter: 0; batch classifier loss: 37.851143; batch adversarial loss: 0.460472\n",
      "epoch 12; iter: 0; batch classifier loss: 28.085127; batch adversarial loss: 0.523847\n",
      "epoch 13; iter: 0; batch classifier loss: 33.730457; batch adversarial loss: 0.543853\n",
      "epoch 14; iter: 0; batch classifier loss: 29.669193; batch adversarial loss: 0.473274\n",
      "epoch 15; iter: 0; batch classifier loss: 36.238693; batch adversarial loss: 0.485006\n",
      "epoch 16; iter: 0; batch classifier loss: 14.124228; batch adversarial loss: 0.441798\n",
      "epoch 17; iter: 0; batch classifier loss: 38.286407; batch adversarial loss: 0.498976\n",
      "epoch 18; iter: 0; batch classifier loss: 24.936209; batch adversarial loss: 0.485504\n",
      "epoch 19; iter: 0; batch classifier loss: 20.711714; batch adversarial loss: 0.498422\n",
      "epoch 20; iter: 0; batch classifier loss: 26.800972; batch adversarial loss: 0.461878\n",
      "epoch 21; iter: 0; batch classifier loss: 27.957376; batch adversarial loss: 0.474235\n",
      "epoch 22; iter: 0; batch classifier loss: 22.265509; batch adversarial loss: 0.459994\n",
      "epoch 23; iter: 0; batch classifier loss: 18.280539; batch adversarial loss: 0.496382\n",
      "epoch 24; iter: 0; batch classifier loss: 37.531582; batch adversarial loss: 0.413348\n",
      "epoch 25; iter: 0; batch classifier loss: 16.903168; batch adversarial loss: 0.475116\n",
      "epoch 26; iter: 0; batch classifier loss: 14.595089; batch adversarial loss: 0.477972\n",
      "epoch 27; iter: 0; batch classifier loss: 20.480721; batch adversarial loss: 0.478893\n",
      "epoch 28; iter: 0; batch classifier loss: 14.512852; batch adversarial loss: 0.454733\n",
      "epoch 29; iter: 0; batch classifier loss: 12.870618; batch adversarial loss: 0.506636\n",
      "epoch 30; iter: 0; batch classifier loss: 20.769676; batch adversarial loss: 0.492357\n",
      "epoch 31; iter: 0; batch classifier loss: 14.789041; batch adversarial loss: 0.471367\n",
      "epoch 32; iter: 0; batch classifier loss: 9.164530; batch adversarial loss: 0.483247\n",
      "epoch 33; iter: 0; batch classifier loss: 7.861546; batch adversarial loss: 0.501742\n",
      "epoch 34; iter: 0; batch classifier loss: 8.745699; batch adversarial loss: 0.493950\n",
      "epoch 35; iter: 0; batch classifier loss: 8.394541; batch adversarial loss: 0.465783\n",
      "epoch 36; iter: 0; batch classifier loss: 12.687309; batch adversarial loss: 0.448481\n",
      "epoch 37; iter: 0; batch classifier loss: 8.476727; batch adversarial loss: 0.434176\n",
      "epoch 38; iter: 0; batch classifier loss: 11.601459; batch adversarial loss: 0.488007\n",
      "epoch 39; iter: 0; batch classifier loss: 12.016554; batch adversarial loss: 0.453598\n",
      "epoch 40; iter: 0; batch classifier loss: 13.840254; batch adversarial loss: 0.534115\n",
      "epoch 41; iter: 0; batch classifier loss: 9.408798; batch adversarial loss: 0.480670\n",
      "epoch 42; iter: 0; batch classifier loss: 8.585636; batch adversarial loss: 0.470055\n",
      "epoch 43; iter: 0; batch classifier loss: 5.746333; batch adversarial loss: 0.403989\n",
      "epoch 44; iter: 0; batch classifier loss: 8.201762; batch adversarial loss: 0.474696\n",
      "epoch 45; iter: 0; batch classifier loss: 4.740445; batch adversarial loss: 0.363969\n",
      "epoch 46; iter: 0; batch classifier loss: 5.703025; batch adversarial loss: 0.453157\n",
      "epoch 47; iter: 0; batch classifier loss: 4.914260; batch adversarial loss: 0.455279\n",
      "epoch 48; iter: 0; batch classifier loss: 6.593405; batch adversarial loss: 0.444756\n",
      "epoch 49; iter: 0; batch classifier loss: 5.367754; batch adversarial loss: 0.402827\n",
      "epoch 50; iter: 0; batch classifier loss: 4.128370; batch adversarial loss: 0.438621\n",
      "epoch 51; iter: 0; batch classifier loss: 6.102639; batch adversarial loss: 0.441862\n",
      "epoch 52; iter: 0; batch classifier loss: 3.298930; batch adversarial loss: 0.433908\n",
      "epoch 53; iter: 0; batch classifier loss: 2.271949; batch adversarial loss: 0.406958\n",
      "epoch 54; iter: 0; batch classifier loss: 4.646158; batch adversarial loss: 0.495650\n",
      "epoch 55; iter: 0; batch classifier loss: 4.879381; batch adversarial loss: 0.574803\n",
      "epoch 56; iter: 0; batch classifier loss: 2.238040; batch adversarial loss: 0.401187\n",
      "epoch 57; iter: 0; batch classifier loss: 3.381795; batch adversarial loss: 0.497136\n",
      "epoch 58; iter: 0; batch classifier loss: 3.189019; batch adversarial loss: 0.486733\n",
      "epoch 59; iter: 0; batch classifier loss: 2.542812; batch adversarial loss: 0.496917\n",
      "epoch 60; iter: 0; batch classifier loss: 2.673145; batch adversarial loss: 0.363671\n",
      "epoch 61; iter: 0; batch classifier loss: 2.683569; batch adversarial loss: 0.447862\n",
      "epoch 62; iter: 0; batch classifier loss: 1.531096; batch adversarial loss: 0.517264\n",
      "epoch 63; iter: 0; batch classifier loss: 2.301376; batch adversarial loss: 0.545791\n",
      "epoch 64; iter: 0; batch classifier loss: 1.764860; batch adversarial loss: 0.450804\n",
      "epoch 65; iter: 0; batch classifier loss: 1.303916; batch adversarial loss: 0.449717\n",
      "epoch 66; iter: 0; batch classifier loss: 1.193132; batch adversarial loss: 0.435080\n",
      "epoch 67; iter: 0; batch classifier loss: 2.622313; batch adversarial loss: 0.555331\n",
      "epoch 68; iter: 0; batch classifier loss: 2.123317; batch adversarial loss: 0.555813\n",
      "epoch 69; iter: 0; batch classifier loss: 0.955437; batch adversarial loss: 0.472004\n",
      "epoch 70; iter: 0; batch classifier loss: 1.001638; batch adversarial loss: 0.363477\n",
      "epoch 71; iter: 0; batch classifier loss: 1.138752; batch adversarial loss: 0.380123\n",
      "epoch 72; iter: 0; batch classifier loss: 1.363492; batch adversarial loss: 0.471638\n",
      "epoch 73; iter: 0; batch classifier loss: 1.057160; batch adversarial loss: 0.499345\n",
      "epoch 74; iter: 0; batch classifier loss: 1.366816; batch adversarial loss: 0.519828\n",
      "epoch 75; iter: 0; batch classifier loss: 1.090314; batch adversarial loss: 0.477660\n",
      "epoch 76; iter: 0; batch classifier loss: 1.093919; batch adversarial loss: 0.441956\n",
      "epoch 77; iter: 0; batch classifier loss: 1.361231; batch adversarial loss: 0.511331\n",
      "epoch 78; iter: 0; batch classifier loss: 1.187621; batch adversarial loss: 0.510085\n",
      "epoch 79; iter: 0; batch classifier loss: 0.797087; batch adversarial loss: 0.416805\n",
      "epoch 0; iter: 0; batch classifier loss: 180.048218; batch adversarial loss: 0.694900\n",
      "epoch 1; iter: 0; batch classifier loss: 117.935905; batch adversarial loss: 0.667598\n",
      "epoch 2; iter: 0; batch classifier loss: 48.647282; batch adversarial loss: 0.662179\n",
      "epoch 3; iter: 0; batch classifier loss: 54.043678; batch adversarial loss: 0.651540\n",
      "epoch 4; iter: 0; batch classifier loss: 68.955154; batch adversarial loss: 0.636683\n",
      "epoch 5; iter: 0; batch classifier loss: 45.389458; batch adversarial loss: 0.585837\n",
      "epoch 6; iter: 0; batch classifier loss: 72.939690; batch adversarial loss: 0.647248\n",
      "epoch 7; iter: 0; batch classifier loss: 57.003357; batch adversarial loss: 0.595176\n",
      "epoch 8; iter: 0; batch classifier loss: 54.176311; batch adversarial loss: 0.594867\n",
      "epoch 9; iter: 0; batch classifier loss: 38.896225; batch adversarial loss: 0.658925\n",
      "epoch 10; iter: 0; batch classifier loss: 37.149391; batch adversarial loss: 0.600496\n",
      "epoch 11; iter: 0; batch classifier loss: 52.097015; batch adversarial loss: 0.609415\n",
      "epoch 12; iter: 0; batch classifier loss: 43.707680; batch adversarial loss: 0.589854\n",
      "epoch 13; iter: 0; batch classifier loss: 41.996593; batch adversarial loss: 0.576077\n",
      "epoch 14; iter: 0; batch classifier loss: 47.184601; batch adversarial loss: 0.605708\n",
      "epoch 15; iter: 0; batch classifier loss: 38.039108; batch adversarial loss: 0.589891\n",
      "epoch 16; iter: 0; batch classifier loss: 58.919655; batch adversarial loss: 0.617804\n",
      "epoch 17; iter: 0; batch classifier loss: 45.467602; batch adversarial loss: 0.597586\n",
      "epoch 18; iter: 0; batch classifier loss: 40.695679; batch adversarial loss: 0.588717\n",
      "epoch 19; iter: 0; batch classifier loss: 44.577030; batch adversarial loss: 0.593280\n",
      "epoch 20; iter: 0; batch classifier loss: 50.054131; batch adversarial loss: 0.579362\n",
      "epoch 21; iter: 0; batch classifier loss: 40.576576; batch adversarial loss: 0.583358\n",
      "epoch 22; iter: 0; batch classifier loss: 23.822433; batch adversarial loss: 0.532459\n",
      "epoch 23; iter: 0; batch classifier loss: 40.052013; batch adversarial loss: 0.513795\n",
      "epoch 24; iter: 0; batch classifier loss: 58.252895; batch adversarial loss: 0.546452\n",
      "epoch 25; iter: 0; batch classifier loss: 27.925880; batch adversarial loss: 0.543243\n",
      "epoch 26; iter: 0; batch classifier loss: 35.935402; batch adversarial loss: 0.550644\n",
      "epoch 27; iter: 0; batch classifier loss: 39.896717; batch adversarial loss: 0.548702\n",
      "epoch 28; iter: 0; batch classifier loss: 43.870956; batch adversarial loss: 0.576205\n",
      "epoch 29; iter: 0; batch classifier loss: 41.953846; batch adversarial loss: 0.549547\n",
      "epoch 30; iter: 0; batch classifier loss: 26.053286; batch adversarial loss: 0.530174\n",
      "epoch 31; iter: 0; batch classifier loss: 26.776659; batch adversarial loss: 0.541885\n",
      "epoch 32; iter: 0; batch classifier loss: 29.222507; batch adversarial loss: 0.503024\n",
      "epoch 33; iter: 0; batch classifier loss: 18.988850; batch adversarial loss: 0.523917\n",
      "epoch 34; iter: 0; batch classifier loss: 27.937389; batch adversarial loss: 0.538518\n",
      "epoch 35; iter: 0; batch classifier loss: 25.394810; batch adversarial loss: 0.522447\n",
      "epoch 36; iter: 0; batch classifier loss: 35.032394; batch adversarial loss: 0.543558\n",
      "epoch 37; iter: 0; batch classifier loss: 20.959761; batch adversarial loss: 0.523602\n",
      "epoch 38; iter: 0; batch classifier loss: 23.021915; batch adversarial loss: 0.523569\n",
      "epoch 39; iter: 0; batch classifier loss: 16.286221; batch adversarial loss: 0.489412\n",
      "epoch 40; iter: 0; batch classifier loss: 14.336994; batch adversarial loss: 0.487970\n",
      "epoch 41; iter: 0; batch classifier loss: 19.113731; batch adversarial loss: 0.541932\n",
      "epoch 42; iter: 0; batch classifier loss: 18.448097; batch adversarial loss: 0.491439\n",
      "epoch 43; iter: 0; batch classifier loss: 23.322241; batch adversarial loss: 0.528285\n",
      "epoch 44; iter: 0; batch classifier loss: 22.496475; batch adversarial loss: 0.492411\n",
      "epoch 45; iter: 0; batch classifier loss: 14.915013; batch adversarial loss: 0.494121\n",
      "epoch 46; iter: 0; batch classifier loss: 33.339424; batch adversarial loss: 0.540266\n",
      "epoch 47; iter: 0; batch classifier loss: 17.885220; batch adversarial loss: 0.542082\n",
      "epoch 48; iter: 0; batch classifier loss: 17.497402; batch adversarial loss: 0.526643\n",
      "epoch 49; iter: 0; batch classifier loss: 12.164472; batch adversarial loss: 0.494244\n",
      "epoch 50; iter: 0; batch classifier loss: 15.354761; batch adversarial loss: 0.477603\n",
      "epoch 51; iter: 0; batch classifier loss: 22.007263; batch adversarial loss: 0.499705\n",
      "epoch 52; iter: 0; batch classifier loss: 13.443224; batch adversarial loss: 0.490431\n",
      "epoch 53; iter: 0; batch classifier loss: 13.186993; batch adversarial loss: 0.464605\n",
      "epoch 54; iter: 0; batch classifier loss: 19.802391; batch adversarial loss: 0.451510\n",
      "epoch 55; iter: 0; batch classifier loss: 10.453865; batch adversarial loss: 0.472081\n",
      "epoch 56; iter: 0; batch classifier loss: 12.780128; batch adversarial loss: 0.536556\n",
      "epoch 57; iter: 0; batch classifier loss: 19.341236; batch adversarial loss: 0.493119\n",
      "epoch 58; iter: 0; batch classifier loss: 14.144367; batch adversarial loss: 0.503323\n",
      "epoch 59; iter: 0; batch classifier loss: 10.802844; batch adversarial loss: 0.495162\n",
      "epoch 60; iter: 0; batch classifier loss: 11.132438; batch adversarial loss: 0.474953\n",
      "epoch 61; iter: 0; batch classifier loss: 14.531331; batch adversarial loss: 0.491219\n",
      "epoch 62; iter: 0; batch classifier loss: 9.811480; batch adversarial loss: 0.484732\n",
      "epoch 63; iter: 0; batch classifier loss: 7.137611; batch adversarial loss: 0.442771\n",
      "epoch 64; iter: 0; batch classifier loss: 8.287626; batch adversarial loss: 0.476781\n",
      "epoch 65; iter: 0; batch classifier loss: 10.064068; batch adversarial loss: 0.494086\n",
      "epoch 66; iter: 0; batch classifier loss: 11.076273; batch adversarial loss: 0.485591\n",
      "epoch 67; iter: 0; batch classifier loss: 9.062344; batch adversarial loss: 0.475375\n",
      "epoch 68; iter: 0; batch classifier loss: 10.561882; batch adversarial loss: 0.487851\n",
      "epoch 69; iter: 0; batch classifier loss: 8.552570; batch adversarial loss: 0.460871\n",
      "epoch 70; iter: 0; batch classifier loss: 11.516125; batch adversarial loss: 0.475843\n",
      "epoch 71; iter: 0; batch classifier loss: 9.067474; batch adversarial loss: 0.416956\n",
      "epoch 72; iter: 0; batch classifier loss: 5.856656; batch adversarial loss: 0.463632\n",
      "epoch 73; iter: 0; batch classifier loss: 9.644071; batch adversarial loss: 0.442681\n",
      "epoch 74; iter: 0; batch classifier loss: 5.602356; batch adversarial loss: 0.468537\n",
      "epoch 75; iter: 0; batch classifier loss: 6.206637; batch adversarial loss: 0.483764\n",
      "epoch 76; iter: 0; batch classifier loss: 7.370050; batch adversarial loss: 0.422926\n",
      "epoch 77; iter: 0; batch classifier loss: 4.605620; batch adversarial loss: 0.419131\n",
      "epoch 78; iter: 0; batch classifier loss: 7.759189; batch adversarial loss: 0.430659\n",
      "epoch 79; iter: 0; batch classifier loss: 6.738472; batch adversarial loss: 0.508152\n",
      "epoch 0; iter: 0; batch classifier loss: 91.104210; batch adversarial loss: 0.940103\n",
      "epoch 1; iter: 0; batch classifier loss: 40.527504; batch adversarial loss: 0.893338\n",
      "epoch 2; iter: 0; batch classifier loss: 38.763458; batch adversarial loss: 0.859171\n",
      "epoch 3; iter: 0; batch classifier loss: 46.682587; batch adversarial loss: 0.885710\n",
      "epoch 4; iter: 0; batch classifier loss: 53.867756; batch adversarial loss: 0.895387\n",
      "epoch 5; iter: 0; batch classifier loss: 42.559410; batch adversarial loss: 0.871326\n",
      "epoch 6; iter: 0; batch classifier loss: 34.021935; batch adversarial loss: 0.845149\n",
      "epoch 7; iter: 0; batch classifier loss: 28.957497; batch adversarial loss: 0.863362\n",
      "epoch 8; iter: 0; batch classifier loss: 38.141205; batch adversarial loss: 0.814236\n",
      "epoch 9; iter: 0; batch classifier loss: 27.445145; batch adversarial loss: 0.847229\n",
      "epoch 10; iter: 0; batch classifier loss: 28.286194; batch adversarial loss: 0.845191\n",
      "epoch 11; iter: 0; batch classifier loss: 53.832634; batch adversarial loss: 0.818714\n",
      "epoch 12; iter: 0; batch classifier loss: 27.094349; batch adversarial loss: 0.839934\n",
      "epoch 13; iter: 0; batch classifier loss: 21.267290; batch adversarial loss: 0.843970\n",
      "epoch 14; iter: 0; batch classifier loss: 30.125486; batch adversarial loss: 0.788097\n",
      "epoch 15; iter: 0; batch classifier loss: 28.377283; batch adversarial loss: 0.784370\n",
      "epoch 16; iter: 0; batch classifier loss: 28.408213; batch adversarial loss: 0.758980\n",
      "epoch 17; iter: 0; batch classifier loss: 29.941687; batch adversarial loss: 0.812376\n",
      "epoch 18; iter: 0; batch classifier loss: 26.197216; batch adversarial loss: 0.839535\n",
      "epoch 19; iter: 0; batch classifier loss: 21.812222; batch adversarial loss: 0.814117\n",
      "epoch 20; iter: 0; batch classifier loss: 24.793686; batch adversarial loss: 0.784058\n",
      "epoch 21; iter: 0; batch classifier loss: 14.075241; batch adversarial loss: 0.780017\n",
      "epoch 22; iter: 0; batch classifier loss: 21.947037; batch adversarial loss: 0.752905\n",
      "epoch 23; iter: 0; batch classifier loss: 18.076923; batch adversarial loss: 0.774359\n",
      "epoch 24; iter: 0; batch classifier loss: 19.371420; batch adversarial loss: 0.767043\n",
      "epoch 25; iter: 0; batch classifier loss: 17.867525; batch adversarial loss: 0.757092\n",
      "epoch 26; iter: 0; batch classifier loss: 11.789398; batch adversarial loss: 0.749631\n",
      "epoch 27; iter: 0; batch classifier loss: 15.321573; batch adversarial loss: 0.745238\n",
      "epoch 28; iter: 0; batch classifier loss: 14.966438; batch adversarial loss: 0.745477\n",
      "epoch 29; iter: 0; batch classifier loss: 17.392046; batch adversarial loss: 0.732626\n",
      "epoch 30; iter: 0; batch classifier loss: 9.531084; batch adversarial loss: 0.734709\n",
      "epoch 31; iter: 0; batch classifier loss: 13.433651; batch adversarial loss: 0.744474\n",
      "epoch 32; iter: 0; batch classifier loss: 14.131990; batch adversarial loss: 0.733498\n",
      "epoch 33; iter: 0; batch classifier loss: 17.467636; batch adversarial loss: 0.715310\n",
      "epoch 34; iter: 0; batch classifier loss: 9.060486; batch adversarial loss: 0.696816\n",
      "epoch 35; iter: 0; batch classifier loss: 6.407438; batch adversarial loss: 0.700082\n",
      "epoch 36; iter: 0; batch classifier loss: 6.889146; batch adversarial loss: 0.708375\n",
      "epoch 37; iter: 0; batch classifier loss: 12.311767; batch adversarial loss: 0.721852\n",
      "epoch 38; iter: 0; batch classifier loss: 7.135951; batch adversarial loss: 0.711600\n",
      "epoch 39; iter: 0; batch classifier loss: 8.455289; batch adversarial loss: 0.700680\n",
      "epoch 40; iter: 0; batch classifier loss: 5.008244; batch adversarial loss: 0.703274\n",
      "epoch 41; iter: 0; batch classifier loss: 6.620462; batch adversarial loss: 0.684800\n",
      "epoch 42; iter: 0; batch classifier loss: 6.403584; batch adversarial loss: 0.677241\n",
      "epoch 43; iter: 0; batch classifier loss: 5.941467; batch adversarial loss: 0.681118\n",
      "epoch 44; iter: 0; batch classifier loss: 7.211239; batch adversarial loss: 0.662974\n",
      "epoch 45; iter: 0; batch classifier loss: 5.641667; batch adversarial loss: 0.666943\n",
      "epoch 46; iter: 0; batch classifier loss: 2.444168; batch adversarial loss: 0.672830\n",
      "epoch 47; iter: 0; batch classifier loss: 3.146758; batch adversarial loss: 0.654025\n",
      "epoch 48; iter: 0; batch classifier loss: 2.034317; batch adversarial loss: 0.663229\n",
      "epoch 49; iter: 0; batch classifier loss: 3.098521; batch adversarial loss: 0.663202\n",
      "epoch 50; iter: 0; batch classifier loss: 2.870094; batch adversarial loss: 0.658301\n",
      "epoch 51; iter: 0; batch classifier loss: 2.442497; batch adversarial loss: 0.643125\n",
      "epoch 52; iter: 0; batch classifier loss: 2.405821; batch adversarial loss: 0.641685\n",
      "epoch 53; iter: 0; batch classifier loss: 1.813171; batch adversarial loss: 0.641378\n",
      "epoch 54; iter: 0; batch classifier loss: 1.829720; batch adversarial loss: 0.630308\n",
      "epoch 55; iter: 0; batch classifier loss: 2.198172; batch adversarial loss: 0.634334\n",
      "epoch 56; iter: 0; batch classifier loss: 1.936661; batch adversarial loss: 0.635149\n",
      "epoch 57; iter: 0; batch classifier loss: 2.236106; batch adversarial loss: 0.611057\n",
      "epoch 58; iter: 0; batch classifier loss: 2.394913; batch adversarial loss: 0.610539\n",
      "epoch 59; iter: 0; batch classifier loss: 0.795875; batch adversarial loss: 0.624295\n",
      "epoch 60; iter: 0; batch classifier loss: 1.156939; batch adversarial loss: 0.626516\n",
      "epoch 61; iter: 0; batch classifier loss: 0.975597; batch adversarial loss: 0.616584\n",
      "epoch 62; iter: 0; batch classifier loss: 1.261246; batch adversarial loss: 0.606849\n",
      "epoch 63; iter: 0; batch classifier loss: 1.087383; batch adversarial loss: 0.604511\n",
      "epoch 64; iter: 0; batch classifier loss: 1.076892; batch adversarial loss: 0.607341\n",
      "epoch 65; iter: 0; batch classifier loss: 1.146001; batch adversarial loss: 0.606569\n",
      "epoch 66; iter: 0; batch classifier loss: 0.953043; batch adversarial loss: 0.612697\n",
      "epoch 67; iter: 0; batch classifier loss: 1.155184; batch adversarial loss: 0.605401\n",
      "epoch 68; iter: 0; batch classifier loss: 0.975956; batch adversarial loss: 0.597096\n",
      "epoch 69; iter: 0; batch classifier loss: 1.306636; batch adversarial loss: 0.578675\n",
      "epoch 70; iter: 0; batch classifier loss: 2.218122; batch adversarial loss: 0.590132\n",
      "epoch 71; iter: 0; batch classifier loss: 2.405025; batch adversarial loss: 0.583732\n",
      "epoch 72; iter: 0; batch classifier loss: 2.304676; batch adversarial loss: 0.568489\n",
      "epoch 73; iter: 0; batch classifier loss: 1.631659; batch adversarial loss: 0.578051\n",
      "epoch 74; iter: 0; batch classifier loss: 5.586250; batch adversarial loss: 0.579836\n",
      "epoch 75; iter: 0; batch classifier loss: 1.082022; batch adversarial loss: 0.592076\n",
      "epoch 76; iter: 0; batch classifier loss: 1.630156; batch adversarial loss: 0.563328\n",
      "epoch 77; iter: 0; batch classifier loss: 2.340863; batch adversarial loss: 0.563352\n",
      "epoch 78; iter: 0; batch classifier loss: 0.878036; batch adversarial loss: 0.579121\n",
      "epoch 79; iter: 0; batch classifier loss: 1.040131; batch adversarial loss: 0.575547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>independence</th>\n",
       "      <th>separation</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.212779</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg</th>\n",
       "      <td>0.191837</td>\n",
       "      <td>0.541119</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>0.748465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg</th>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.666801</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.255583</td>\n",
       "      <td>0.254205</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.749444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost</th>\n",
       "      <td>0.131224</td>\n",
       "      <td>0.551323</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.137006</td>\n",
       "      <td>0.736582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir_eqOdds_fnr</th>\n",
       "      <td>0.676735</td>\n",
       "      <td>0.668418</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>0.200653</td>\n",
       "      <td>0.322785</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir_eqOdds_fpr</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.721257</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.236175</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir_RejOpt_spd</th>\n",
       "      <td>0.534646</td>\n",
       "      <td>0.652960</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.228591</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir_RejOpt_aod</th>\n",
       "      <td>0.574242</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.091110</td>\n",
       "      <td>0.206169</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir_RejOpt_eod</th>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.678925</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.213710</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                best_threshold   bal_acc       acc  independence  separation  \\\n",
       "logreg                0.777755  0.711053  0.653333      0.212779    0.179526   \n",
       "xgboost               0.757551  0.699333  0.680000      0.109181    0.045250   \n",
       "RW_logreg             0.191837  0.541119  0.693333      0.009926    0.000893   \n",
       "DI_logreg             0.838367  0.666801  0.586667      0.255583    0.254205   \n",
       "RW_xgboost            0.131224  0.551323  0.700000      0.028536    0.040774   \n",
       "...                        ...       ...       ...           ...         ...   \n",
       "pir_eqOdds_fnr        0.676735  0.668418  0.666667      0.318519    0.200653   \n",
       "pir_eqOdds_fpr        0.777755  0.721257  0.660000      0.444444    0.236175   \n",
       "pir_RejOpt_spd        0.534646  0.652960  0.660000      0.103704    0.228591   \n",
       "pir_RejOpt_aod        0.574242  0.678319  0.680000      0.037037    0.091110   \n",
       "pir_RejOpt_eod        0.663333  0.678925  0.666667      0.288889    0.213710   \n",
       "\n",
       "                sufficiency       auc  \n",
       "logreg             0.260606  0.745201  \n",
       "xgboost            0.136364  0.756112  \n",
       "RW_logreg          0.160339  0.748465  \n",
       "DI_logreg          0.601449  0.749444  \n",
       "RW_xgboost         0.137006  0.736582  \n",
       "...                     ...       ...  \n",
       "pir_eqOdds_fnr     0.322785  0.733279  \n",
       "pir_eqOdds_fpr     0.916667  0.733279  \n",
       "pir_RejOpt_spd     0.328947  0.733279  \n",
       "pir_RejOpt_aod     0.206169  0.733279  \n",
       "pir_RejOpt_eod     0.340000  0.733279  \n",
       "\n",
       "[80 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvar = 2\n",
    " \n",
    "measurement = 'combination'\n",
    "combination = ['bal_acc', 'separation']\n",
    " \n",
    "data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single = GermanDataset2V()\n",
    " \n",
    "modelsNames = [\n",
    "    'logreg',\n",
    "    'xgboost',\n",
    "    'adversarial',\n",
    "    'metafair',\n",
    "    'pir'\n",
    "]\n",
    "\n",
    "\n",
    "modelsBenchmark = [\n",
    "    'logreg',\n",
    "    'xgboost'\n",
    "]\n",
    "\n",
    "modelsFair = [\n",
    "    'adversarial',\n",
    "    'metafair_sr',\n",
    "    'metafair_fdr',\n",
    "    'pir'\n",
    "]\n",
    "\n",
    "modelsPre = [\n",
    "    prefix + '_' + model_name for prefix in ['RW', 'DI'] for model_name in modelsBenchmark\n",
    "]\n",
    "\n",
    "\n",
    "modelsPost = modelsPre + modelsFair\n",
    "\n",
    "\n",
    "modelsTrain = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'xgboost': XGBClassifier,\n",
    "    'adversarial': AdversarialDebiasing,\n",
    "    'metafair': MetaFairClassifier,\n",
    "    'pir': PrejudiceRemover\n",
    "}\n",
    "\n",
    "modelsArgs = {\n",
    "    'logreg': {\n",
    "        'solver': 'liblinear',\n",
    "        'random_state': seed\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'eval_metric': 'error',\n",
    "        'eta':0.1,\n",
    "        'max_depth':6,\n",
    "        'subsample':0.8\n",
    "    },\n",
    "    'adversarial': {\n",
    "        'privileged_groups': privileged_groups,\n",
    "        'unprivileged_groups': unprivileged_groups,\n",
    "        'scope_name': 'debiased_classifier',\n",
    "        'debias': True,\n",
    "        'num_epochs': 80\n",
    "    },\n",
    "    'metafair': {\n",
    "        'tau': 0.8,\n",
    "        'sensitive_attr': sensitive_attribute,\n",
    "        'type': 'sr',\n",
    "        'seed': seed\n",
    "    },\n",
    "#    'metafair_fdr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'fdr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "    'pir': {\n",
    "        'sensitive_attr': sensitive_attribute,\n",
    "        'eta': 50.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing + In processing\n",
    "for model in modelsNames:\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "\n",
    "    if model == 'adversarial':\n",
    "        sess.close()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        sess = tf.compat.v1.Session()\n",
    "\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "    \n",
    "    if model == 'adversarial':\n",
    "        sess.close()\n",
    "\n",
    "\n",
    "# Pre/In processing + Post processing\n",
    "\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "\n",
    "for model in modelsPost:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "        \n",
    "sess.close()\n",
    "\n",
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 78.328941; batch adversarial loss: 0.583292\n",
      "epoch 1; iter: 0; batch classifier loss: 65.280716; batch adversarial loss: 0.564662\n",
      "epoch 2; iter: 0; batch classifier loss: 32.841087; batch adversarial loss: 0.517795\n",
      "epoch 3; iter: 0; batch classifier loss: 38.843472; batch adversarial loss: 0.526234\n",
      "epoch 4; iter: 0; batch classifier loss: 36.136360; batch adversarial loss: 0.543333\n",
      "epoch 5; iter: 0; batch classifier loss: 51.419430; batch adversarial loss: 0.534661\n",
      "epoch 6; iter: 0; batch classifier loss: 50.930008; batch adversarial loss: 0.572119\n",
      "epoch 7; iter: 0; batch classifier loss: 40.479343; batch adversarial loss: 0.534250\n",
      "epoch 8; iter: 0; batch classifier loss: 43.165482; batch adversarial loss: 0.562091\n",
      "epoch 9; iter: 0; batch classifier loss: 42.964333; batch adversarial loss: 0.521938\n",
      "epoch 10; iter: 0; batch classifier loss: 42.900841; batch adversarial loss: 0.545499\n",
      "epoch 11; iter: 0; batch classifier loss: 38.518967; batch adversarial loss: 0.542401\n",
      "epoch 12; iter: 0; batch classifier loss: 27.839470; batch adversarial loss: 0.543198\n",
      "epoch 13; iter: 0; batch classifier loss: 31.554300; batch adversarial loss: 0.571741\n",
      "epoch 14; iter: 0; batch classifier loss: 30.571178; batch adversarial loss: 0.516164\n",
      "epoch 15; iter: 0; batch classifier loss: 38.042793; batch adversarial loss: 0.550371\n",
      "epoch 16; iter: 0; batch classifier loss: 13.710626; batch adversarial loss: 0.498519\n",
      "epoch 17; iter: 0; batch classifier loss: 38.336700; batch adversarial loss: 0.512246\n",
      "epoch 18; iter: 0; batch classifier loss: 28.114685; batch adversarial loss: 0.486323\n",
      "epoch 19; iter: 0; batch classifier loss: 21.125179; batch adversarial loss: 0.503528\n",
      "epoch 20; iter: 0; batch classifier loss: 27.458620; batch adversarial loss: 0.533948\n",
      "epoch 21; iter: 0; batch classifier loss: 34.298561; batch adversarial loss: 0.643572\n",
      "epoch 22; iter: 0; batch classifier loss: 21.465538; batch adversarial loss: 0.525651\n",
      "epoch 23; iter: 0; batch classifier loss: 22.913225; batch adversarial loss: 0.481093\n",
      "epoch 24; iter: 0; batch classifier loss: 48.859673; batch adversarial loss: 0.437825\n",
      "epoch 25; iter: 0; batch classifier loss: 18.205744; batch adversarial loss: 0.529604\n",
      "epoch 26; iter: 0; batch classifier loss: 17.190975; batch adversarial loss: 0.585074\n",
      "epoch 27; iter: 0; batch classifier loss: 24.642925; batch adversarial loss: 0.526083\n",
      "epoch 28; iter: 0; batch classifier loss: 16.251045; batch adversarial loss: 0.531540\n",
      "epoch 29; iter: 0; batch classifier loss: 14.806496; batch adversarial loss: 0.517058\n",
      "epoch 30; iter: 0; batch classifier loss: 22.098057; batch adversarial loss: 0.492181\n",
      "epoch 31; iter: 0; batch classifier loss: 16.675415; batch adversarial loss: 0.492204\n",
      "epoch 32; iter: 0; batch classifier loss: 11.456402; batch adversarial loss: 0.559214\n",
      "epoch 33; iter: 0; batch classifier loss: 9.491199; batch adversarial loss: 0.538045\n",
      "epoch 34; iter: 0; batch classifier loss: 10.278039; batch adversarial loss: 0.467860\n",
      "epoch 35; iter: 0; batch classifier loss: 8.515280; batch adversarial loss: 0.493449\n",
      "epoch 36; iter: 0; batch classifier loss: 16.868366; batch adversarial loss: 0.501493\n",
      "epoch 37; iter: 0; batch classifier loss: 10.556383; batch adversarial loss: 0.491452\n",
      "epoch 38; iter: 0; batch classifier loss: 13.983143; batch adversarial loss: 0.565539\n",
      "epoch 39; iter: 0; batch classifier loss: 17.069645; batch adversarial loss: 0.483590\n",
      "epoch 40; iter: 0; batch classifier loss: 10.388662; batch adversarial loss: 0.532287\n",
      "epoch 41; iter: 0; batch classifier loss: 10.935918; batch adversarial loss: 0.476801\n",
      "epoch 42; iter: 0; batch classifier loss: 11.346522; batch adversarial loss: 0.539418\n",
      "epoch 43; iter: 0; batch classifier loss: 7.245730; batch adversarial loss: 0.431304\n",
      "epoch 44; iter: 0; batch classifier loss: 10.076423; batch adversarial loss: 0.447874\n",
      "epoch 45; iter: 0; batch classifier loss: 6.745779; batch adversarial loss: 0.486339\n",
      "epoch 46; iter: 0; batch classifier loss: 6.041595; batch adversarial loss: 0.471248\n",
      "epoch 47; iter: 0; batch classifier loss: 5.273021; batch adversarial loss: 0.454857\n",
      "epoch 48; iter: 0; batch classifier loss: 8.158537; batch adversarial loss: 0.539838\n",
      "epoch 49; iter: 0; batch classifier loss: 7.278372; batch adversarial loss: 0.530883\n",
      "epoch 50; iter: 0; batch classifier loss: 6.681669; batch adversarial loss: 0.484169\n",
      "epoch 51; iter: 0; batch classifier loss: 7.803533; batch adversarial loss: 0.463405\n",
      "epoch 52; iter: 0; batch classifier loss: 4.490322; batch adversarial loss: 0.469284\n",
      "epoch 53; iter: 0; batch classifier loss: 3.856228; batch adversarial loss: 0.471797\n",
      "epoch 54; iter: 0; batch classifier loss: 6.301878; batch adversarial loss: 0.441704\n",
      "epoch 55; iter: 0; batch classifier loss: 6.398008; batch adversarial loss: 0.600650\n",
      "epoch 56; iter: 0; batch classifier loss: 3.329650; batch adversarial loss: 0.465989\n",
      "epoch 57; iter: 0; batch classifier loss: 4.399462; batch adversarial loss: 0.490989\n",
      "epoch 58; iter: 0; batch classifier loss: 3.086243; batch adversarial loss: 0.432370\n",
      "epoch 59; iter: 0; batch classifier loss: 4.015261; batch adversarial loss: 0.512496\n",
      "epoch 60; iter: 0; batch classifier loss: 3.599471; batch adversarial loss: 0.467172\n",
      "epoch 61; iter: 0; batch classifier loss: 3.825002; batch adversarial loss: 0.568162\n",
      "epoch 62; iter: 0; batch classifier loss: 2.108548; batch adversarial loss: 0.530323\n",
      "epoch 63; iter: 0; batch classifier loss: 1.746735; batch adversarial loss: 0.479370\n",
      "epoch 64; iter: 0; batch classifier loss: 2.659571; batch adversarial loss: 0.485587\n",
      "epoch 65; iter: 0; batch classifier loss: 3.440312; batch adversarial loss: 0.458340\n",
      "epoch 66; iter: 0; batch classifier loss: 2.365382; batch adversarial loss: 0.437671\n",
      "epoch 67; iter: 0; batch classifier loss: 5.724104; batch adversarial loss: 0.579251\n",
      "epoch 68; iter: 0; batch classifier loss: 1.547963; batch adversarial loss: 0.439477\n",
      "epoch 69; iter: 0; batch classifier loss: 1.751155; batch adversarial loss: 0.461705\n",
      "epoch 70; iter: 0; batch classifier loss: 1.419798; batch adversarial loss: 0.423028\n",
      "epoch 71; iter: 0; batch classifier loss: 1.729592; batch adversarial loss: 0.470225\n",
      "epoch 72; iter: 0; batch classifier loss: 2.283039; batch adversarial loss: 0.404203\n",
      "epoch 73; iter: 0; batch classifier loss: 1.640347; batch adversarial loss: 0.446530\n",
      "epoch 74; iter: 0; batch classifier loss: 5.419665; batch adversarial loss: 0.604074\n",
      "epoch 75; iter: 0; batch classifier loss: 1.398837; batch adversarial loss: 0.428638\n",
      "epoch 76; iter: 0; batch classifier loss: 1.546277; batch adversarial loss: 0.375861\n",
      "epoch 77; iter: 0; batch classifier loss: 4.889842; batch adversarial loss: 0.595225\n",
      "epoch 78; iter: 0; batch classifier loss: 1.514507; batch adversarial loss: 0.464858\n",
      "epoch 79; iter: 0; batch classifier loss: 1.226492; batch adversarial loss: 0.503316\n",
      "epoch 0; iter: 0; batch classifier loss: 180.047256; batch adversarial loss: 0.694927\n",
      "epoch 1; iter: 0; batch classifier loss: 176.272186; batch adversarial loss: 0.681350\n",
      "epoch 2; iter: 0; batch classifier loss: 85.189774; batch adversarial loss: 0.690997\n",
      "epoch 3; iter: 0; batch classifier loss: 57.473122; batch adversarial loss: 0.674780\n",
      "epoch 4; iter: 0; batch classifier loss: 64.238350; batch adversarial loss: 0.662512\n",
      "epoch 5; iter: 0; batch classifier loss: 47.356689; batch adversarial loss: 0.593786\n",
      "epoch 6; iter: 0; batch classifier loss: 77.838791; batch adversarial loss: 0.695209\n",
      "epoch 7; iter: 0; batch classifier loss: 56.932621; batch adversarial loss: 0.619756\n",
      "epoch 8; iter: 0; batch classifier loss: 57.835945; batch adversarial loss: 0.608215\n",
      "epoch 9; iter: 0; batch classifier loss: 37.640877; batch adversarial loss: 0.655025\n",
      "epoch 10; iter: 0; batch classifier loss: 34.419022; batch adversarial loss: 0.583571\n",
      "epoch 11; iter: 0; batch classifier loss: 49.063423; batch adversarial loss: 0.638040\n",
      "epoch 12; iter: 0; batch classifier loss: 42.066410; batch adversarial loss: 0.611778\n",
      "epoch 13; iter: 0; batch classifier loss: 39.430618; batch adversarial loss: 0.579843\n",
      "epoch 14; iter: 0; batch classifier loss: 45.241318; batch adversarial loss: 0.635577\n",
      "epoch 15; iter: 0; batch classifier loss: 38.292915; batch adversarial loss: 0.565585\n",
      "epoch 16; iter: 0; batch classifier loss: 62.000629; batch adversarial loss: 0.634181\n",
      "epoch 17; iter: 0; batch classifier loss: 44.704063; batch adversarial loss: 0.621989\n",
      "epoch 18; iter: 0; batch classifier loss: 41.596302; batch adversarial loss: 0.636569\n",
      "epoch 19; iter: 0; batch classifier loss: 44.095184; batch adversarial loss: 0.617996\n",
      "epoch 20; iter: 0; batch classifier loss: 51.713005; batch adversarial loss: 0.611121\n",
      "epoch 21; iter: 0; batch classifier loss: 43.452953; batch adversarial loss: 0.598754\n",
      "epoch 22; iter: 0; batch classifier loss: 24.182617; batch adversarial loss: 0.574268\n",
      "epoch 23; iter: 0; batch classifier loss: 38.081417; batch adversarial loss: 0.581347\n",
      "epoch 24; iter: 0; batch classifier loss: 53.996868; batch adversarial loss: 0.585621\n",
      "epoch 25; iter: 0; batch classifier loss: 28.806532; batch adversarial loss: 0.546526\n",
      "epoch 26; iter: 0; batch classifier loss: 37.239250; batch adversarial loss: 0.594021\n",
      "epoch 27; iter: 0; batch classifier loss: 42.584900; batch adversarial loss: 0.572538\n",
      "epoch 28; iter: 0; batch classifier loss: 42.927330; batch adversarial loss: 0.602339\n",
      "epoch 29; iter: 0; batch classifier loss: 43.075809; batch adversarial loss: 0.572691\n",
      "epoch 30; iter: 0; batch classifier loss: 27.586168; batch adversarial loss: 0.541850\n",
      "epoch 31; iter: 0; batch classifier loss: 27.327076; batch adversarial loss: 0.561262\n",
      "epoch 32; iter: 0; batch classifier loss: 29.511589; batch adversarial loss: 0.536020\n",
      "epoch 33; iter: 0; batch classifier loss: 17.950939; batch adversarial loss: 0.574407\n",
      "epoch 34; iter: 0; batch classifier loss: 27.276236; batch adversarial loss: 0.555920\n",
      "epoch 35; iter: 0; batch classifier loss: 25.943119; batch adversarial loss: 0.552598\n",
      "epoch 36; iter: 0; batch classifier loss: 35.986340; batch adversarial loss: 0.557045\n",
      "epoch 37; iter: 0; batch classifier loss: 21.636055; batch adversarial loss: 0.530741\n",
      "epoch 38; iter: 0; batch classifier loss: 24.208921; batch adversarial loss: 0.540326\n",
      "epoch 39; iter: 0; batch classifier loss: 16.465736; batch adversarial loss: 0.558565\n",
      "epoch 40; iter: 0; batch classifier loss: 15.253766; batch adversarial loss: 0.524898\n",
      "epoch 41; iter: 0; batch classifier loss: 20.261576; batch adversarial loss: 0.587531\n",
      "epoch 42; iter: 0; batch classifier loss: 19.300600; batch adversarial loss: 0.530069\n",
      "epoch 43; iter: 0; batch classifier loss: 23.701555; batch adversarial loss: 0.535495\n",
      "epoch 44; iter: 0; batch classifier loss: 24.596004; batch adversarial loss: 0.558454\n",
      "epoch 45; iter: 0; batch classifier loss: 15.771265; batch adversarial loss: 0.547998\n",
      "epoch 46; iter: 0; batch classifier loss: 34.491585; batch adversarial loss: 0.554586\n",
      "epoch 47; iter: 0; batch classifier loss: 19.252678; batch adversarial loss: 0.568182\n",
      "epoch 48; iter: 0; batch classifier loss: 18.077250; batch adversarial loss: 0.572337\n",
      "epoch 49; iter: 0; batch classifier loss: 12.831244; batch adversarial loss: 0.538184\n",
      "epoch 50; iter: 0; batch classifier loss: 14.770988; batch adversarial loss: 0.527452\n",
      "epoch 51; iter: 0; batch classifier loss: 22.039639; batch adversarial loss: 0.528634\n",
      "epoch 52; iter: 0; batch classifier loss: 14.208361; batch adversarial loss: 0.524079\n",
      "epoch 53; iter: 0; batch classifier loss: 12.898199; batch adversarial loss: 0.561958\n",
      "epoch 54; iter: 0; batch classifier loss: 18.162567; batch adversarial loss: 0.486166\n",
      "epoch 55; iter: 0; batch classifier loss: 10.889892; batch adversarial loss: 0.525849\n",
      "epoch 56; iter: 0; batch classifier loss: 14.437555; batch adversarial loss: 0.560560\n",
      "epoch 57; iter: 0; batch classifier loss: 19.994617; batch adversarial loss: 0.514608\n",
      "epoch 58; iter: 0; batch classifier loss: 14.728582; batch adversarial loss: 0.522673\n",
      "epoch 59; iter: 0; batch classifier loss: 11.903090; batch adversarial loss: 0.492186\n",
      "epoch 60; iter: 0; batch classifier loss: 12.263661; batch adversarial loss: 0.546711\n",
      "epoch 61; iter: 0; batch classifier loss: 14.554187; batch adversarial loss: 0.520550\n",
      "epoch 62; iter: 0; batch classifier loss: 10.215115; batch adversarial loss: 0.548965\n",
      "epoch 63; iter: 0; batch classifier loss: 9.279319; batch adversarial loss: 0.495835\n",
      "epoch 64; iter: 0; batch classifier loss: 9.228930; batch adversarial loss: 0.522287\n",
      "epoch 65; iter: 0; batch classifier loss: 10.886311; batch adversarial loss: 0.551922\n",
      "epoch 66; iter: 0; batch classifier loss: 10.597816; batch adversarial loss: 0.528149\n",
      "epoch 67; iter: 0; batch classifier loss: 8.934147; batch adversarial loss: 0.522333\n",
      "epoch 68; iter: 0; batch classifier loss: 11.435581; batch adversarial loss: 0.521905\n",
      "epoch 69; iter: 0; batch classifier loss: 9.598458; batch adversarial loss: 0.518525\n",
      "epoch 70; iter: 0; batch classifier loss: 11.696688; batch adversarial loss: 0.564331\n",
      "epoch 71; iter: 0; batch classifier loss: 9.756066; batch adversarial loss: 0.466433\n",
      "epoch 72; iter: 0; batch classifier loss: 6.285099; batch adversarial loss: 0.494071\n",
      "epoch 73; iter: 0; batch classifier loss: 9.783995; batch adversarial loss: 0.480173\n",
      "epoch 74; iter: 0; batch classifier loss: 6.660805; batch adversarial loss: 0.517904\n",
      "epoch 75; iter: 0; batch classifier loss: 6.652174; batch adversarial loss: 0.510721\n",
      "epoch 76; iter: 0; batch classifier loss: 7.812017; batch adversarial loss: 0.490919\n",
      "epoch 77; iter: 0; batch classifier loss: 5.547711; batch adversarial loss: 0.496758\n",
      "epoch 78; iter: 0; batch classifier loss: 8.193098; batch adversarial loss: 0.504835\n",
      "epoch 79; iter: 0; batch classifier loss: 7.642372; batch adversarial loss: 0.503252\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable debiased_classifier/classifier_model/W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m     InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m, do_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    104\u001b[0m InprocPI(data_train, data_val, data_test, sensitive_attribute, eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50.0\u001b[39m, do_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 105\u001b[0m \u001b[43mInprocAdvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munprivileged_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m modelsPost:\n\u001b[0;32m    109\u001b[0m     PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
      "Cell \u001b[1;32mIn[9], line 91\u001b[0m, in \u001b[0;36mInprocAdvs\u001b[1;34m(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#We train the model\u001b[39;00m\n\u001b[0;32m     83\u001b[0m methods[model_name] \u001b[38;5;241m=\u001b[39m AdversarialDebiasing(\n\u001b[0;32m     84\u001b[0m     privileged_groups \u001b[38;5;241m=\u001b[39m privileged_groups,\n\u001b[0;32m     85\u001b[0m     unprivileged_groups \u001b[38;5;241m=\u001b[39m unprivileged_groups,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m\n\u001b[0;32m     90\u001b[0m )    \n\u001b[1;32m---> 91\u001b[0m \u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Obtain results\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_results:\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:152\u001b[0m, in \u001b[0;36mAdversarialDebiasing.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_prob \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Obtain classifier predictions and classifier loss\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_labels, pred_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_classifier_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_ph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m pred_labels_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msigmoid_cross_entropy_with_logits(labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_labels_ph, logits\u001b[38;5;241m=\u001b[39mpred_logits))\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Obtain adversary predictions and adversary loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:84\u001b[0m, in \u001b[0;36mAdversarialDebiasing._classifier_model\u001b[1;34m(self, features, features_dim, keep_prob)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the classifier predictions for the outcome variable.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mvariable_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_model\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     W1 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_num_hidden_units\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                          \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglorot_uniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     b1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(tf\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_num_hidden_units]), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     88\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(tf\u001b[38;5;241m.\u001b[39mmatmul(features, W1) \u001b[38;5;241m+\u001b[39m b1)\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1565\u001b[0m, in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_variable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variable\u001b[39m(name,\n\u001b[0;32m   1551\u001b[0m                  shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1563\u001b[0m                  synchronization\u001b[38;5;241m=\u001b[39mVariableSynchronization\u001b[38;5;241m.\u001b[39mAUTO,\n\u001b[0;32m   1564\u001b[0m                  aggregation\u001b[38;5;241m=\u001b[39mVariableAggregation\u001b[38;5;241m.\u001b[39mNONE):\n\u001b[1;32m-> 1565\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_variable_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_get_default_variable_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m      \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcustom_getter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1275\u001b[0m, in \u001b[0;36mVariableScope.get_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1274\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype\n\u001b[1;32m-> 1275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_getter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:520\u001b[0m, in \u001b[0;36m_VariableStore.get_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    518\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m custom_getter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom_getter_kwargs)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_true_getter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m      \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:473\u001b[0m, in \u001b[0;36m_VariableStore.get_variable.<locals>._true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/part_0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vars:\n\u001b[0;32m    468\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    469\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo partitioner was provided, but a partitioned version of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable was found: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/part_0. Perhaps a variable of the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname was already created with partitioning?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m--> 473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_single_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:831\u001b[0m, in \u001b[0;36m_VariableStore._get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# ResourceVariables don't have an op associated with so no traceback\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(var, resource_variable_ops\u001b[38;5;241m.\u001b[39mResourceVariable):\n\u001b[1;32m--> 831\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m    832\u001b[0m tb \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mtraceback[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# Throw away internal tf entries and only take a few lines. In some\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# cases the traceback can be longer (e.g. if someone uses factory\u001b[39;00m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# functions to create variables) so we take more than needed in the\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# default case.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable debiased_classifier/classifier_model/W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": [
    "nvar = 1\n",
    " \n",
    "measurement = 'combination'\n",
    "combination = ['bal_acc', 'separation']\n",
    " \n",
    "data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups = GermanDataset1V()\n",
    " \n",
    "modelsNames = [\n",
    "    'logreg',\n",
    "    'xgboost',\n",
    "    'adversarial',\n",
    "    'metafair',\n",
    "    'pir'\n",
    "]\n",
    "\n",
    "\n",
    "modelsBenchmark = [\n",
    "    'logreg',\n",
    "    'xgboost'\n",
    "]\n",
    "\n",
    "modelsFair = [\n",
    "    'adversarial',\n",
    "    'metafair',\n",
    "    'pir'\n",
    "]\n",
    "\n",
    "modelsPre = [\n",
    "    prefix + '_' + model_name for prefix in ['RW', 'DI'] for model_name in modelsBenchmark\n",
    "]\n",
    "\n",
    "modelsPost = modelsPre + modelsFair\n",
    "\n",
    "\n",
    "modelsTrain = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'xgboost': XGBClassifier,\n",
    "    'adversarial': AdversarialDebiasing,\n",
    "    'metafair': MetaFairClassifier,\n",
    "    'pir': PrejudiceRemover\n",
    "}\n",
    "\n",
    "modelsArgs = {\n",
    "    'logreg': {\n",
    "        'solver': 'liblinear',\n",
    "        'random_state': seed\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'eval_metric': 'error',\n",
    "        'eta':0.1,\n",
    "        'max_depth':6,\n",
    "        'subsample':0.8\n",
    "    },\n",
    "    'adversarial': {\n",
    "        'privileged_groups': privileged_groups,\n",
    "        'unprivileged_groups': unprivileged_groups,\n",
    "        'scope_name': 'debiased_classifier',\n",
    "        'debias': True,\n",
    "        'num_epochs': 80\n",
    "    },\n",
    "    'metafair': {\n",
    "        'tau': 0.8,\n",
    "        'sensitive_attr': sensitive_attribute,\n",
    "        'type': 'sr',\n",
    "        'seed': seed\n",
    "    },\n",
    "#    'metafair_fdr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'fdr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "    'pir': {\n",
    "        'sensitive_attr': sensitive_attribute,\n",
    "        'eta': 50.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing + In processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "# Pre/In processing + Post processing\n",
    "\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "\n",
    "for model in modelsPost:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "\n",
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>independence</th>\n",
       "      <th>separation</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.212779</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg</th>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.661063</td>\n",
       "      <td>0.658679</td>\n",
       "      <td>0.156145</td>\n",
       "      <td>0.135985</td>\n",
       "      <td>0.219985</td>\n",
       "      <td>0.743514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg</th>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.676702</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.178660</td>\n",
       "      <td>0.182776</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.744191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost</th>\n",
       "      <td>0.878776</td>\n",
       "      <td>0.659640</td>\n",
       "      <td>0.622095</td>\n",
       "      <td>0.241415</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.736582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.703374</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.249480</td>\n",
       "      <td>0.757325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_adversarial</th>\n",
       "      <td>0.090816</td>\n",
       "      <td>0.506168</td>\n",
       "      <td>0.357673</td>\n",
       "      <td>0.188780</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.527233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_adversarial</th>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.542029</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>0.131335</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.467367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_metafair</th>\n",
       "      <td>0.636327</td>\n",
       "      <td>0.663103</td>\n",
       "      <td>0.674318</td>\n",
       "      <td>0.191546</td>\n",
       "      <td>0.169192</td>\n",
       "      <td>0.266712</td>\n",
       "      <td>0.692216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_metafair</th>\n",
       "      <td>0.636327</td>\n",
       "      <td>0.676399</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.156328</td>\n",
       "      <td>0.139296</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.728026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_pir</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.679979</td>\n",
       "      <td>0.658759</td>\n",
       "      <td>0.143229</td>\n",
       "      <td>0.105808</td>\n",
       "      <td>0.180574</td>\n",
       "      <td>0.747871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_pir</th>\n",
       "      <td>0.858571</td>\n",
       "      <td>0.632148</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.245658</td>\n",
       "      <td>0.255436</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.720752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_sr</th>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.616791</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.218983</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>0.298108</td>\n",
       "      <td>0.653869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_fdr</th>\n",
       "      <td>0.636327</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.094583</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.728430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir</th>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.686603</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.287841</td>\n",
       "      <td>0.277193</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.722772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adversarial</th>\n",
       "      <td>0.939388</td>\n",
       "      <td>0.492524</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.070099</td>\n",
       "      <td>0.129044</td>\n",
       "      <td>0.234610</td>\n",
       "      <td>0.427662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_eqOdds</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.689735</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.161911</td>\n",
       "      <td>0.118698</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_eqOdds_weighted</th>\n",
       "      <td>0.454490</td>\n",
       "      <td>0.583754</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.045285</td>\n",
       "      <td>0.177728</td>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_eqOdds_fnr</th>\n",
       "      <td>0.454490</td>\n",
       "      <td>0.578804</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.037221</td>\n",
       "      <td>0.171981</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_eqOdds_fpr</th>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.079404</td>\n",
       "      <td>0.067867</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_RejOpt_spd</th>\n",
       "      <td>0.732626</td>\n",
       "      <td>0.705496</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.197177</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_RejOpt_aod</th>\n",
       "      <td>0.742525</td>\n",
       "      <td>0.705496</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.141997</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg_RejOpt_eod</th>\n",
       "      <td>0.742525</td>\n",
       "      <td>0.705496</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.141997</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_eqOdds</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.704284</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.117246</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.138399</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_eqOdds_weighted</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.632552</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.104839</td>\n",
       "      <td>0.064744</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_eqOdds_fnr</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.637199</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>0.182084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_eqOdds_fpr</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_RejOpt_spd</th>\n",
       "      <td>0.425758</td>\n",
       "      <td>0.636593</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.140199</td>\n",
       "      <td>0.160770</td>\n",
       "      <td>0.270513</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_RejOpt_aod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost_RejOpt_eod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_eqOdds</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.679228</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.101117</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.225175</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_eqOdds_weighted</th>\n",
       "      <td>0.595918</td>\n",
       "      <td>0.656395</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.357320</td>\n",
       "      <td>0.115164</td>\n",
       "      <td>0.193478</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_eqOdds_fnr</th>\n",
       "      <td>0.676735</td>\n",
       "      <td>0.583451</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.581266</td>\n",
       "      <td>0.044707</td>\n",
       "      <td>0.055072</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_eqOdds_fpr</th>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.701455</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.265509</td>\n",
       "      <td>0.156332</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_RejOpt_spd</th>\n",
       "      <td>0.683131</td>\n",
       "      <td>0.695292</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.075062</td>\n",
       "      <td>0.183664</td>\n",
       "      <td>0.312281</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_RejOpt_aod</th>\n",
       "      <td>0.702929</td>\n",
       "      <td>0.705496</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.141997</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg_RejOpt_eod</th>\n",
       "      <td>0.702929</td>\n",
       "      <td>0.705496</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.141997</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.744797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_eqOdds</th>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.680137</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.075062</td>\n",
       "      <td>0.067762</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_eqOdds_weighted</th>\n",
       "      <td>0.878776</td>\n",
       "      <td>0.681956</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.162531</td>\n",
       "      <td>0.093901</td>\n",
       "      <td>0.131818</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_eqOdds_fnr</th>\n",
       "      <td>0.636327</td>\n",
       "      <td>0.677106</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.130893</td>\n",
       "      <td>0.075007</td>\n",
       "      <td>0.176420</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_eqOdds_fpr</th>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.675187</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.103476</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_RejOpt_spd</th>\n",
       "      <td>0.485152</td>\n",
       "      <td>0.652657</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.259305</td>\n",
       "      <td>0.144627</td>\n",
       "      <td>0.281513</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_RejOpt_aod</th>\n",
       "      <td>0.613838</td>\n",
       "      <td>0.668418</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>0.126770</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost_RejOpt_eod</th>\n",
       "      <td>0.584141</td>\n",
       "      <td>0.698121</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.161252</td>\n",
       "      <td>0.264865</td>\n",
       "      <td>0.761770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            best_threshold   bal_acc       acc  independence  \\\n",
       "logreg                            0.777755  0.711053  0.653333      0.212779   \n",
       "xgboost                           0.757551  0.699333  0.680000      0.109181   \n",
       "RW_logreg                         0.717143  0.661063  0.658679      0.156145   \n",
       "DI_logreg                         0.818163  0.676702  0.600000      0.178660   \n",
       "RW_xgboost                        0.878776  0.659640  0.622095      0.241415   \n",
       "DI_xgboost                        0.656531  0.703374  0.706667      0.096774   \n",
       "RW_adversarial                    0.090816  0.506168  0.357673      0.188780   \n",
       "DI_adversarial                    0.838367  0.542029  0.673333      0.057072   \n",
       "RW_metafair                       0.636327  0.663103  0.674318      0.191546   \n",
       "DI_metafair                       0.636327  0.676399  0.606667      0.156328   \n",
       "RW_pir                            0.777755  0.679979  0.658759      0.143229   \n",
       "DI_pir                            0.858571  0.632148  0.540000      0.245658   \n",
       "metafair_sr                       0.575714  0.616791  0.646667      0.218983   \n",
       "metafair_fdr                      0.636327  0.710750  0.660000      0.143921   \n",
       "pir                               0.797959  0.686603  0.613333      0.287841   \n",
       "adversarial                       0.939388  0.492524  0.606667      0.070099   \n",
       "RW_logreg_eqOdds                  0.757551  0.689735  0.660000      0.161911   \n",
       "RW_logreg_eqOdds_weighted         0.454490  0.583754  0.680000      0.045285   \n",
       "RW_logreg_eqOdds_fnr              0.454490  0.578804  0.673333      0.037221   \n",
       "RW_logreg_eqOdds_fpr              0.797959  0.686300  0.620000      0.079404   \n",
       "RW_logreg_RejOpt_spd              0.732626  0.705496  0.660000      0.066998   \n",
       "RW_logreg_RejOpt_aod              0.742525  0.705496  0.660000      0.113524   \n",
       "RW_logreg_RejOpt_eod              0.742525  0.705496  0.660000      0.113524   \n",
       "RW_xgboost_eqOdds                 0.757551  0.704284  0.686667      0.117246   \n",
       "RW_xgboost_eqOdds_weighted        0.696939  0.632552  0.646667      0.104839   \n",
       "RW_xgboost_eqOdds_fnr             0.696939  0.637199  0.660000      0.137097   \n",
       "RW_xgboost_eqOdds_fpr             0.757551  0.699333  0.680000      0.109181   \n",
       "RW_xgboost_RejOpt_spd             0.425758  0.636593  0.673333      0.140199   \n",
       "RW_xgboost_RejOpt_aod             0.594040  0.652354  0.673333      0.090571   \n",
       "RW_xgboost_RejOpt_eod             0.594040  0.652354  0.673333      0.090571   \n",
       "DI_logreg_eqOdds                  0.696939  0.679228  0.660000      0.101117   \n",
       "DI_logreg_eqOdds_weighted         0.595918  0.656395  0.700000      0.357320   \n",
       "DI_logreg_eqOdds_fnr              0.676735  0.583451  0.686667      0.581266   \n",
       "DI_logreg_eqOdds_fpr              0.797959  0.701455  0.633333      0.265509   \n",
       "DI_logreg_RejOpt_spd              0.683131  0.695292  0.653333      0.075062   \n",
       "DI_logreg_RejOpt_aod              0.702929  0.705496  0.660000      0.113524   \n",
       "DI_logreg_RejOpt_eod              0.702929  0.705496  0.660000      0.113524   \n",
       "DI_xgboost_eqOdds                 0.797959  0.680137  0.640000      0.075062   \n",
       "DI_xgboost_eqOdds_weighted        0.878776  0.681956  0.600000      0.162531   \n",
       "DI_xgboost_eqOdds_fnr             0.636327  0.677106  0.706667      0.130893   \n",
       "DI_xgboost_eqOdds_fpr             0.797959  0.675187  0.633333      0.113524   \n",
       "DI_xgboost_RejOpt_spd             0.485152  0.652657  0.666667      0.259305   \n",
       "DI_xgboost_RejOpt_aod             0.613838  0.668418  0.666667      0.028536   \n",
       "DI_xgboost_RejOpt_eod             0.584141  0.698121  0.706667      0.019851   \n",
       "\n",
       "                            separation  sufficiency       auc  \n",
       "logreg                        0.179526     0.260606  0.745201  \n",
       "xgboost                       0.045250     0.136364  0.756112  \n",
       "RW_logreg                     0.135985     0.219985  0.743514  \n",
       "DI_logreg                     0.182776     0.334783  0.744191  \n",
       "RW_xgboost                    0.037121     0.011749  0.736582  \n",
       "DI_xgboost                    0.155300     0.249480  0.757325  \n",
       "RW_adversarial                0.009091     0.406250  0.527233  \n",
       "DI_adversarial                0.131335     0.232143  0.467367  \n",
       "RW_metafair                   0.169192     0.266712  0.692216  \n",
       "DI_metafair                   0.139296     0.250000  0.728026  \n",
       "RW_pir                        0.105808     0.180574  0.747871  \n",
       "DI_pir                        0.255436     0.925000  0.720752  \n",
       "metafair_sr                   0.192961     0.298108  0.653869  \n",
       "metafair_fdr                  0.094583     0.160714  0.728430  \n",
       "pir                           0.277193     0.606667  0.722772  \n",
       "adversarial                   0.129044     0.234610  0.427662  \n",
       "RW_logreg_eqOdds              0.118698     0.206349  0.733279  \n",
       "RW_logreg_eqOdds_weighted     0.177728     0.262376  0.733279  \n",
       "RW_logreg_eqOdds_fnr          0.171981     0.260000  0.733279  \n",
       "RW_logreg_eqOdds_fpr          0.067867     0.166667  0.733279  \n",
       "RW_logreg_RejOpt_spd          0.197177     0.328571  0.733279  \n",
       "RW_logreg_RejOpt_aod          0.141997     0.245614  0.733279  \n",
       "RW_logreg_RejOpt_eod          0.141997     0.245614  0.733279  \n",
       "RW_xgboost_eqOdds             0.050997     0.138399  0.756112  \n",
       "RW_xgboost_eqOdds_weighted    0.064744     0.184615  0.756112  \n",
       "RW_xgboost_eqOdds_fnr         0.068472     0.182084  0.756112  \n",
       "RW_xgboost_eqOdds_fpr         0.045250     0.136364  0.756112  \n",
       "RW_xgboost_RejOpt_spd         0.160770     0.270513  0.756112  \n",
       "RW_xgboost_RejOpt_aod         0.142913     0.249084  0.756112  \n",
       "RW_xgboost_RejOpt_eod         0.142913     0.249084  0.756112  \n",
       "DI_logreg_eqOdds              0.116884     0.225175  0.744797  \n",
       "DI_logreg_eqOdds_weighted     0.115164     0.193478  0.744797  \n",
       "DI_logreg_eqOdds_fnr          0.044707     0.055072  0.744797  \n",
       "DI_logreg_eqOdds_fpr          0.156332     0.173077  0.744797  \n",
       "DI_logreg_RejOpt_spd          0.183664     0.312281  0.744797  \n",
       "DI_logreg_RejOpt_aod          0.141997     0.245614  0.744797  \n",
       "DI_logreg_RejOpt_eod          0.141997     0.245614  0.744797  \n",
       "DI_xgboost_eqOdds             0.067762     0.177193  0.761770  \n",
       "DI_xgboost_eqOdds_weighted    0.093901     0.131818  0.761770  \n",
       "DI_xgboost_eqOdds_fnr         0.075007     0.176420  0.761770  \n",
       "DI_xgboost_eqOdds_fpr         0.103476     0.210526  0.761770  \n",
       "DI_xgboost_RejOpt_spd         0.144627     0.281513  0.761770  \n",
       "DI_xgboost_RejOpt_aod         0.126770     0.252941  0.761770  \n",
       "DI_xgboost_RejOpt_eod         0.161252     0.264865  0.761770  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 78.328941; batch adversarial loss: 0.603363\n",
      "epoch 1; iter: 0; batch classifier loss: 65.281548; batch adversarial loss: 0.541025\n",
      "epoch 2; iter: 0; batch classifier loss: 32.843430; batch adversarial loss: 0.521141\n",
      "epoch 3; iter: 0; batch classifier loss: 38.578865; batch adversarial loss: 0.482325\n",
      "epoch 4; iter: 0; batch classifier loss: 36.352013; batch adversarial loss: 0.504410\n",
      "epoch 5; iter: 0; batch classifier loss: 51.872528; batch adversarial loss: 0.501763\n",
      "epoch 6; iter: 0; batch classifier loss: 50.874722; batch adversarial loss: 0.522069\n",
      "epoch 7; iter: 0; batch classifier loss: 40.608467; batch adversarial loss: 0.498124\n",
      "epoch 8; iter: 0; batch classifier loss: 43.048775; batch adversarial loss: 0.531246\n",
      "epoch 9; iter: 0; batch classifier loss: 43.378441; batch adversarial loss: 0.467275\n",
      "epoch 10; iter: 0; batch classifier loss: 42.046997; batch adversarial loss: 0.495165\n",
      "epoch 11; iter: 0; batch classifier loss: 37.851143; batch adversarial loss: 0.460472\n",
      "epoch 12; iter: 0; batch classifier loss: 28.085127; batch adversarial loss: 0.523847\n",
      "epoch 13; iter: 0; batch classifier loss: 33.730457; batch adversarial loss: 0.543853\n",
      "epoch 14; iter: 0; batch classifier loss: 29.669193; batch adversarial loss: 0.473274\n",
      "epoch 15; iter: 0; batch classifier loss: 36.238693; batch adversarial loss: 0.485006\n",
      "epoch 16; iter: 0; batch classifier loss: 14.124228; batch adversarial loss: 0.441798\n",
      "epoch 17; iter: 0; batch classifier loss: 38.286407; batch adversarial loss: 0.498976\n",
      "epoch 18; iter: 0; batch classifier loss: 24.936209; batch adversarial loss: 0.485504\n",
      "epoch 19; iter: 0; batch classifier loss: 20.711714; batch adversarial loss: 0.498422\n",
      "epoch 20; iter: 0; batch classifier loss: 26.800972; batch adversarial loss: 0.461878\n",
      "epoch 21; iter: 0; batch classifier loss: 27.957376; batch adversarial loss: 0.474235\n",
      "epoch 22; iter: 0; batch classifier loss: 22.265509; batch adversarial loss: 0.459994\n",
      "epoch 23; iter: 0; batch classifier loss: 18.280539; batch adversarial loss: 0.496382\n",
      "epoch 24; iter: 0; batch classifier loss: 37.531582; batch adversarial loss: 0.413348\n",
      "epoch 25; iter: 0; batch classifier loss: 16.903168; batch adversarial loss: 0.475116\n",
      "epoch 26; iter: 0; batch classifier loss: 14.595089; batch adversarial loss: 0.477972\n",
      "epoch 27; iter: 0; batch classifier loss: 20.480721; batch adversarial loss: 0.478893\n",
      "epoch 28; iter: 0; batch classifier loss: 14.512852; batch adversarial loss: 0.454733\n",
      "epoch 29; iter: 0; batch classifier loss: 12.870618; batch adversarial loss: 0.506636\n",
      "epoch 30; iter: 0; batch classifier loss: 20.769676; batch adversarial loss: 0.492357\n",
      "epoch 31; iter: 0; batch classifier loss: 14.789041; batch adversarial loss: 0.471367\n",
      "epoch 32; iter: 0; batch classifier loss: 9.164530; batch adversarial loss: 0.483247\n",
      "epoch 33; iter: 0; batch classifier loss: 7.861546; batch adversarial loss: 0.501742\n",
      "epoch 34; iter: 0; batch classifier loss: 8.745699; batch adversarial loss: 0.493950\n",
      "epoch 35; iter: 0; batch classifier loss: 8.394541; batch adversarial loss: 0.465783\n",
      "epoch 36; iter: 0; batch classifier loss: 12.687309; batch adversarial loss: 0.448481\n",
      "epoch 37; iter: 0; batch classifier loss: 8.476727; batch adversarial loss: 0.434176\n",
      "epoch 38; iter: 0; batch classifier loss: 11.601459; batch adversarial loss: 0.488007\n",
      "epoch 39; iter: 0; batch classifier loss: 12.016554; batch adversarial loss: 0.453598\n",
      "epoch 40; iter: 0; batch classifier loss: 13.840254; batch adversarial loss: 0.534115\n",
      "epoch 41; iter: 0; batch classifier loss: 9.408798; batch adversarial loss: 0.480670\n",
      "epoch 42; iter: 0; batch classifier loss: 8.585636; batch adversarial loss: 0.470055\n",
      "epoch 43; iter: 0; batch classifier loss: 5.746333; batch adversarial loss: 0.403989\n",
      "epoch 44; iter: 0; batch classifier loss: 8.201762; batch adversarial loss: 0.474696\n",
      "epoch 45; iter: 0; batch classifier loss: 4.740445; batch adversarial loss: 0.363969\n",
      "epoch 46; iter: 0; batch classifier loss: 5.703025; batch adversarial loss: 0.453157\n",
      "epoch 47; iter: 0; batch classifier loss: 4.914260; batch adversarial loss: 0.455279\n",
      "epoch 48; iter: 0; batch classifier loss: 6.593405; batch adversarial loss: 0.444756\n",
      "epoch 49; iter: 0; batch classifier loss: 5.367754; batch adversarial loss: 0.402827\n",
      "epoch 50; iter: 0; batch classifier loss: 4.128370; batch adversarial loss: 0.438621\n",
      "epoch 51; iter: 0; batch classifier loss: 6.102639; batch adversarial loss: 0.441862\n",
      "epoch 52; iter: 0; batch classifier loss: 3.298930; batch adversarial loss: 0.433908\n",
      "epoch 53; iter: 0; batch classifier loss: 2.271949; batch adversarial loss: 0.406958\n",
      "epoch 54; iter: 0; batch classifier loss: 4.646158; batch adversarial loss: 0.495650\n",
      "epoch 55; iter: 0; batch classifier loss: 4.879381; batch adversarial loss: 0.574803\n",
      "epoch 56; iter: 0; batch classifier loss: 2.238040; batch adversarial loss: 0.401187\n",
      "epoch 57; iter: 0; batch classifier loss: 3.381795; batch adversarial loss: 0.497136\n",
      "epoch 58; iter: 0; batch classifier loss: 3.189019; batch adversarial loss: 0.486733\n",
      "epoch 59; iter: 0; batch classifier loss: 2.542812; batch adversarial loss: 0.496917\n",
      "epoch 60; iter: 0; batch classifier loss: 2.673145; batch adversarial loss: 0.363671\n",
      "epoch 61; iter: 0; batch classifier loss: 2.683569; batch adversarial loss: 0.447862\n",
      "epoch 62; iter: 0; batch classifier loss: 1.531096; batch adversarial loss: 0.517264\n",
      "epoch 63; iter: 0; batch classifier loss: 2.301376; batch adversarial loss: 0.545791\n",
      "epoch 64; iter: 0; batch classifier loss: 1.764860; batch adversarial loss: 0.450804\n",
      "epoch 65; iter: 0; batch classifier loss: 1.303916; batch adversarial loss: 0.449717\n",
      "epoch 66; iter: 0; batch classifier loss: 1.193132; batch adversarial loss: 0.435080\n",
      "epoch 67; iter: 0; batch classifier loss: 2.622313; batch adversarial loss: 0.555331\n",
      "epoch 68; iter: 0; batch classifier loss: 2.123317; batch adversarial loss: 0.555813\n",
      "epoch 69; iter: 0; batch classifier loss: 0.955437; batch adversarial loss: 0.472004\n",
      "epoch 70; iter: 0; batch classifier loss: 1.001638; batch adversarial loss: 0.363477\n",
      "epoch 71; iter: 0; batch classifier loss: 1.138752; batch adversarial loss: 0.380123\n",
      "epoch 72; iter: 0; batch classifier loss: 1.363492; batch adversarial loss: 0.471638\n",
      "epoch 73; iter: 0; batch classifier loss: 1.057160; batch adversarial loss: 0.499345\n",
      "epoch 74; iter: 0; batch classifier loss: 1.366816; batch adversarial loss: 0.519828\n",
      "epoch 75; iter: 0; batch classifier loss: 1.090314; batch adversarial loss: 0.477660\n",
      "epoch 76; iter: 0; batch classifier loss: 1.093919; batch adversarial loss: 0.441956\n",
      "epoch 77; iter: 0; batch classifier loss: 1.361231; batch adversarial loss: 0.511331\n",
      "epoch 78; iter: 0; batch classifier loss: 1.187621; batch adversarial loss: 0.510085\n",
      "epoch 79; iter: 0; batch classifier loss: 0.797087; batch adversarial loss: 0.416805\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>independence</th>\n",
       "      <th>separation</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.212779</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg</th>\n",
       "      <td>0.191837</td>\n",
       "      <td>0.541119</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>0.748465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg</th>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.666801</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.255583</td>\n",
       "      <td>0.254205</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.749444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost</th>\n",
       "      <td>0.131224</td>\n",
       "      <td>0.551323</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.137006</td>\n",
       "      <td>0.736582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost</th>\n",
       "      <td>0.676735</td>\n",
       "      <td>0.729036</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.157568</td>\n",
       "      <td>0.079733</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.769852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_sr</th>\n",
       "      <td>0.474694</td>\n",
       "      <td>0.718529</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.329404</td>\n",
       "      <td>0.091943</td>\n",
       "      <td>0.098101</td>\n",
       "      <td>0.746818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_fdr</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.661245</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.156328</td>\n",
       "      <td>0.120035</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.736917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir</th>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.691554</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.068164</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.733279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adversarial</th>\n",
       "      <td>0.474694</td>\n",
       "      <td>0.517882</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.440291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_Platt</th>\n",
       "      <td>0.776310</td>\n",
       "      <td>0.607699</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.113671</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.739543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.698727</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.283835</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_weighted</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.667509</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.021889</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_fnr</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.625177</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.451852</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_fpr</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.701152</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.077765</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_spd</th>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.655991</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>0.057988</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_aod</th>\n",
       "      <td>0.722727</td>\n",
       "      <td>0.705799</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_eod</th>\n",
       "      <td>0.762323</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.047235</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_Platt</th>\n",
       "      <td>0.771836</td>\n",
       "      <td>0.652253</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>0.162058</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.746413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.694383</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_weighted</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.642756</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.125926</td>\n",
       "      <td>0.117896</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_fnr</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.652960</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.238698</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_fpr</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.694383</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.164267</td>\n",
       "      <td>0.259155</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_spd</th>\n",
       "      <td>0.346566</td>\n",
       "      <td>0.640938</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.089478</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_aod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.127496</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_eod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.127496</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         best_threshold   bal_acc       acc  independence  \\\n",
       "logreg                         0.777755  0.711053  0.653333      0.212779   \n",
       "xgboost                        0.757551  0.699333  0.680000      0.109181   \n",
       "RW_logreg                      0.191837  0.541119  0.693333      0.009926   \n",
       "DI_logreg                      0.838367  0.666801  0.586667      0.255583   \n",
       "RW_xgboost                     0.131224  0.551323  0.700000      0.028536   \n",
       "DI_xgboost                     0.676735  0.729036  0.720000      0.157568   \n",
       "metafair_sr                    0.474694  0.718529  0.720000      0.329404   \n",
       "metafair_fdr                   0.656531  0.661245  0.593333      0.156328   \n",
       "pir                            0.818163  0.691554  0.620000      0.251852   \n",
       "adversarial                    0.474694  0.517882  0.626667      0.022222   \n",
       "logreg_Platt                   0.776310  0.607699  0.500000      0.251852   \n",
       "logreg_eqOdds                  0.656531  0.698727  0.693333      0.096296   \n",
       "logreg_eqOdds_weighted         0.656531  0.667509  0.686667      0.311111   \n",
       "logreg_eqOdds_fnr              0.656531  0.625177  0.693333      0.451852   \n",
       "logreg_eqOdds_fpr              0.777755  0.701152  0.640000      0.288889   \n",
       "logreg_RejOpt_spd              0.712828  0.655991  0.593333      0.029630   \n",
       "logreg_RejOpt_aod              0.722727  0.705799  0.653333      0.096296   \n",
       "logreg_RejOpt_eod              0.762323  0.711053  0.653333      0.155556   \n",
       "xgboost_Platt                  0.771836  0.652253  0.560000      0.318519   \n",
       "xgboost_eqOdds                 0.777755  0.694383  0.673333      0.192593   \n",
       "xgboost_eqOdds_weighted        0.696939  0.642756  0.653333      0.125926   \n",
       "xgboost_eqOdds_fnr             0.696939  0.652960  0.660000      0.118519   \n",
       "xgboost_eqOdds_fpr             0.757551  0.694383  0.673333      0.192593   \n",
       "xgboost_RejOpt_spd             0.346566  0.640938  0.693333      0.259259   \n",
       "xgboost_RejOpt_aod             0.594040  0.652354  0.673333      0.162963   \n",
       "xgboost_RejOpt_eod             0.594040  0.652354  0.673333      0.162963   \n",
       "\n",
       "                         separation  sufficiency       auc  \n",
       "logreg                     0.179526     0.260606  0.745201  \n",
       "xgboost                    0.045250     0.136364  0.756112  \n",
       "RW_logreg                  0.000893     0.160339  0.748465  \n",
       "DI_logreg                  0.254205     0.601449  0.749444  \n",
       "RW_xgboost                 0.040774     0.137006  0.736582  \n",
       "DI_xgboost                 0.079733     0.147727  0.769852  \n",
       "metafair_sr                0.091943     0.098101  0.746818  \n",
       "metafair_fdr               0.120035     0.229167  0.736917  \n",
       "pir                        0.068164     0.096154  0.733279  \n",
       "adversarial                0.080069     0.202703  0.440291  \n",
       "logreg_Platt               0.113671     0.882353  0.739543  \n",
       "logreg_eqOdds              0.182796     0.283835  0.745201  \n",
       "logreg_eqOdds_weighted     0.021889     0.006897  0.745201  \n",
       "logreg_eqOdds_fnr          0.075269     0.054717  0.745201  \n",
       "logreg_eqOdds_fpr          0.077765     0.105263  0.745201  \n",
       "logreg_RejOpt_spd          0.057988     0.213333  0.745201  \n",
       "logreg_RejOpt_aod          0.029570     0.096552  0.745201  \n",
       "logreg_RejOpt_eod          0.047235     0.105263  0.745201  \n",
       "xgboost_Platt              0.162058     0.906977  0.746413  \n",
       "xgboost_eqOdds             0.013057     0.045070  0.756112  \n",
       "xgboost_eqOdds_weighted    0.117896     0.228571  0.756112  \n",
       "xgboost_eqOdds_fnr         0.129800     0.238698  0.756112  \n",
       "xgboost_eqOdds_fpr         0.164267     0.259155  0.756112  \n",
       "xgboost_RejOpt_spd         0.089478     0.219780  0.756112  \n",
       "xgboost_RejOpt_aod         0.127496     0.228571  0.756112  \n",
       "xgboost_RejOpt_eod         0.127496     0.228571  0.756112  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvar = 2\n",
    "\n",
    "modelsNames = ['logreg', 'xgboost']\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single = GermanDataset2V()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "\n",
    "# Post processing\n",
    "for model in modelsNames:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 78.328941; batch adversarial loss: 0.583292\n",
      "epoch 1; iter: 0; batch classifier loss: 65.280716; batch adversarial loss: 0.564662\n",
      "epoch 2; iter: 0; batch classifier loss: 32.841087; batch adversarial loss: 0.517795\n",
      "epoch 3; iter: 0; batch classifier loss: 38.843472; batch adversarial loss: 0.526234\n",
      "epoch 4; iter: 0; batch classifier loss: 36.136360; batch adversarial loss: 0.543333\n",
      "epoch 5; iter: 0; batch classifier loss: 51.419430; batch adversarial loss: 0.534661\n",
      "epoch 6; iter: 0; batch classifier loss: 50.930008; batch adversarial loss: 0.572119\n",
      "epoch 7; iter: 0; batch classifier loss: 40.479343; batch adversarial loss: 0.534250\n",
      "epoch 8; iter: 0; batch classifier loss: 43.165482; batch adversarial loss: 0.562091\n",
      "epoch 9; iter: 0; batch classifier loss: 42.964333; batch adversarial loss: 0.521938\n",
      "epoch 10; iter: 0; batch classifier loss: 42.900841; batch adversarial loss: 0.545499\n",
      "epoch 11; iter: 0; batch classifier loss: 38.518967; batch adversarial loss: 0.542401\n",
      "epoch 12; iter: 0; batch classifier loss: 27.839470; batch adversarial loss: 0.543198\n",
      "epoch 13; iter: 0; batch classifier loss: 31.554300; batch adversarial loss: 0.571741\n",
      "epoch 14; iter: 0; batch classifier loss: 30.571178; batch adversarial loss: 0.516164\n",
      "epoch 15; iter: 0; batch classifier loss: 38.042793; batch adversarial loss: 0.550371\n",
      "epoch 16; iter: 0; batch classifier loss: 13.710626; batch adversarial loss: 0.498519\n",
      "epoch 17; iter: 0; batch classifier loss: 38.336700; batch adversarial loss: 0.512246\n",
      "epoch 18; iter: 0; batch classifier loss: 28.114685; batch adversarial loss: 0.486323\n",
      "epoch 19; iter: 0; batch classifier loss: 21.125179; batch adversarial loss: 0.503528\n",
      "epoch 20; iter: 0; batch classifier loss: 27.458620; batch adversarial loss: 0.533948\n",
      "epoch 21; iter: 0; batch classifier loss: 34.298561; batch adversarial loss: 0.643572\n",
      "epoch 22; iter: 0; batch classifier loss: 21.465538; batch adversarial loss: 0.525651\n",
      "epoch 23; iter: 0; batch classifier loss: 22.913225; batch adversarial loss: 0.481093\n",
      "epoch 24; iter: 0; batch classifier loss: 48.859673; batch adversarial loss: 0.437825\n",
      "epoch 25; iter: 0; batch classifier loss: 18.205744; batch adversarial loss: 0.529604\n",
      "epoch 26; iter: 0; batch classifier loss: 17.190975; batch adversarial loss: 0.585074\n",
      "epoch 27; iter: 0; batch classifier loss: 24.642925; batch adversarial loss: 0.526083\n",
      "epoch 28; iter: 0; batch classifier loss: 16.251045; batch adversarial loss: 0.531540\n",
      "epoch 29; iter: 0; batch classifier loss: 14.806496; batch adversarial loss: 0.517058\n",
      "epoch 30; iter: 0; batch classifier loss: 22.098057; batch adversarial loss: 0.492181\n",
      "epoch 31; iter: 0; batch classifier loss: 16.675415; batch adversarial loss: 0.492204\n",
      "epoch 32; iter: 0; batch classifier loss: 11.456402; batch adversarial loss: 0.559214\n",
      "epoch 33; iter: 0; batch classifier loss: 9.491199; batch adversarial loss: 0.538045\n",
      "epoch 34; iter: 0; batch classifier loss: 10.278039; batch adversarial loss: 0.467860\n",
      "epoch 35; iter: 0; batch classifier loss: 8.515280; batch adversarial loss: 0.493449\n",
      "epoch 36; iter: 0; batch classifier loss: 16.868366; batch adversarial loss: 0.501493\n",
      "epoch 37; iter: 0; batch classifier loss: 10.556383; batch adversarial loss: 0.491452\n",
      "epoch 38; iter: 0; batch classifier loss: 13.983143; batch adversarial loss: 0.565539\n",
      "epoch 39; iter: 0; batch classifier loss: 17.069645; batch adversarial loss: 0.483590\n",
      "epoch 40; iter: 0; batch classifier loss: 10.388662; batch adversarial loss: 0.532287\n",
      "epoch 41; iter: 0; batch classifier loss: 10.935918; batch adversarial loss: 0.476801\n",
      "epoch 42; iter: 0; batch classifier loss: 11.346522; batch adversarial loss: 0.539418\n",
      "epoch 43; iter: 0; batch classifier loss: 7.245730; batch adversarial loss: 0.431304\n",
      "epoch 44; iter: 0; batch classifier loss: 10.076423; batch adversarial loss: 0.447874\n",
      "epoch 45; iter: 0; batch classifier loss: 6.745779; batch adversarial loss: 0.486339\n",
      "epoch 46; iter: 0; batch classifier loss: 6.041595; batch adversarial loss: 0.471248\n",
      "epoch 47; iter: 0; batch classifier loss: 5.273021; batch adversarial loss: 0.454857\n",
      "epoch 48; iter: 0; batch classifier loss: 8.158537; batch adversarial loss: 0.539838\n",
      "epoch 49; iter: 0; batch classifier loss: 7.278372; batch adversarial loss: 0.530883\n",
      "epoch 50; iter: 0; batch classifier loss: 6.681669; batch adversarial loss: 0.484169\n",
      "epoch 51; iter: 0; batch classifier loss: 7.803533; batch adversarial loss: 0.463405\n",
      "epoch 52; iter: 0; batch classifier loss: 4.490322; batch adversarial loss: 0.469284\n",
      "epoch 53; iter: 0; batch classifier loss: 3.856228; batch adversarial loss: 0.471797\n",
      "epoch 54; iter: 0; batch classifier loss: 6.301878; batch adversarial loss: 0.441704\n",
      "epoch 55; iter: 0; batch classifier loss: 6.398008; batch adversarial loss: 0.600650\n",
      "epoch 56; iter: 0; batch classifier loss: 3.329650; batch adversarial loss: 0.465989\n",
      "epoch 57; iter: 0; batch classifier loss: 4.399462; batch adversarial loss: 0.490989\n",
      "epoch 58; iter: 0; batch classifier loss: 3.086243; batch adversarial loss: 0.432370\n",
      "epoch 59; iter: 0; batch classifier loss: 4.015261; batch adversarial loss: 0.512496\n",
      "epoch 60; iter: 0; batch classifier loss: 3.599471; batch adversarial loss: 0.467172\n",
      "epoch 61; iter: 0; batch classifier loss: 3.825002; batch adversarial loss: 0.568162\n",
      "epoch 62; iter: 0; batch classifier loss: 2.108548; batch adversarial loss: 0.530323\n",
      "epoch 63; iter: 0; batch classifier loss: 1.746735; batch adversarial loss: 0.479370\n",
      "epoch 64; iter: 0; batch classifier loss: 2.659571; batch adversarial loss: 0.485587\n",
      "epoch 65; iter: 0; batch classifier loss: 3.440312; batch adversarial loss: 0.458340\n",
      "epoch 66; iter: 0; batch classifier loss: 2.365382; batch adversarial loss: 0.437671\n",
      "epoch 67; iter: 0; batch classifier loss: 5.724104; batch adversarial loss: 0.579251\n",
      "epoch 68; iter: 0; batch classifier loss: 1.547963; batch adversarial loss: 0.439477\n",
      "epoch 69; iter: 0; batch classifier loss: 1.751155; batch adversarial loss: 0.461705\n",
      "epoch 70; iter: 0; batch classifier loss: 1.419798; batch adversarial loss: 0.423028\n",
      "epoch 71; iter: 0; batch classifier loss: 1.729592; batch adversarial loss: 0.470225\n",
      "epoch 72; iter: 0; batch classifier loss: 2.283039; batch adversarial loss: 0.404203\n",
      "epoch 73; iter: 0; batch classifier loss: 1.640347; batch adversarial loss: 0.446530\n",
      "epoch 74; iter: 0; batch classifier loss: 5.419665; batch adversarial loss: 0.604074\n",
      "epoch 75; iter: 0; batch classifier loss: 1.398837; batch adversarial loss: 0.428638\n",
      "epoch 76; iter: 0; batch classifier loss: 1.546277; batch adversarial loss: 0.375861\n",
      "epoch 77; iter: 0; batch classifier loss: 4.889842; batch adversarial loss: 0.595225\n",
      "epoch 78; iter: 0; batch classifier loss: 1.514507; batch adversarial loss: 0.464858\n",
      "epoch 79; iter: 0; batch classifier loss: 1.226492; batch adversarial loss: 0.503316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>independence</th>\n",
       "      <th>separation</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.212779</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg</th>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.661063</td>\n",
       "      <td>0.658679</td>\n",
       "      <td>0.156145</td>\n",
       "      <td>0.135985</td>\n",
       "      <td>0.219985</td>\n",
       "      <td>0.743514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg</th>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.676702</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.178660</td>\n",
       "      <td>0.182776</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.744191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost</th>\n",
       "      <td>0.878776</td>\n",
       "      <td>0.659640</td>\n",
       "      <td>0.622095</td>\n",
       "      <td>0.241415</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.736582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.703374</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.249480</td>\n",
       "      <td>0.757325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_sr</th>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.616791</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.218983</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>0.298108</td>\n",
       "      <td>0.653869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_fdr</th>\n",
       "      <td>0.636327</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.094583</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.728430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pir</th>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.686603</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.287841</td>\n",
       "      <td>0.277193</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.722772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adversarial</th>\n",
       "      <td>0.070612</td>\n",
       "      <td>0.509598</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.586583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_Platt</th>\n",
       "      <td>0.774267</td>\n",
       "      <td>0.642655</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.160609</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.752677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds</th>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.689735</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.068859</td>\n",
       "      <td>0.132417</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_weighted</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.671853</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>0.081264</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_fnr</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.572641</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.637717</td>\n",
       "      <td>0.062259</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_fpr</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.716306</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.289702</td>\n",
       "      <td>0.173573</td>\n",
       "      <td>0.177273</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_spd</th>\n",
       "      <td>0.683131</td>\n",
       "      <td>0.690341</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.020471</td>\n",
       "      <td>0.233097</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_aod</th>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.097395</td>\n",
       "      <td>0.149764</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_eod</th>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.097395</td>\n",
       "      <td>0.149764</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_Platt</th>\n",
       "      <td>0.771322</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.218080</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.755708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.704284</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.117246</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.138399</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_weighted</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.632552</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.104839</td>\n",
       "      <td>0.064744</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_fnr</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.637199</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>0.182084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_fpr</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_spd</th>\n",
       "      <td>0.425758</td>\n",
       "      <td>0.636593</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.140199</td>\n",
       "      <td>0.160770</td>\n",
       "      <td>0.270513</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_aod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_eod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         best_threshold   bal_acc       acc  independence  \\\n",
       "logreg                         0.777755  0.711053  0.653333      0.212779   \n",
       "xgboost                        0.757551  0.699333  0.680000      0.109181   \n",
       "RW_logreg                      0.717143  0.661063  0.658679      0.156145   \n",
       "DI_logreg                      0.818163  0.676702  0.600000      0.178660   \n",
       "RW_xgboost                     0.878776  0.659640  0.622095      0.241415   \n",
       "DI_xgboost                     0.656531  0.703374  0.706667      0.096774   \n",
       "metafair_sr                    0.575714  0.616791  0.646667      0.218983   \n",
       "metafair_fdr                   0.636327  0.710750  0.660000      0.143921   \n",
       "pir                            0.797959  0.686603  0.613333      0.287841   \n",
       "adversarial                    0.070612  0.509598  0.346667      0.006203   \n",
       "logreg_Platt                   0.774267  0.642655  0.540000      0.306452   \n",
       "logreg_eqOdds                  0.717143  0.689735  0.660000      0.068859   \n",
       "logreg_eqOdds_weighted         0.656531  0.671853  0.706667      0.379653   \n",
       "logreg_eqOdds_fnr              0.656531  0.572641  0.693333      0.637717   \n",
       "logreg_eqOdds_fpr              0.777755  0.716306  0.653333      0.289702   \n",
       "logreg_RejOpt_spd              0.683131  0.690341  0.646667      0.020471   \n",
       "logreg_RejOpt_aod              0.712828  0.710750  0.660000      0.097395   \n",
       "logreg_RejOpt_eod              0.712828  0.710750  0.660000      0.097395   \n",
       "xgboost_Platt                  0.771322  0.692160  0.606667      0.387097   \n",
       "xgboost_eqOdds                 0.757551  0.704284  0.686667      0.117246   \n",
       "xgboost_eqOdds_weighted        0.696939  0.632552  0.646667      0.104839   \n",
       "xgboost_eqOdds_fnr             0.696939  0.637199  0.660000      0.137097   \n",
       "xgboost_eqOdds_fpr             0.757551  0.699333  0.680000      0.109181   \n",
       "xgboost_RejOpt_spd             0.425758  0.636593  0.673333      0.140199   \n",
       "xgboost_RejOpt_aod             0.594040  0.652354  0.673333      0.090571   \n",
       "xgboost_RejOpt_eod             0.594040  0.652354  0.673333      0.090571   \n",
       "\n",
       "                         separation  sufficiency       auc  \n",
       "logreg                     0.179526     0.260606  0.745201  \n",
       "xgboost                    0.045250     0.136364  0.756112  \n",
       "RW_logreg                  0.135985     0.219985  0.743514  \n",
       "DI_logreg                  0.182776     0.334783  0.744191  \n",
       "RW_xgboost                 0.037121     0.011749  0.736582  \n",
       "DI_xgboost                 0.155300     0.249480  0.757325  \n",
       "metafair_sr                0.192961     0.298108  0.653869  \n",
       "metafair_fdr               0.094583     0.160714  0.728430  \n",
       "pir                        0.277193     0.606667  0.722772  \n",
       "adversarial                0.031986     0.250000  0.586583  \n",
       "logreg_Platt               0.160609     0.921053  0.752677  \n",
       "logreg_eqOdds              0.132417     0.248882  0.745201  \n",
       "logreg_eqOdds_weighted     0.081264     0.133333  0.745201  \n",
       "logreg_eqOdds_fnr          0.062259     0.046448  0.745201  \n",
       "logreg_eqOdds_fpr          0.173573     0.177273  0.745201  \n",
       "logreg_RejOpt_spd          0.233097     0.381818  0.745201  \n",
       "logreg_RejOpt_aod          0.149764     0.260606  0.745201  \n",
       "logreg_RejOpt_eod          0.149764     0.260606  0.745201  \n",
       "xgboost_Platt              0.218080     0.937500  0.755708  \n",
       "xgboost_eqOdds             0.050997     0.138399  0.756112  \n",
       "xgboost_eqOdds_weighted    0.064744     0.184615  0.756112  \n",
       "xgboost_eqOdds_fnr         0.068472     0.182084  0.756112  \n",
       "xgboost_eqOdds_fpr         0.045250     0.136364  0.756112  \n",
       "xgboost_RejOpt_spd         0.160770     0.270513  0.756112  \n",
       "xgboost_RejOpt_aod         0.142913     0.249084  0.756112  \n",
       "xgboost_RejOpt_eod         0.142913     0.249084  0.756112  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvar = 1\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups = GermanDataset1V()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "\n",
    "# Post processing\n",
    "for model in modelsNames:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logreg', 'xgboost']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'age' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Pre processing\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m modelsNames:\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mPreprocRW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munprivileged_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#for quality in quality_constraints_meta:\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m## InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#    for key_metric in fair_metrics_optrej:\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m, in \u001b[0;36mPreprocRW\u001b[1;34m(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results)\u001b[0m\n\u001b[0;32m     36\u001b[0m         methods[model_name] \u001b[38;5;241m=\u001b[39m Algorithm\u001b[38;5;241m.\u001b[39mfit(trainRW\u001b[38;5;241m.\u001b[39mfeatures, trainRW\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     methods[model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainRW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Obtain results\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_results:\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:203\u001b[0m, in \u001b[0;36mAdversarialDebiasing.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    200\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfeatures[batch_ids]\n\u001b[0;32m    201\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(temp_labels[batch_ids], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    202\u001b[0m batch_protected_attributes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(dataset\u001b[38;5;241m.\u001b[39mprotected_attributes[batch_ids][:,\n\u001b[1;32m--> 203\u001b[0m                              dataset\u001b[38;5;241m.\u001b[39mprotected_attribute_names\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotected_attribute_name)], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    205\u001b[0m batch_feed_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_ph: batch_features,\n\u001b[0;32m    206\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_labels_ph: batch_labels,\n\u001b[0;32m    207\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotected_attributes_ph: batch_protected_attributes,\n\u001b[0;32m    208\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_prob: \u001b[38;5;241m0.8\u001b[39m}\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias:\n",
      "\u001b[1;31mValueError\u001b[0m: 'age' is not in list"
     ]
    }
   ],
   "source": [
    "nvar = 1\n",
    "\n",
    "modelsNames = [\n",
    "    'logreg',\n",
    "    'xgboost',\n",
    "    'adversarial',\n",
    "    'metafair',\n",
    "    'piremover'\n",
    "]\n",
    "\n",
    "modelsTrain = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'xgboost': XGBClassifier,\n",
    "    'adversarial': AdversarialDebiasing,\n",
    "    'metafair': MetaFairClassifier,\n",
    "    'piremover': PrejudiceRemover\n",
    "}\n",
    "\n",
    "modelsArgs = {\n",
    "    'logreg': {\n",
    "        'solver': 'liblinear',\n",
    "        'random_state': seed\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'eval_metric': 'error',\n",
    "        'eta':0.1,\n",
    "        'max_depth':6,\n",
    "        'subsample':0.8\n",
    "    },\n",
    "    'adversarial': {\n",
    "        'privileged_groups': privileged_groups,\n",
    "        'unprivileged_groups': unprivileged_groups,\n",
    "        'scope_name': 'debiased_classifier',\n",
    "        'debias': True,\n",
    "        'sess': tf.Session(), # Mirar esto de la sesion\n",
    "        'num_epochs': 80\n",
    "    },\n",
    "    'metafair': {\n",
    "        'tau': 0.8,\n",
    "        'sensitive_attribute': sensitive_attribute,\n",
    "        'type': 'sr',\n",
    "        'seed': seed\n",
    "    },\n",
    "#    'metafair_fdr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'fdr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "    'piremover': {\n",
    "        'sensitive_attr': sensitive_attribute,\n",
    "        'eta': 50.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single = GermanDataset2V()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "#for quality in quality_constraints_meta:\n",
    "#    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "## InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "#InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "#\n",
    "#\n",
    "## Post processing\n",
    "#for model in modelsNames:\n",
    "#    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "#    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "#    for quality in quality_constraints_eqodds:\n",
    "#        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "#    for key_metric in fair_metrics_optrej:\n",
    "#        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grids = {\n",
    "    'RW': {\n",
    "        \n",
    "    },\n",
    "    'DIR': {\n",
    "        'repair_level': [0.25, 0.5, 0.75]\n",
    "    },\n",
    "    'MetaFair': {\n",
    "        'tau': [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "    },\n",
    "    'PrejReg': {\n",
    "        'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.metrics_threshold_sweep_mult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
