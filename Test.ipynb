{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arturo\\AppData\\Local\\Temp\\ipykernel_27928\\2845975971.py:20: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "\n",
    "# TF\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "\n",
    "def GermanDataset1V():\n",
    "    dataset_german = GermanDataset(\n",
    "            protected_attribute_names=['age'],            \n",
    "            privileged_classes=[lambda x: x >= 25],      \n",
    "            features_to_drop=['personal_status', 'sex'] \n",
    "        )\n",
    "        \n",
    "    # xgboost requires labels to start at zero\n",
    "    dataset_german.labels[dataset_german.labels.ravel() == 2] =  dataset_german.labels[dataset_german.labels.ravel() == 2] - 2\n",
    "    dataset_german.unfavorable_label = dataset_german.unfavorable_label - 2\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = dataset_german.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We obtain sensitive attribute\n",
    "    sensitive_attribute = dataset_german.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "def GermanDataset2V():\n",
    "    dataset_german = GermanDataset(\n",
    "            protected_attribute_names=['age'],            \n",
    "            privileged_classes=[lambda x: x >= 25],      \n",
    "            features_to_drop=['personal_status', 'sex'] \n",
    "        )\n",
    "        \n",
    "    # xgboost requires labels to start at zero\n",
    "    dataset_german.labels[dataset_german.labels.ravel() == 2] =  dataset_german.labels[dataset_german.labels.ravel() == 2] - 2\n",
    "    dataset_german.unfavorable_label = dataset_german.unfavorable_label - 2\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = dataset_german.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We obtain sensitive attribute\n",
    "    sensitive_attribute = dataset_german.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "def Homecredit1V():\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos diccionarios\n",
    "methods = dict()\n",
    "\n",
    "# Rango de umbrales para evaluar el score de los modelos\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsNames = [\n",
    "    'logreg',\n",
    "    'xgboost'\n",
    "#    'adversarial',\n",
    "#    'metafair',\n",
    "#    'piremover'\n",
    "]\n",
    "\n",
    "modelsTrain = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'xgboost': XGBClassifier\n",
    "#    'adversarial': AdversarialDebiasing,\n",
    "#    'metafair': MetaFairClassifier,\n",
    "#    'piremover': PrejudiceRemover\n",
    "}\n",
    "\n",
    "modelsArgs = {\n",
    "    'logreg': {\n",
    "        'solver': 'liblinear',\n",
    "        'random_state': seed\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'eval_metric': 'error',\n",
    "        'eta':0.1,\n",
    "        'max_depth':6,\n",
    "        'subsample':0.8\n",
    "    }\n",
    "#    'adversarial': {\n",
    "#        'privileged_groups': privileged_groups,\n",
    "#        'unprivileged_groups': unprivileged_groups,\n",
    "#        'scope_name': 'debiased_classifier',\n",
    "#        'debias': True,\n",
    "#        'sess': tf.session(), # Mirar esto de la sesion\n",
    "#        'num_epochs': 80\n",
    "#    },\n",
    "#    'metafair_sr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'sr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "#    'metafair_fdr': {\n",
    "#        'tau': 0.8,\n",
    "#        'sensitive_attribute': sensitive_attribute,\n",
    "#        'type': 'fdr',\n",
    "#        'seed': seed\n",
    "#    },\n",
    "#    'piremover': {\n",
    "#        'sensitive_attr': sensitive_attribute,\n",
    "#        'eta': 50.0\n",
    "#    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_privileged_groups(dataset, sens_attr_ix=0):\n",
    "    \"\"\"\n",
    "    Helper function to privileged and unprivileged group dictionaries from `dataset`\n",
    "    Args:\n",
    "        dataset (StandardDataset): dataset in the aif360 format.\n",
    "        sens_attr_ix (int, optional): Index of the dataset.privileged_protected_attributes pointing to the sensitive attribute. Defaults to 0.\n",
    "    Returns:\n",
    "        tuple(list, list): privileged group and unprivileged group\n",
    "    \"\"\"\n",
    "\n",
    "    sens_attr = dataset.protected_attribute_names[sens_attr_ix]\n",
    "    priviledged_groups = [{sens_attr: v} for v in dataset.privileged_protected_attributes[sens_attr_ix]]\n",
    "    unpriviledged_groups = [{sens_attr: v} for v in dataset.unprivileged_protected_attributes[sens_attr_ix]]\n",
    "\n",
    "    return priviledged_groups, unpriviledged_groups\n",
    "\n",
    "def results(val, test, method):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep(\n",
    "        dataset=val,\n",
    "        model=methods[method],\n",
    "        thresh_arr=thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(metrics_sweep[method])\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics(\n",
    "        dataset=test, \n",
    "        model=methods[method], \n",
    "        threshold=metrics_best_thresh_validate[method]['best_threshold'])\n",
    "    \n",
    "\n",
    "def resultsPost(val, val_preds, test, test_preds, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "    \n",
    "    # Compute the metrics on the validation set\n",
    "    metrics_best_thresh_validate[model_name] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=val, \n",
    "        dataset_preds=val_preds, \n",
    "        model=methods[model_name], \n",
    "        required_threshold=False)\n",
    "    \n",
    "    # Compute the metrics on the test set\n",
    "    metrics_best_thresh_test[model_name] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test, \n",
    "        dataset_preds=test_preds, \n",
    "        model=methods[model_name], \n",
    "        required_threshold=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BenchmarkLogistic(data_train, data_val, data_test):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'logreg'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'sample_weight': train.instance_weights}\n",
    "\n",
    "    # Introduce the model in the model dict\n",
    "    methods[model_name] = LogisticRegression(solver='liblinear', random_state=seed)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel(), **fit_params)\n",
    "\n",
    "    # Obtain results\n",
    "    results(val, test, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BenchmarkXGB(data_train, data_val, data_test):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'xgboost'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "    # Assign the correct dict\n",
    "    methods[model_name] = XGBClassifier(**fit_params)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel())\n",
    "\n",
    "    # Obtain results\n",
    "    results(val, test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True):\n",
    "    #Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"RW\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Call the processor\n",
    "    PreProcessor = Reweighing(\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "    # Transform the data\n",
    "    PreProcessor.fit(train)\n",
    "    trainRW = PreProcessor.transform(train)\n",
    "    valRW = PreProcessor.transform(test)\n",
    "    testRW = PreProcessor.transform(val)\n",
    "\n",
    "    # Train the model\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model == 'logreg':\n",
    "        fit_params = {'sample_weight': trainRW.instance_weights}\n",
    "        methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel(), **fit_params)\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel())\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(valRW, testRW, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True):\n",
    "    #Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"DI\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Initialize the processor\n",
    "    PreProcessor = DisparateImpactRemover(\n",
    "        repair_level=repair_level,\n",
    "        sensitive_attribute=sensitive_attribute\n",
    "    )\n",
    "    # Transform the data\n",
    "    PreProcessor.fit_transform(train)\n",
    "    trainDI = PreProcessor.fit_transform(train)\n",
    "    valDI = PreProcessor.fit_transform(val)\n",
    "    testDI = PreProcessor.fit_transform(test)\n",
    "\n",
    "    # Train the model\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model == 'logreg':\n",
    "        fit_params = {'sample_weight': trainDI.instance_weights}\n",
    "        methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel(), **fit_params)\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel())\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(valDI, testDI, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality,  tau = 0.8, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # assign the correct name\n",
    "    model_name = \"metafair\"\n",
    "    model_name_quality = '{}_{}'.format(model_name, quality)\n",
    "\n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name_quality] = MetaFairClassifier(\n",
    "        tau=tau,\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        type=quality,\n",
    "        seed=seed\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_quality] = methods[model_name_quality].fit(train)\n",
    "\n",
    "    # Obtain scores\n",
    "    methods[model_name_quality].scores_train = methods[model_name_quality].predict(train).scores\n",
    "    methods[model_name_quality].scores_val = methods[model_name_quality].predict(val).scores\n",
    "    methods[model_name_quality].scores_test = methods[model_name_quality].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name_quality)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'pir'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name] = PrejudiceRemover(\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        eta=eta\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train)\n",
    "    \n",
    "    # Obtain scores\n",
    "    methods[model_name].scores_train = methods[model_name].predict(train).scores\n",
    "    methods[model_name].scores_val = methods[model_name].predict(val).scores\n",
    "    methods[model_name].scores_test = methods[model_name].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "\n",
    "\n",
    "def InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    \n",
    "    # Assign the correct name\n",
    "    model_name = 'NNAdvs'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Tensorflow session\n",
    "    sess = tf.compat.v1.Session()\n",
    "    \n",
    "    #We train the model\n",
    "    methods[model_name] = AdversarialDebiasing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups,\n",
    "        scope_name = 'debiased_classifier',\n",
    "        debias=True,\n",
    "        sess=sess,\n",
    "        num_epochs=80\n",
    "    )    \n",
    "    methods[model_name].fit(train)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PosprocPlatt(data_train, data_val, data_test, privileged_groups, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_sweep\n",
    "    global metrics_best_thresh_validate\n",
    "    global metrics_best_thresh_test\n",
    "    \n",
    "    # Assign the correct name\n",
    "    fairness_method = '_Platt'\n",
    "\n",
    "    # Validation\n",
    "    #---------------\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy = True), data_val.copy(deepcopy = True), data_test.copy(deepcopy = True)\n",
    "\n",
    "    # Copy the predictions\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Platt Scaling:\n",
    "    #---------------\n",
    "    #1. Split training data on sensitive attribute\n",
    "    val_preds_priv, val_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = val_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    \n",
    "    #2. Copy validation data predictions\n",
    "    val_preds2 = val_preds.copy(deepcopy = True)\n",
    "    \n",
    "    #3. Make one model for each group\n",
    "    sensitive_groups_data = {'priv': [val_preds_priv, priv_indices],\n",
    "                             'unpriv': [val_preds_unpriv, unpriv_indices]}\n",
    "    for group, data_group_list in sensitive_groups_data.items():\n",
    "        # Assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "        # Initialize the model, store it in the dict\n",
    "        methods[model_name_group] = LogisticRegression()\n",
    "        # Train the model using the validation data divided by group\n",
    "        methods[ model_name_group ] = methods[model_name_group].fit(\n",
    "            data_group_list[0].scores,   # data_group_list[0] -> data_val_preds_priv or data_val_preds_unpriv\n",
    "            val.subset(data_group_list[1]).labels.ravel()\n",
    "        ) # data_group_list[1] -> priv_indices or unpriv_indices\n",
    "\n",
    "        # predict group probabilities, store in val_preds2\n",
    "        # Platt scores are given by the predictions of the posterior probabilities\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        val_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "   \n",
    "    # Evaluate the model in a range of values\n",
    "    thresh_sweep_platt = np.linspace(np.min(val_preds2.scores.ravel()),\n",
    "                                     np.max(val_preds2.scores.ravel()),\n",
    "                                     50)\n",
    "\n",
    "    # Obtain the metrics for the val set\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep_from_scores(\n",
    "            dataset_true = val,\n",
    "            dataset_preds = val_preds,\n",
    "            thresh_arr = thresh_sweep_platt\n",
    "        )\n",
    "\n",
    "    # Evaluate metrics and obtain the best thresh\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    # Test\n",
    "    #---------------\n",
    "\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Plat Scaling:\n",
    "    #---------------\n",
    "    \n",
    "    # 1. Divide test set using sensitive varaible's groups\n",
    "    test_preds_priv, test_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = test_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    # 2. Copy test data\n",
    "    test_preds2 = test_preds.copy(deepcopy = True)\n",
    "    \n",
    "    # 3. Predict for each group\n",
    "    sensitive_groups_data_test = {'priv': [test_preds_priv, priv_indices],\n",
    "                                  'unpriv': [test_preds_unpriv, unpriv_indices]}\n",
    "\n",
    "    for group, data_group_list in sensitive_groups_data_test.items():    \n",
    "        # We assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "\n",
    "        # Predict in each group, store the result in data_val_preds2\n",
    "        # The probabilities are the Platt scores\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        test_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "    \n",
    "    # Obtain metrics\n",
    "    metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores(\n",
    "        dataset_true = test,\n",
    "        dataset_pred = test_preds2,\n",
    "        threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold']\n",
    "    )\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate    \n",
    "    \n",
    "    # Assign the correct name\n",
    "    fairness_method = '_eqOdds' \n",
    "\n",
    "    # Copy the dataset\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the predictions of the base model\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Initialize the model and store the predictions\n",
    "    methods[model_name+fairness_method] = EqOddsPostprocessing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups, \n",
    "        seed = seed)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name+fairness_method] = methods[model_name+fairness_method].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true=val,\n",
    "        dataset_preds=val_preds,\n",
    "        model=methods[model_name+fairness_method],\n",
    "        thresh_arr=thresh_sweep,\n",
    "        scores_or_labels='labels'\n",
    "    )\n",
    "\n",
    "    # Evaluate the model for the best threshold\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    # We use the best threshold to obtain predicitions for test\n",
    "    metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test,\n",
    "        dataset_preds=test_preds,\n",
    "        model=methods[model_name+fairness_method], \n",
    "        threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "        scores_or_labels='labels'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name, quality):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate\n",
    "\n",
    "     # Assign the correct name\n",
    "    fairness_method = '_eqOdds'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the model's predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name_metric = model_name + fairness_method + '_' + quality\n",
    "    \n",
    "    # Initialize the model \n",
    "    methods[model_name_metric] = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        cost_constraint=quality,\n",
    "        seed=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model for a range of thresholds\n",
    "    metrics_sweep[model_name_metric] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true = val,\n",
    "        dataset_preds = val_preds,\n",
    "        model = methods[model_name_metric],\n",
    "        thresh_arr = thresh_sweep,\n",
    "        scores_or_labels = 'scores'\n",
    "    )\n",
    "\n",
    "    # Evaluate in best thresh\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.describe_metrics(metrics_sweep[model_name_metric])\n",
    "\n",
    "    # Using the best thresh, evaluate in test\n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test,\n",
    "        dataset_preds=test_preds,\n",
    "        model=methods[model_name_metric], \n",
    "        threshold=metrics_best_thresh_validate[model_name_metric]['best_threshold'], \n",
    "        scores_or_labels='scores'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model_name, key_metric):\n",
    "    # Global variables\n",
    "    global methods\n",
    "    global metrics_best_thresh_validate\n",
    "    global fair_metrics_optrej\n",
    "\n",
    "    # Assign the correct name\n",
    "    fairness_method = '_RejOpt'\n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = RejectOptionClassification(\n",
    "        unprivileged_groups=unprivileged_groups, \n",
    "        privileged_groups=privileged_groups, \n",
    "        metric_name=fair_metrics_optrej[key_metric],\n",
    "        metric_lb=-0.01,\n",
    "        metric_ub=0.01\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "    \n",
    "    # Obtain best threshold in val\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=val, \n",
    "        dataset_preds=val_preds, \n",
    "        model=methods[model_name_metric], \n",
    "        required_threshold=False)\n",
    "    \n",
    "    # Obtain it in test\n",
    "    metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "        dataset_true=test, \n",
    "        dataset_preds=test_preds, \n",
    "        model=methods[model_name_metric], \n",
    "        required_threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Arturo\\AppData\\Local\\Temp\\ipykernel_27928\\4202609489.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\Arturo\\Desktop\\PhD\\PublicarTFM\\Code2\\venv\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 78.328941; batch adversarial loss: 0.583292\n",
      "epoch 1; iter: 0; batch classifier loss: 65.280716; batch adversarial loss: 0.564662\n",
      "epoch 2; iter: 0; batch classifier loss: 32.841087; batch adversarial loss: 0.517795\n",
      "epoch 3; iter: 0; batch classifier loss: 38.843472; batch adversarial loss: 0.526234\n",
      "epoch 4; iter: 0; batch classifier loss: 36.136360; batch adversarial loss: 0.543333\n",
      "epoch 5; iter: 0; batch classifier loss: 51.419430; batch adversarial loss: 0.534661\n",
      "epoch 6; iter: 0; batch classifier loss: 50.930008; batch adversarial loss: 0.572119\n",
      "epoch 7; iter: 0; batch classifier loss: 40.479343; batch adversarial loss: 0.534250\n",
      "epoch 8; iter: 0; batch classifier loss: 43.165482; batch adversarial loss: 0.562091\n",
      "epoch 9; iter: 0; batch classifier loss: 42.964333; batch adversarial loss: 0.521938\n",
      "epoch 10; iter: 0; batch classifier loss: 42.900841; batch adversarial loss: 0.545499\n",
      "epoch 11; iter: 0; batch classifier loss: 38.518967; batch adversarial loss: 0.542401\n",
      "epoch 12; iter: 0; batch classifier loss: 27.839470; batch adversarial loss: 0.543198\n",
      "epoch 13; iter: 0; batch classifier loss: 31.554300; batch adversarial loss: 0.571741\n",
      "epoch 14; iter: 0; batch classifier loss: 30.571178; batch adversarial loss: 0.516164\n",
      "epoch 15; iter: 0; batch classifier loss: 38.042793; batch adversarial loss: 0.550371\n",
      "epoch 16; iter: 0; batch classifier loss: 13.710626; batch adversarial loss: 0.498519\n",
      "epoch 17; iter: 0; batch classifier loss: 38.336700; batch adversarial loss: 0.512246\n",
      "epoch 18; iter: 0; batch classifier loss: 28.114685; batch adversarial loss: 0.486323\n",
      "epoch 19; iter: 0; batch classifier loss: 21.125179; batch adversarial loss: 0.503528\n",
      "epoch 20; iter: 0; batch classifier loss: 27.458620; batch adversarial loss: 0.533948\n",
      "epoch 21; iter: 0; batch classifier loss: 34.298561; batch adversarial loss: 0.643572\n",
      "epoch 22; iter: 0; batch classifier loss: 21.465538; batch adversarial loss: 0.525651\n",
      "epoch 23; iter: 0; batch classifier loss: 22.913225; batch adversarial loss: 0.481093\n",
      "epoch 24; iter: 0; batch classifier loss: 48.859673; batch adversarial loss: 0.437825\n",
      "epoch 25; iter: 0; batch classifier loss: 18.205744; batch adversarial loss: 0.529604\n",
      "epoch 26; iter: 0; batch classifier loss: 17.190975; batch adversarial loss: 0.585074\n",
      "epoch 27; iter: 0; batch classifier loss: 24.642925; batch adversarial loss: 0.526083\n",
      "epoch 28; iter: 0; batch classifier loss: 16.251045; batch adversarial loss: 0.531540\n",
      "epoch 29; iter: 0; batch classifier loss: 14.806496; batch adversarial loss: 0.517058\n",
      "epoch 30; iter: 0; batch classifier loss: 22.098057; batch adversarial loss: 0.492181\n",
      "epoch 31; iter: 0; batch classifier loss: 16.675415; batch adversarial loss: 0.492204\n",
      "epoch 32; iter: 0; batch classifier loss: 11.456402; batch adversarial loss: 0.559214\n",
      "epoch 33; iter: 0; batch classifier loss: 9.491199; batch adversarial loss: 0.538045\n",
      "epoch 34; iter: 0; batch classifier loss: 10.278039; batch adversarial loss: 0.467860\n",
      "epoch 35; iter: 0; batch classifier loss: 8.515280; batch adversarial loss: 0.493449\n",
      "epoch 36; iter: 0; batch classifier loss: 16.868366; batch adversarial loss: 0.501493\n",
      "epoch 37; iter: 0; batch classifier loss: 10.556383; batch adversarial loss: 0.491452\n",
      "epoch 38; iter: 0; batch classifier loss: 13.983143; batch adversarial loss: 0.565539\n",
      "epoch 39; iter: 0; batch classifier loss: 17.069645; batch adversarial loss: 0.483590\n",
      "epoch 40; iter: 0; batch classifier loss: 10.388662; batch adversarial loss: 0.532287\n",
      "epoch 41; iter: 0; batch classifier loss: 10.935918; batch adversarial loss: 0.476801\n",
      "epoch 42; iter: 0; batch classifier loss: 11.346522; batch adversarial loss: 0.539418\n",
      "epoch 43; iter: 0; batch classifier loss: 7.245730; batch adversarial loss: 0.431304\n",
      "epoch 44; iter: 0; batch classifier loss: 10.076423; batch adversarial loss: 0.447874\n",
      "epoch 45; iter: 0; batch classifier loss: 6.745779; batch adversarial loss: 0.486339\n",
      "epoch 46; iter: 0; batch classifier loss: 6.041595; batch adversarial loss: 0.471248\n",
      "epoch 47; iter: 0; batch classifier loss: 5.273021; batch adversarial loss: 0.454857\n",
      "epoch 48; iter: 0; batch classifier loss: 8.158537; batch adversarial loss: 0.539838\n",
      "epoch 49; iter: 0; batch classifier loss: 7.278372; batch adversarial loss: 0.530883\n",
      "epoch 50; iter: 0; batch classifier loss: 6.681669; batch adversarial loss: 0.484169\n",
      "epoch 51; iter: 0; batch classifier loss: 7.803533; batch adversarial loss: 0.463405\n",
      "epoch 52; iter: 0; batch classifier loss: 4.490322; batch adversarial loss: 0.469284\n",
      "epoch 53; iter: 0; batch classifier loss: 3.856228; batch adversarial loss: 0.471797\n",
      "epoch 54; iter: 0; batch classifier loss: 6.301878; batch adversarial loss: 0.441704\n",
      "epoch 55; iter: 0; batch classifier loss: 6.398008; batch adversarial loss: 0.600650\n",
      "epoch 56; iter: 0; batch classifier loss: 3.329650; batch adversarial loss: 0.465989\n",
      "epoch 57; iter: 0; batch classifier loss: 4.399462; batch adversarial loss: 0.490989\n",
      "epoch 58; iter: 0; batch classifier loss: 3.086243; batch adversarial loss: 0.432370\n",
      "epoch 59; iter: 0; batch classifier loss: 4.015261; batch adversarial loss: 0.512496\n",
      "epoch 60; iter: 0; batch classifier loss: 3.599471; batch adversarial loss: 0.467172\n",
      "epoch 61; iter: 0; batch classifier loss: 3.825002; batch adversarial loss: 0.568162\n",
      "epoch 62; iter: 0; batch classifier loss: 2.108548; batch adversarial loss: 0.530323\n",
      "epoch 63; iter: 0; batch classifier loss: 1.746735; batch adversarial loss: 0.479370\n",
      "epoch 64; iter: 0; batch classifier loss: 2.659571; batch adversarial loss: 0.485587\n",
      "epoch 65; iter: 0; batch classifier loss: 3.440312; batch adversarial loss: 0.458340\n",
      "epoch 66; iter: 0; batch classifier loss: 2.365382; batch adversarial loss: 0.437671\n",
      "epoch 67; iter: 0; batch classifier loss: 5.724104; batch adversarial loss: 0.579251\n",
      "epoch 68; iter: 0; batch classifier loss: 1.547963; batch adversarial loss: 0.439477\n",
      "epoch 69; iter: 0; batch classifier loss: 1.751155; batch adversarial loss: 0.461705\n",
      "epoch 70; iter: 0; batch classifier loss: 1.419798; batch adversarial loss: 0.423028\n",
      "epoch 71; iter: 0; batch classifier loss: 1.729592; batch adversarial loss: 0.470225\n",
      "epoch 72; iter: 0; batch classifier loss: 2.283039; batch adversarial loss: 0.404203\n",
      "epoch 73; iter: 0; batch classifier loss: 1.640347; batch adversarial loss: 0.446530\n",
      "epoch 74; iter: 0; batch classifier loss: 5.419665; batch adversarial loss: 0.604074\n",
      "epoch 75; iter: 0; batch classifier loss: 1.398837; batch adversarial loss: 0.428638\n",
      "epoch 76; iter: 0; batch classifier loss: 1.546277; batch adversarial loss: 0.375861\n",
      "epoch 77; iter: 0; batch classifier loss: 4.889842; batch adversarial loss: 0.595225\n",
      "epoch 78; iter: 0; batch classifier loss: 1.514507; batch adversarial loss: 0.464858\n",
      "epoch 79; iter: 0; batch classifier loss: 1.226492; batch adversarial loss: 0.503316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>independence</th>\n",
       "      <th>separation</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.212779</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_logreg</th>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.661063</td>\n",
       "      <td>0.658679</td>\n",
       "      <td>0.156145</td>\n",
       "      <td>0.135985</td>\n",
       "      <td>0.219985</td>\n",
       "      <td>0.743514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_logreg</th>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.676702</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.178660</td>\n",
       "      <td>0.182776</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.744191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RW_xgboost</th>\n",
       "      <td>0.878776</td>\n",
       "      <td>0.659640</td>\n",
       "      <td>0.622095</td>\n",
       "      <td>0.241415</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.736582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI_xgboost</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.703374</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.249480</td>\n",
       "      <td>0.757325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_sr</th>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.616791</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.218983</td>\n",
       "      <td>0.192961</td>\n",
       "      <td>0.298108</td>\n",
       "      <td>0.653869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metafair_fdr</th>\n",
       "      <td>0.636327</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.094583</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.728430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNAdvs</th>\n",
       "      <td>0.070612</td>\n",
       "      <td>0.509598</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.586583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_Platt</th>\n",
       "      <td>0.774267</td>\n",
       "      <td>0.642655</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.160609</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.752677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds</th>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.689735</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.068859</td>\n",
       "      <td>0.132417</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_weighted</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.671853</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>0.081264</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_fnr</th>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.572641</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.637717</td>\n",
       "      <td>0.062259</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_eqOdds_fpr</th>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.716306</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.289702</td>\n",
       "      <td>0.173573</td>\n",
       "      <td>0.177273</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_spd</th>\n",
       "      <td>0.683131</td>\n",
       "      <td>0.690341</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.020471</td>\n",
       "      <td>0.233097</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_aod</th>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.097395</td>\n",
       "      <td>0.149764</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_RejOpt_eod</th>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.097395</td>\n",
       "      <td>0.149764</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.745201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_Platt</th>\n",
       "      <td>0.771322</td>\n",
       "      <td>0.692160</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.218080</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.755708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.704284</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.117246</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.138399</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_weighted</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.632552</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.104839</td>\n",
       "      <td>0.064744</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_fnr</th>\n",
       "      <td>0.696939</td>\n",
       "      <td>0.637199</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>0.182084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_eqOdds_fpr</th>\n",
       "      <td>0.757551</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_spd</th>\n",
       "      <td>0.425758</td>\n",
       "      <td>0.636593</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.140199</td>\n",
       "      <td>0.160770</td>\n",
       "      <td>0.270513</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_aod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_RejOpt_eod</th>\n",
       "      <td>0.594040</td>\n",
       "      <td>0.652354</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.090571</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249084</td>\n",
       "      <td>0.756112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         best_threshold   bal_acc       acc  independence  \\\n",
       "logreg                         0.777755  0.711053  0.653333      0.212779   \n",
       "xgboost                        0.757551  0.699333  0.680000      0.109181   \n",
       "RW_logreg                      0.717143  0.661063  0.658679      0.156145   \n",
       "DI_logreg                      0.818163  0.676702  0.600000      0.178660   \n",
       "RW_xgboost                     0.878776  0.659640  0.622095      0.241415   \n",
       "DI_xgboost                     0.656531  0.703374  0.706667      0.096774   \n",
       "metafair_sr                    0.575714  0.616791  0.646667      0.218983   \n",
       "metafair_fdr                   0.636327  0.710750  0.660000      0.143921   \n",
       "NNAdvs                         0.070612  0.509598  0.346667      0.006203   \n",
       "logreg_Platt                   0.774267  0.642655  0.540000      0.306452   \n",
       "logreg_eqOdds                  0.717143  0.689735  0.660000      0.068859   \n",
       "logreg_eqOdds_weighted         0.656531  0.671853  0.706667      0.379653   \n",
       "logreg_eqOdds_fnr              0.656531  0.572641  0.693333      0.637717   \n",
       "logreg_eqOdds_fpr              0.777755  0.716306  0.653333      0.289702   \n",
       "logreg_RejOpt_spd              0.683131  0.690341  0.646667      0.020471   \n",
       "logreg_RejOpt_aod              0.712828  0.710750  0.660000      0.097395   \n",
       "logreg_RejOpt_eod              0.712828  0.710750  0.660000      0.097395   \n",
       "xgboost_Platt                  0.771322  0.692160  0.606667      0.387097   \n",
       "xgboost_eqOdds                 0.757551  0.704284  0.686667      0.117246   \n",
       "xgboost_eqOdds_weighted        0.696939  0.632552  0.646667      0.104839   \n",
       "xgboost_eqOdds_fnr             0.696939  0.637199  0.660000      0.137097   \n",
       "xgboost_eqOdds_fpr             0.757551  0.699333  0.680000      0.109181   \n",
       "xgboost_RejOpt_spd             0.425758  0.636593  0.673333      0.140199   \n",
       "xgboost_RejOpt_aod             0.594040  0.652354  0.673333      0.090571   \n",
       "xgboost_RejOpt_eod             0.594040  0.652354  0.673333      0.090571   \n",
       "\n",
       "                         separation  sufficiency       auc  \n",
       "logreg                     0.179526     0.260606  0.745201  \n",
       "xgboost                    0.045250     0.136364  0.756112  \n",
       "RW_logreg                  0.135985     0.219985  0.743514  \n",
       "DI_logreg                  0.182776     0.334783  0.744191  \n",
       "RW_xgboost                 0.037121     0.011749  0.736582  \n",
       "DI_xgboost                 0.155300     0.249480  0.757325  \n",
       "metafair_sr                0.192961     0.298108  0.653869  \n",
       "metafair_fdr               0.094583     0.160714  0.728430  \n",
       "NNAdvs                     0.031986     0.250000  0.586583  \n",
       "logreg_Platt               0.160609     0.921053  0.752677  \n",
       "logreg_eqOdds              0.132417     0.248882  0.745201  \n",
       "logreg_eqOdds_weighted     0.081264     0.133333  0.745201  \n",
       "logreg_eqOdds_fnr          0.062259     0.046448  0.745201  \n",
       "logreg_eqOdds_fpr          0.173573     0.177273  0.745201  \n",
       "logreg_RejOpt_spd          0.233097     0.381818  0.745201  \n",
       "logreg_RejOpt_aod          0.149764     0.260606  0.745201  \n",
       "logreg_RejOpt_eod          0.149764     0.260606  0.745201  \n",
       "xgboost_Platt              0.218080     0.937500  0.755708  \n",
       "xgboost_eqOdds             0.050997     0.138399  0.756112  \n",
       "xgboost_eqOdds_weighted    0.064744     0.184615  0.756112  \n",
       "xgboost_eqOdds_fnr         0.068472     0.182084  0.756112  \n",
       "xgboost_eqOdds_fpr         0.045250     0.136364  0.756112  \n",
       "xgboost_RejOpt_spd         0.160770     0.270513  0.756112  \n",
       "xgboost_RejOpt_aod         0.142913     0.249084  0.756112  \n",
       "xgboost_RejOpt_eod         0.142913     0.249084  0.756112  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dicts\n",
    "methods = dict()\n",
    "\n",
    "# Range of thresholds to evaluate our models\n",
    "thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "metrics_sweep = dict()\n",
    "\n",
    "# Store results from validation and test\n",
    "metrics_best_thresh_validate = dict()\n",
    "metrics_best_thresh_test = dict()\n",
    "\n",
    "data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups = GermanDataset1V()\n",
    "\n",
    "# Benchmarks\n",
    "BenchmarkLogistic(data_train, data_val, data_test)\n",
    "BenchmarkXGB(data_train, data_val, data_test)\n",
    "\n",
    "# Pre processing\n",
    "for model in modelsNames:\n",
    "    PreprocRW(data_train, data_val, data_test, privileged_groups, unprivileged_groups, model, do_results = True)\n",
    "    PreprocDI(data_train, data_val, data_test, sensitive_attribute, repair_level, model, do_results = True)\n",
    "\n",
    "for quality in quality_constraints_meta:\n",
    "    InprocMeta(data_train, data_val, data_test, sensitive_attribute, quality, tau = 0.8, do_results = True)\n",
    "# InprocPI(data_train, data_val, data_test, sensitive_attribute, eta = 50.0, do_results = True)\n",
    "InprocAdvs(data_train, data_val, data_test, privileged_groups, unprivileged_groups, do_results = True)\n",
    "\n",
    "\n",
    "# Post processing\n",
    "for model in modelsNames:\n",
    "    PosprocPlatt(data_train, data_val, data_test, privileged_groups, model)\n",
    "    PosprocEqoddsLABELS(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model)\n",
    "    for quality in quality_constraints_eqodds:\n",
    "        PosprocEqoddsSCORES(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, quality)\n",
    "    for key_metric in fair_metrics_optrej:\n",
    "        PosprocReject(data_train, data_val, data_test, unprivileged_groups, privileged_groups, model, key_metric)\n",
    "\n",
    "algorithm_performance_summary = pd.DataFrame(metrics_best_thresh_test).T\n",
    "algorithm_performance_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grids = {\n",
    "    'RW': {\n",
    "        \n",
    "    },\n",
    "    'DIR': {\n",
    "        'repair_level': [0.25, 0.5, 0.75]\n",
    "    },\n",
    "    'MetaFair': {\n",
    "        'tau': [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "    },\n",
    "    'PrejReg': {\n",
    "        'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# DI remover\n",
    "repair_level = 0.5                      \n",
    "dir_grid = {                           \n",
    "    'repair_level': [0.25, 0.5, 0.75]\n",
    "}                \n",
    "\n",
    "\n",
    "# MetaFair classifier\n",
    "quality_constraints_meta = ['sr', 'fdr']\n",
    "tau = 0.8   \n",
    "metafair_grid = {\n",
    "    'tau': [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Prejudice index regularizer\n",
    "pir_grid = {\n",
    "    'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "}\n",
    "\n",
    "# Adversarial learning\n",
    "pir_grid = {\n",
    "    'eta': [ 0.5, 5.0, 50.0, 500.0]\n",
    "}\n",
    "\n",
    "# Equal odds\n",
    "# Quality constraints\n",
    "quality_constraints_eqodds = [\"weighted\", 'fnr', 'fpr']\n",
    "\n",
    "# Option rejection\n",
    "# Fairness metrics\n",
    "fair_metrics_optrej = {\n",
    "    'spd': \"Statistical parity difference\",\n",
    "    'aod': \"Average odds difference\",\n",
    "    'eod': \"Equal opportunity difference\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
