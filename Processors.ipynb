{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of multistage and binary processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the idea of multistage and logical processors.\n",
    "\n",
    "A **multistage processor** is a fairness processor that modifies several steps of the algorithm making process. In particular, we will investigate a hybrid approach in which we combine known processors that affect different stages of the machine learning pipeline. We will consider the following three types of multistage processors:\n",
    "\n",
    "1. Pre processor + in processor.\n",
    "1. In processor + post processor.\n",
    "1. Pre processor + post processor.\n",
    "\n",
    "We will consider the following fairness processors to build the multistage processors:\n",
    "\n",
    "1. Pre processors:  \n",
    "    1.1 Reweighting.  \n",
    "    1.2 Disparate Impact Remover.  \n",
    "1. In processors:  \n",
    "    2.1 Metafair classifier.  \n",
    "    2.2 Adversarial Learning.  \n",
    "    2.3 Prejudice Index Regularizer.  \n",
    "1. Post processors:  \n",
    "    3.1 Equal odds processor.  \n",
    "    3.2 Option rejection.  \n",
    "    3.3 Platt scaling.   \n",
    "\n",
    "A **logical processor** is a tool used when dealing with multiple sensitive attributes or multilabel sensitive attributes which allows us to transform the prottected information into a binary variable for which many more fairness methods are available. We will restrict ourselves to the case of two sensitive variables and we will consider the following logical processors:\n",
    "\n",
    "1. OR processor.\n",
    "2. AND processor.\n",
    "3. XOR processor.\n",
    "\n",
    "The objectives of this notebook are the following:\n",
    "\n",
    "1. Implement both logical processors and multistage processors.\n",
    "1. Measure their performance in the context of credit scoring. \n",
    "1. Store our results for later analysis (we will proceed with the analysis of the results in the companion notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by making the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.AUTO_REUSE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# aif360\n",
    "# German dataset\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "# Pre processors\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "# In processors\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "# Post processors\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "\n",
    "# Custom imports\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a seed for reproductibility purposes and set it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains the data used. The three data sets that were considered were the following:\n",
    "\n",
    "1. Simulated data set (custom).\n",
    "1. German data set (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "1. Homecredit data set (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "\n",
    "We ended up discarding the Homecredit data set. Nonetheless, we show how we implemented the data processing of that data set as well.\n",
    "\n",
    "The characteristic of the data sets we used are summarized in the following table:\n",
    "\n",
    "| Data set | No. rows | No. features | Default rate | Sensitive group rate | Second sensitive group rate | OR group rate | AND group rate | XOR group rate| \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Simulation | 5000 | 3 | 0.17 | 0.50 | 0.5 | 0.75 | 0.25 | 0.50 | \n",
    "| German | 1000 | 61 | 0.30 | 0.15 | 0.19 | 0.39 | 0.11 | 0.29 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation schema is an adaptation of one due to Zhang et al. (https://arxiv.org/abs/1801.07593):\n",
    "\n",
    "1. Let $a_i^{(j)} \\in \\{0,1\\}$ with $i = 1,2; j = 1,..., N$ be picked uniformly at random. They will represent our sensitive variables.\n",
    "2. Let $v_i^{(j)} \\sim \\mathcal{N}(a_i^{(j)}, 1)$ be an innacurate measurement of the sensitive variables.\n",
    "3. Compute $v^{(j)} = (v_1^{(j)} + v_1^{(j)})/2$ their mean.\n",
    "4. Let $u^{(j)}, w^{(j)} \\sim \\mathcal{N}(v^{(j)},1)$ be two independent variables.\n",
    "5. The simulated data set is $D = (X, A, Y)$ where $X = (a_1^{(j)}, a_2^{(j)}, u^{(j)})$, $Y = (\\mathbf{1}(w^{(j)} > 0))_{j=1}^N$, $A = (a_1^{(j)}, a_2^{(j)})_{j = 1}^N$\n",
    "\n",
    "We sample $5000$ observations using this schema. The implementation can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a single sensitive variable, we use just $a_1$ as our sensitive variable. In the case of multiple sensitive variables we use both $a_1$ and $a_2$ and use a logical processor to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "#                          SIMULATION DATASET\n",
    "#=========================================================================\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          One variable\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def simul1V(seed: int = 12345, N: int = 5000, p1: float = 0.5, p2: float = 0.5):\n",
    "    \"\"\"\n",
    "    Obtain a simulated dataset from the toy model for the case of one sensitive variable\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        N (int): number of individuals in the dataset.\n",
    "        p1 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the first sensitive variable.\n",
    "        p2 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the second sensitive variable.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the simulation.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the simulation.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the simulation.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create variables\n",
    "    vars = dict()\n",
    "\n",
    "    # Sensitive variables (drawn from a binomial distribution)\n",
    "    vars['sens1'] = np.random.binomial(n = 1, p = p1, size = N)\n",
    "    vars['sens2'] = np.random.binomial(n = 1, p = p2, size = N)\n",
    "\n",
    "    # v1, v2 (noisy measurements of the sensitive variables) and their sum\n",
    "    vars['v1'] = np.random.normal(loc = vars['sens1'], scale = 1.0, size = N)\n",
    "    vars['v2'] = np.random.normal(loc = vars['sens2'], scale = 1.0, size = N)\n",
    "    vars['unrelated1'] = np.random.normal(loc = 0.5, scale = 1.0, size = N)\n",
    "    vars['unrelated2'] = np.random.normal(loc = 0.5, scale = 1.0, size = N)\n",
    "    vars['mean'] = np.mean(vars['v1'] + vars['v2'] + vars['unrelated1'] + vars['unrelated2'])\n",
    "\n",
    "    # Noisy measurements of the sum of v1 and v2\n",
    "    vars['indirect'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "    vars['weight_response'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "\n",
    "    # Response variable\n",
    "    vars['response'] = vars['weight_response'] > 0.0\n",
    "\n",
    "    # Create the dataset with the correct variables\n",
    "    final_vars = ['sens1', 'sens2', 'indirect', 'unrelated1', 'unrelated2', 'response']\n",
    "    df = dict()\n",
    "    for name in final_vars:\n",
    "        df[name] = vars[name]\n",
    "    \n",
    "    # Transform the sensitive variables to boolean\n",
    "    df['sens1'] = df['sens1'] == 1\n",
    "    df['sens2'] = df['sens2'] == 1\n",
    "\n",
    "    # Create the dataset from the dictionary\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Convert to standard dataset\n",
    "    data = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['sens1'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = ['sens2']\n",
    "    )\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = data.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = data.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(data)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Two variables\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def simul2V(seed: int = 12345, operation: str = \"OR\", N: int = 5000, p1: float = 0.5, p2: float = 0.5):\n",
    "    \"\"\"\n",
    "    Obtain a simulated dataset from the toy model for the case of two sensitive variables\n",
    "    =====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        N (int): number of individuals in the dataset.\n",
    "        p1 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the first sensitive variable.\n",
    "        p2 (float, between 0.0 and 1.0): probability for a binomial distribution from which to draw the second sensitive variable.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the simulation with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute.\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create variables\n",
    "    vars = dict()\n",
    "\n",
    "    # Sensitive variables (drawn from a binomial distribution)\n",
    "    vars['sens1'] = np.random.binomial(n = 1, p = p1, size = N)\n",
    "    vars['sens2'] = np.random.binomial(n = 1, p = p2, size = N)\n",
    "\n",
    "    # v1, v2 (noisy measurements of the sensitive variables) and their sum\n",
    "    vars['v1'] = np.random.normal(loc = vars['sens1'], scale = 1.0, size = N)\n",
    "    vars['v2'] = np.random.normal(loc = vars['sens2'], scale = 1.0, size = N)\n",
    "    vars['unrelated1'] = np.random.normal(loc = 0.5, scale = 1.0, size = N)\n",
    "    vars['unrelated2'] = np.random.normal(loc = 0.5, scale = 1.0, size = N)\n",
    "    vars['mean'] = np.mean(vars['v1'] + vars['v2'] + vars['unrelated1'] + vars['unrelated2'])\n",
    "\n",
    "    # Noisy measurements of the sum of v1 and v2\n",
    "    vars['indirect'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "    vars['weight_response'] = np.random.normal(loc = vars['mean'], scale = 1.0, size = N)\n",
    "\n",
    "    # Response variable\n",
    "    vars['response'] = vars['weight_response'] > 0.0\n",
    "\n",
    "    # Create the dataset with the correct variables\n",
    "    final_vars = ['sens1', 'sens2', 'indirect', 'unrelated1', 'unrelated2', 'response']\n",
    "    df = dict()\n",
    "    for name in final_vars:\n",
    "        df[name] = vars[name]\n",
    "    \n",
    "    df['sens1'] = df['sens1'] == 1\n",
    "    df['sens2'] = df['sens2'] == 1\n",
    "\n",
    "    # Apply bitwise operation\n",
    "    if operation == 'OR':\n",
    "        df['prot_attr'] = np.logical_or(df['sens1'], df['sens2'])\n",
    "\n",
    "    elif operation == 'AND':\n",
    "        df['prot_attr'] = np.logical_and(df['sens1'], df['sens2'])\n",
    "\n",
    "    elif operation == 'XOR':\n",
    "        df['prot_attr'] = np.logical_xor(df['sens1'], df['sens2'])\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Convert to standard datasets\n",
    "    data_single = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['sens1'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    data = utils.convert_to_standard_dataset(\n",
    "        df=df,\n",
    "        target_label_name = 'response',\n",
    "        sensitive_attribute = ['prot_attr'],\n",
    "        priviledged_classes = [lambda x: x == 1],\n",
    "        favorable_target_label = [1],\n",
    "        features_to_keep = [],\n",
    "        categorical_features = []\n",
    "    )\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = data.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    _, vt_single = data_single.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt_single.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = data.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(data)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German data set (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data) is a typical benchmark in credit scoring which we used to explore our methods. In the code cells below you will find how we download the data. \n",
    "In the case of a single sensitive variable we used age, considering that individuals whose age is below $25$ to be vulnerable to discrimination. In the case of multiple sensitive variables we also used gender, considering that women may face discrimination in credit scoring, and then a logical processor was used to handle this situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "#                          GERMAN DATASET\n",
    "#=========================================================================\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          One variable\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "def GermanDataset1V(seed = 12345):\n",
    "    \"\"\"\n",
    "    Read and preprocess the German dataset for the case of one sensitive variable\n",
    "    (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the German dataset.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the German dataset.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the German dataset.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Read the data\n",
    "    dataset_german = GermanDataset(\n",
    "            protected_attribute_names=['age'],            \n",
    "            privileged_classes=[lambda x: x >= 25],      \n",
    "            features_to_drop=['personal_status', 'sex'] \n",
    "        )\n",
    "        \n",
    "    # xgboost requires labels to start at zero\n",
    "    dataset_german.labels[dataset_german.labels.ravel() == 2] =  dataset_german.labels[dataset_german.labels.ravel() == 2] - 2\n",
    "    dataset_german.unfavorable_label = dataset_german.unfavorable_label - 2\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = dataset_german.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We obtain sensitive attribute\n",
    "    sensitive_attribute = dataset_german.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Two variables\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def GermanDataset2V(seed = 12345, operation = \"OR\"):\n",
    "    \"\"\"\n",
    "    Read and preprocess the German dataset for the case of two sensitive variables\n",
    "    (https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the German dataset with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Read the data\n",
    "    dataset = GermanDataset(\n",
    "        protected_attribute_names=['age'],            \n",
    "        privileged_classes=[lambda x: x >= 25],      \n",
    "        features_to_drop=['personal_status', 'sex'] \n",
    "    )\n",
    "\n",
    "    # load the german dataset and update the data with the OR sum of sex and age\n",
    "    dataset_german_upd = utils.update_german_dataset_from_multiple_protected_attributes(dataset, operation)\n",
    "\n",
    "    # change favorable/unfavorable labels to 1: good; 0: bad\n",
    "    dataset_german_upd.labels[dataset_german_upd.labels.ravel() == 2] =  dataset_german_upd.labels[dataset_german_upd.labels.ravel() == 2] - 2\n",
    "    dataset_german_upd.unfavorable_label = dataset_german_upd.unfavorable_label - 2\n",
    "\n",
    "    # For the single dataset as well\n",
    "    dataset.labels[dataset.labels.ravel() == 2] =  dataset.labels[dataset.labels.ravel() == 2] - 2\n",
    "    dataset.unfavorable_label = dataset.unfavorable_label - 2\n",
    "\n",
    "    # Train, val, test split\n",
    "    data_train, vt = dataset_german_upd.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # We do the same on the single variable dataset\n",
    "    _, vt = dataset.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = dataset_german_upd.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_german_upd)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homecredit data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also considered the Homecredit data set (https://www.kaggle.com/c/home-credit-default-risk), although we did not use it in the end. The sensitive variables are the same as in German data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "#                          HOMECREDIT DATASET\n",
    "#=========================================================================\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Data handling\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "def LoadHomecredit(\n",
    "        seed: int = 12345,\n",
    "        sample_size: int = 5000\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Reads the homecredit dataset, obtains a sample and store it in the 'data/' folder\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        sample_size (int): size of the sample \n",
    "    Outputs:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # We set a seed    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # We download the data\n",
    "    homecredit = pd.read_csv('data/homecredit.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "    nrows = homecredit.shape[0]\n",
    "\n",
    "    # We sample the dataset to make it more maneagable\n",
    "    ssample = np.random.choice(nrows, size = sample_size, replace = False)\n",
    "    homecredit = homecredit.iloc[ssample, :]\n",
    "    homecredit = homecredit.reset_index(drop=True)\n",
    "    \n",
    "    # We store the homecredit dataset in the data folder\n",
    "    path = 'data/'\n",
    "    with open(path + 'homecredit.pickle', 'wb') as handle:\n",
    "        pickle.dump(homecredit, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "    \n",
    "def ReadHomecredit():\n",
    "    \"\"\"\n",
    "    Reads the sample from the homecredit dataset that we stored in the 'data/' folder\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        None\n",
    "    Outputs:\n",
    "        homecredit (pd.DataFrame): dataframe that contains a subsample from the homecredit dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    homecredit = pd.read_pickle('data/homecredit.pickle')\n",
    "    return homecredit\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          One variable\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Homecredit1V(seed = 12345, subsample = None):\n",
    "    \"\"\"\n",
    "    Read and preprocess the Homecredit dataset for the case of one sensitive variable\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        dataset_homecredit (pd.DataFrame): subsample of the homecredit dataset\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the Homecredit dataset.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the Homecredit dataset.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the Homecredit dataset.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute .\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if subsample:\n",
    "        # Read the data\n",
    "        dataset_homecredit = ReadHomecredit()\n",
    "    else: \n",
    "        dataset_homecredit = pd.read_csv('data/homecredit.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "    # Make a copy of the dataset\n",
    "    homecredit = dataset_homecredit.copy(deep = True)\n",
    "\n",
    "    # Pre process\n",
    "    homecredit = utils.preprocess_homecredit(homecredit)\n",
    "\n",
    "    # Transform to standard dataset\n",
    "    dataset_homecredit_aif = utils.convert_to_standard_dataset(\n",
    "            df=homecredit,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute='AGE',\n",
    "            priviledged_classes=[lambda x: x >= 25],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "    \n",
    "    # Perform train, test, val split\n",
    "    data_train, vt = dataset_homecredit_aif.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = dataset_homecredit_aif.protected_attribute_names[0] # age\n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(dataset_homecredit_aif)\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "#                          Two variables\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Homecredit2V(seed = 12345, operation = \"OR\", subsample = None):\n",
    "    \"\"\"\n",
    "    Read and preprocess the Homecredit dataset for the case of one sensitive variable\n",
    "    (https://www.kaggle.com/c/home-credit-default-risk).\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        dataset_homecredit (pd.DataFrame): subsample of the homecredit dataset\n",
    "        seed (int): seed needed to ensure reproductibility.\n",
    "        operation (str): bitwise operation that we apply to the sensitive variables.\n",
    "                         Allowed values: \"OR\", \"AND\", \"XOR\".\n",
    "        \n",
    "    Outputs:\n",
    "        data_train (aif360.StandardDataset): Train dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_val (aif360.StandardDataset): Validation dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        data_test (aif360.StandardDataset): Test dataset obtained from the Homecredit dataset with a bitwise operation applied to two sensitive variables.\n",
    "        sensitive_attribute (str): Name of the sensitive attribute.\n",
    "        privileged_groups (list): list that stores a dictionary with the sensitive attribute and the privileged label.\n",
    "        unprivileged_groups (list): list that stores a dictionary with the sensitive attribute and the unprivileged label.\n",
    "        data_val_single (aif360.StandardDataset): Validation dataset with just one sensitive variable.\n",
    "        data_test_single (aif360.StandardDataset): Test dataset with just one sensitive variable.\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if subsample:\n",
    "        # Read the data\n",
    "        dataset_homecredit = ReadHomecredit()\n",
    "    else: \n",
    "        dataset_homecredit = pd.read_csv('data/homecredit.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "    # Copy the dataset\n",
    "    homecredit = dataset_homecredit.copy(deep = True)\n",
    "\n",
    "    # Pre process the data\n",
    "    homecredit = utils.preprocess_homecredit_mult(homecredit, operation = operation)\n",
    "    homecredit_single = homecredit.copy(deep = True)\n",
    "    \n",
    "    # Transform both datasets to aif360 format\n",
    "    homecredit = utils.convert_to_standard_dataset(\n",
    "            df=homecredit,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute=['PROT_ATTR'],\n",
    "            priviledged_classes=[lambda x: x == 1],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    homecredit_single = utils.convert_to_standard_dataset(\n",
    "            df=homecredit_single,\n",
    "            target_label_name='TARGET',\n",
    "            sensitive_attribute=['AGE'],\n",
    "            priviledged_classes=[lambda x: x >= 25],\n",
    "            favorable_target_label=[1],\n",
    "            features_to_keep=[],\n",
    "            categorical_features=[])\n",
    "\n",
    "    # train, val, test split\n",
    "    data_train, vt = homecredit.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val, data_test = vt.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    _, vt_single = homecredit.split([0.7], shuffle=True, seed=seed)\n",
    "    data_val_single, data_test_single = vt_single.split([0.5], shuffle=True, seed=seed)\n",
    "\n",
    "    # Obtain sensitive attributes and privileged groups\n",
    "    sensitive_attribute = homecredit.protected_attribute_names[0] \n",
    "    privileged_groups, unprivileged_groups = utils.get_privileged_groups(homecredit)\n",
    "\n",
    "    return data_train, data_val, data_test, sensitive_attribute, privileged_groups, unprivileged_groups, data_val_single, data_test_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce two functions that initialize preliminary data. The data we load is a list with the names of the models that will be inserted into a pre processor or post processor, a dictionary which relates those names to their corresponding functions and a dictionary of dictionaries that stores the kwargs of each method. In the case we also load the names of the models to whom we need to apply a post processor later (that is, the names of pre processors and the names of in processors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtainPrelDataSingle() -> tuple[dict]:\n",
    "    \"\"\"\n",
    "    Compute the results dictionary in the univariate case\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        None\n",
    "        \n",
    "    Outputs:\n",
    "        modelsNames (list): name of the models to whom we will apply a pre processor on post processor.\n",
    "        modelsTrain (dictionary): dictionary that relates the previous names with their functions.\n",
    "        modelsArgs (dictionary): dictionary that stores for each of the previous names the corresponding keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    # names \n",
    "    modelsNames = [\n",
    "        'logreg',\n",
    "        'xgboost'\n",
    "    ]\n",
    "\n",
    "    modelsTrain = {\n",
    "        'logreg': LogisticRegression,\n",
    "        'xgboost': XGBClassifier\n",
    "    }\n",
    "\n",
    "    modelsArgs = {\n",
    "        'logreg': {\n",
    "            'solver': 'liblinear',\n",
    "            'random_state': seed\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'eval_metric': 'error',\n",
    "            'eta':0.1,\n",
    "            'max_depth':6,\n",
    "            'subsample':0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return modelsNames, modelsTrain, modelsArgs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups) -> tuple[dict]:\n",
    "    \"\"\"\n",
    "    Compute the results dictionary in the univariate case\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        None\n",
    "        \n",
    "    Outputs:\n",
    "        modelsNames (list): name of the models to whom we will apply a pre processor on post processor.\n",
    "        modelsBenchmark (list): name of the models that are not fairness processors themselves.\n",
    "        modelsPost (list): name of the models to whom we will apply a post processor.\n",
    "        modelsTrain (dictionary): dictionary that relates the previous names with their functions.\n",
    "        modelsArgs (dictionary): dictionary that stores for each of the previous names the corresponding keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Names of the models \n",
    "    modelsNames = [\n",
    "        'logreg',\n",
    "        'xgboost',\n",
    "        'adversarial',\n",
    "        'metafair',\n",
    "        'pir'\n",
    "    ]\n",
    "\n",
    "    # Which models are previous benchmarks\n",
    "    modelsBenchmark = [\n",
    "        'logreg',\n",
    "        'xgboost'\n",
    "    ]\n",
    "\n",
    "    # Which models are fairness processors\n",
    "    modelsFair = [\n",
    "        'adversarial',\n",
    "        'metafair_sr',\n",
    "        'metafair_fdr',\n",
    "        'pir'\n",
    "    ]\n",
    "\n",
    "    # We obtain the names of pre processors + benchmarks (later we will apply a post processor)\n",
    "    modelsPre = [\n",
    "        prefix + '_' + model_name for prefix in ['RW', 'DI'] for model_name in modelsBenchmark\n",
    "    ]\n",
    "\n",
    "    # modelsPost is a list with the names of the models to whom we need to apply a post processor later\n",
    "    # (i.e. pre processors or in processors)\n",
    "    modelsPost = modelsPre + modelsFair\n",
    "\n",
    "    # Names of the models with their functions\n",
    "    modelsTrain = {\n",
    "        'logreg': LogisticRegression,\n",
    "        'xgboost': XGBClassifier,\n",
    "        'adversarial': AdversarialDebiasing,\n",
    "        'metafair': MetaFairClassifier,\n",
    "        'pir': PrejudiceRemover\n",
    "    }\n",
    "\n",
    "    # Dictionary of kwargs\n",
    "    modelsArgs = {\n",
    "        'logreg': {\n",
    "            'solver': 'liblinear',\n",
    "            'random_state': seed\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'eval_metric': 'error',\n",
    "            'eta':0.1,\n",
    "            'max_depth':6,\n",
    "            'subsample':0.8\n",
    "        },\n",
    "        'adversarial': {\n",
    "            'privileged_groups': privileged_groups,\n",
    "            'unprivileged_groups': unprivileged_groups,\n",
    "            'scope_name': 'debiased_classifier',\n",
    "            'debias': True,\n",
    "            'num_epochs': 80\n",
    "        },\n",
    "        'metafair': {\n",
    "            'tau': 0.8,\n",
    "            'sensitive_attr': sensitive_attribute,\n",
    "            'type': 'sr',\n",
    "            'seed': seed\n",
    "        },\n",
    "    #    'metafair_fdr': {\n",
    "    #        'tau': 0.8,\n",
    "    #        'sensitive_attribute': sensitive_attribute,\n",
    "    #        'type': 'fdr',\n",
    "    #        'seed': seed\n",
    "    #    },\n",
    "        'pir': {\n",
    "            'sensitive_attr': sensitive_attribute,\n",
    "            'eta': 50.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return modelsNames, modelsBenchmark, modelsPost, modelsTrain, modelsArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce the functions we used to produce our results and compute our metrics. These functions compute, given a score, the threshold that maximizes balanced accuracy and the metrics for that given threshold. In particular, we compute:\n",
    "\n",
    "1. Accuracy.\n",
    "2. Balanced accuracy.\n",
    "3. Independence.\n",
    "4. Separation.\n",
    "5. Sufficiency.\n",
    "\n",
    "The implementation can be found in the code cells below and requires the use of methods in the *utils.py* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(val: pd.DataFrame, test: pd.DataFrame, method: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the results dictionary in the univariate case\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        val (pd.DataFrame): validation data set with LP.\n",
    "        test (pd.DataFrame): validation data set with LP. \n",
    "        method (str): name of the model whose results we want to compute.\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  modifies the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep(\n",
    "        dataset=val,\n",
    "        model=methods[method],\n",
    "        thresh_arr=thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(\n",
    "        metrics_sweep[method]\n",
    "        )\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics(\n",
    "        dataset=test, \n",
    "        model=methods[method], \n",
    "        threshold=metrics_best_thresh_validate[method]['best_threshold'])\n",
    "    \n",
    "\n",
    "\n",
    "def results_mult(val: pd.DataFrame, val_single: pd.DataFrame, test: pd.DataFrame, test_single: pd.DataFrame, method: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute the results dictionary in the multivariate case\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        val (pd.DataFrame): validation data set with LP.\n",
    "        val_single (pd.DataFrame): validation data set with a single sensitive variable.\n",
    "        test (pd.DataFrame): validation data set with LP.\n",
    "        test_single (pd.DataFrame): validation data set with a single sensitive variable.\n",
    "        method (str): name of the model whose results we want to compute.\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  modifies the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[method] = utils.metrics_threshold_sweep_mult(\n",
    "        dataset = val,\n",
    "        dataset_single = val_single,\n",
    "        model = methods[method],\n",
    "        thresh_arr = thresh_sweep\n",
    "    )\n",
    "\n",
    "    # Evaluate the metrics for the best threshold\n",
    "    metrics_best_thresh_validate[method] = utils.describe_metrics(metrics_sweep[method])\n",
    "\n",
    "    # Compute the metrics in test using the best threshold for validation\n",
    "    metrics_best_thresh_test[method] = utils.compute_metrics_mult(\n",
    "        dataset = test, \n",
    "        dataset_single = test_single,\n",
    "        model = methods[method], \n",
    "        threshold = metrics_best_thresh_validate[method]['best_threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two machine learning models as benchmarks and to train pre processors and post processors. In particular, we have chosen logistic regression and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BenchmarkLogistic():\n",
    "    \"\"\"\n",
    "    Training and validation of a logistic regression model\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        None\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'logreg'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'sample_weight': train.instance_weights}\n",
    "\n",
    "    # Introduce the model in the model dict\n",
    "    methods[model_name] = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel(), **fit_params)\n",
    "\n",
    "    # Obtain results\n",
    "    if nvar == 1:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "    elif nvar == 2:\n",
    "        val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "        results_mult(val, val_single, test, test_single, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BenchmarkXGB():\n",
    "    \"\"\"\n",
    "    Training and validation of a XGBoost model\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        None\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'xgboost'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Model parameters\n",
    "    fit_params = {'eval_metric': 'error', 'eta':0.1, 'max_depth':6, 'subsample':0.8}\n",
    "\n",
    "    # Assign the correct dict\n",
    "    methods[model_name] = XGBClassifier(**fit_params)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train.features, train.labels.ravel())\n",
    "\n",
    "    # Obtain results\n",
    "    if nvar == 1:\n",
    "        results(val, test, model_name)\n",
    "\n",
    "    elif nvar == 2:\n",
    "        val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "        results_mult(val, val_single, test, test_single, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by studying the preprocessors. These are methods that modify the data set before any model has been applied. \n",
    "In particular we will review the following procedures:\n",
    "\n",
    "1. Reweighting.\n",
    "2. Disparate Impact Remover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweighting adjusts the sampling frequency in the data to make the prior probabilities closer to those expected from independence. The weights used to achieve this are given by the expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W(A = a| Y = y) = \\frac{\\mathbb{P}_{exp}(A = a, Y =y)}{\\mathbb{P}_{act}(A = a, Y = y)} \\approx \\frac{\\hat{\\mathbb{P}}(A = a) \\hat{\\mathbb{P}}(Y =y)}{\\hat{\\mathbb{P}}(A = a, Y = y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#=========================================================================\n",
    "#                          REWEIGHTING\n",
    "#=========================================================================\n",
    "\n",
    "\n",
    "def PreprocRW(model, do_results = True):\n",
    "    \"\"\"\n",
    "    Implement the reweighting processor and then applies a given model\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        model (sklearn or aif360 model): The model to whom we are going to apply reweighting\n",
    "        do_results (boolean): If true, it modifies the results dictionaries in place. \n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"RW\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Call the processor\n",
    "    PreProcessor = Reweighing(\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "    # Transform the data\n",
    "    PreProcessor.fit(train)\n",
    "    trainRW = PreProcessor.transform(train)\n",
    "    valRW = PreProcessor.transform(test)\n",
    "    testRW = PreProcessor.transform(val)\n",
    "\n",
    "    # Train the model\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        modelsArgs[model]['sess'] = tf.Session()\n",
    "\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    if model in modelsBenchmark:\n",
    "        if model == 'logreg':\n",
    "            fit_params = {'sample_weight': trainRW.instance_weights}\n",
    "            methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel(), **fit_params)\n",
    "        else:\n",
    "            methods[model_name] = Algorithm.fit(trainRW.features, trainRW.labels.ravel())\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainRW)\n",
    "            \n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(valRW, testRW, model_name)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(valRW, val_single, testRW, test_single, model_name)\n",
    "\n",
    "    if model == 'adversarial':\n",
    "        modelsArgs[model]['sess'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: Calders, T., Kamiran, F., & Pechenizkiy, M. (2009, December). Building classifiers with independency constraints. In 2009 IEEE international conference on data mining workshops (pp. 13-18). IEEE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disparate Impact Remover proposes a repaired dataset, $\\overline{D}$ obtained through a certain median distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\overline{x} = F_M(F_a^{-1}(x))\\quad  \\text{where } A(x) = a, \\,  F_M^{-1} (u) = \\text{median}\\{ F_a^{-1}(u) | a \\in A\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This greatly compromises predictive power so one can adjust the performance-fairness trade-off with the following linear interpolation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_{M_a}^{-1} (\\alpha) = (1-\\lambda) (F_a)^{-1} (\\alpha)  + \\lambda (F_M)^{-1} (\\alpha) \\quad \\text{where }\\lambda \\in[0,1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "#                          DISPARATE IMPACT REMOVER\n",
    "#=========================================================================\n",
    "\n",
    "\n",
    "def PreprocDI(repair_level, model, do_results = True):\n",
    "    \"\"\"\n",
    "    Implement the reweighting processor and then applies a given model\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        repair_level (float between 0 and 1): Parameter that controls the level of repair.\n",
    "            The closer it is to one, the fairer the data set.\n",
    "        model (sklearn or aif360 model): The model to whom we are going to apply reweighting.\n",
    "        do_results (boolean): If true, it modifies the results dictionaries in place. \n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assign the correct name\n",
    "    method = \"DI\"\n",
    "    model_name = method + \"_\" + model\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Initialize the processor\n",
    "    PreProcessor = DisparateImpactRemover(\n",
    "        repair_level=repair_level,\n",
    "        sensitive_attribute=sensitive_attribute\n",
    "    )\n",
    "    # Transform the data\n",
    "    PreProcessor.fit_transform(train)\n",
    "    trainDI = PreProcessor.fit_transform(train)\n",
    "    valDI = PreProcessor.fit_transform(val)\n",
    "    testDI = PreProcessor.fit_transform(test)\n",
    "\n",
    "    # Train the model\n",
    "    # If we are training adversarial debiasing we need a tf session.\n",
    "    if model == 'adversarial':\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        modelsArgs[model]['sess'] = tf.Session()\n",
    "\n",
    "    Algorithm = modelsTrain[model](**modelsArgs[model])\n",
    "\n",
    "    # This logic handles whether or not the model is a sklearn model or a aif360 model.\n",
    "    if model in modelsBenchmark:\n",
    "        if model == 'logreg':\n",
    "            fit_params = {'sample_weight': trainDI.instance_weights}\n",
    "            methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel(), **fit_params)\n",
    "        else:\n",
    "            methods[model_name] = Algorithm.fit(trainDI.features, trainDI.labels.ravel())\n",
    "    else:\n",
    "        methods[model_name] = Algorithm.fit(trainDI)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(valDI, testDI, model_name)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(valDI, val_single, testDI, test_single, model_name)\n",
    "\n",
    "    # If we are dealing with adversarial debiasing we close the session\n",
    "    if model == 'adversarial':\n",
    "        modelsArgs[model]['sess'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015, August). Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 259-268)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta fair classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meta fair classifier requires that we define a group performance measure, $q_a$, and then tries to implement fairness by adding a constraint on the minimum quotient of the group performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min \\mathbb{P}(\\hat{Y} \\neq Y ) \\quad \n",
    "\\text{s.t } \\min_{a\\in A} q_a / \\max_{a\\in A} q_a \\geq \\tau$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InprocMeta(quality: str,  tau: float = 0.8, do_results: bool = True):\n",
    "    \"\"\"\n",
    "    Implement the meta fair in processor\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        quality (str): \"fdr\" for false discovery ratio, \"sr\" for statistical rate.\n",
    "        tau (float): penalty parameter of the fairness constraint.\n",
    "        do_results (boolean): If true, it modifies the results dictionaries in place. \n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # assign the correct name\n",
    "    model_name = \"metafair\"\n",
    "    model_name_quality = '{}_{}'.format(model_name, quality)\n",
    "\n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name_quality] = MetaFairClassifier(\n",
    "        tau=tau,\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        type=quality,\n",
    "        seed=seed\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_quality] = methods[model_name_quality].fit(train)\n",
    "\n",
    "    # Obtain scores\n",
    "    methods[model_name_quality].scores_train = methods[model_name_quality].predict(train).scores\n",
    "    methods[model_name_quality].scores_val = methods[model_name_quality].predict(val).scores\n",
    "    methods[model_name_quality].scores_test = methods[model_name_quality].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        if nvar == 1:\n",
    "            results(val, test, model_name_quality)\n",
    "\n",
    "        elif nvar == 2:\n",
    "            global data_val_single\n",
    "            global data_test_single\n",
    "            val_single, test_single = data_val_single.copy(deepcopy = True), data_test_single.copy(deepcopy = True)\n",
    "            results_mult(val, val_single, test, test_single, model_name_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Celis, L. E., Huang, L., Keswani, V., & Vishnoi, N. K. (2019, January). Classification with fairness constraints: A meta-algorithm with provable guarantees. In Proceedings of the conference on fairness, accountability, and transparency (pp. 319-328)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prejudice index regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method adds a regularizer in the form of the prejudice index (PI) to penalize the use of mutual information of the sensitive attribute on the response of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ PI = \\sum_{\n",
    "(y,a) \\in D\n",
    "} \\mathbb{P}(y,s) \\log \\frac{\n",
    "\\mathbb{P}(y,s)}{{\\mathbb{P}(y)\\mathbb{P}(s)}\n",
    "} \\approx \\sum_{\n",
    "(x_i, a_i) \\in D\n",
    "} \\sum_{\n",
    " y \\in \\{0, 1\\}\n",
    "} f(y|x_i, a_i) \\log \\frac{\\hat{\\mathbb{P}}(y|a)}{\\hat{\\mathbb{P}}(y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InprocPI(eta = 50.0, do_results = True):\n",
    "    \"\"\"\n",
    "    Implement the prejudice index regularizer in processor\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        eta (float): parameter that weights the importance given to the regularizer (similar to lambda in lasso regression).\n",
    "        do_results (boolean): If true, it modifies the results dictionaries in place. \n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name = 'pir'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    # Initialize the model and store it in the dictionary\n",
    "    methods[model_name] = PrejudiceRemover(\n",
    "        sensitive_attr=sensitive_attribute,\n",
    "        eta=eta\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name] = methods[model_name].fit(train)\n",
    "    \n",
    "    # Obtain scores\n",
    "    methods[model_name].scores_train = methods[model_name].predict(train).scores\n",
    "    methods[model_name].scores_val = methods[model_name].predict(val).scores\n",
    "    methods[model_name].scores_test = methods[model_name].predict(test).scores\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Kamishima, T., Akaho, S., Asoh, H., & Sakuma, J. (2012). Fairness-aware classifier with prejudice remover regularizer. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23 (pp. 35-50). Springer Berlin Heidelberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial debiasing proposes the use of an adversarial classifier that tries to predict the sensitive attribute using the predictions of the machine learning algorithm. This requires updating the gradient of the loss function so it avoids benefiting the adversarial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla _W L - \\text{proj}_{\\nabla_W L_A} \\nabla_WL - \\alpha \\nabla_W L_A$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InprocAdvs(do_results = True):\n",
    "    \"\"\"\n",
    "    Implement the adversarial debiasing in processor\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        do_results (boolean): If true, it modifies the results dictionaries in place. \n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assign the correct name\n",
    "    model_name = 'adversarial'\n",
    "    \n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "    \n",
    "    #We train the model\n",
    "    methods[model_name] = AdversarialDebiasing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups,\n",
    "        scope_name = 'debiased_classifier',\n",
    "        debias=True,\n",
    "        sess=sess,\n",
    "        num_epochs=80\n",
    "    )    \n",
    "    methods[model_name].fit(train)\n",
    "\n",
    "    # Obtain results\n",
    "    if do_results:\n",
    "        results(val, test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Zhang, B. H., Lemoine, B., & Mitchell, M. (2018, December). Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 335-340)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platt scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Platt scaling is a procedure that allows to generate probabilities from predictions. Therefore it can be used to calibrate a score. Hence, if we use it to calibrate a score by groups then we can achieve fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PosprocPlatt(model_name):\n",
    "    \"\"\"\n",
    "    Implement the Platt scaling by groups post processor\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        model_name (str): Name of the model we want to do post processing to. \n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Assign the correct name\n",
    "    fairness_method = '_Platt'\n",
    "\n",
    "    # Validation\n",
    "    #---------------\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy = True), data_val.copy(deepcopy = True), data_test.copy(deepcopy = True)\n",
    "\n",
    "    # Copy the predictions\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Platt Scaling:\n",
    "    #---------------\n",
    "    #1. Split training data on sensitive attribute\n",
    "    val_preds_priv, val_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = val_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    \n",
    "    #2. Copy validation data predictions\n",
    "    val_preds2 = val_preds.copy(deepcopy = True)\n",
    "    \n",
    "    #3. Make one model for each group\n",
    "    sensitive_groups_data = {'priv': [val_preds_priv, priv_indices],\n",
    "                             'unpriv': [val_preds_unpriv, unpriv_indices]}\n",
    "    for group, data_group_list in sensitive_groups_data.items():\n",
    "        # Assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "        # Initialize the model, store it in the dict\n",
    "        methods[model_name_group] = LogisticRegression()\n",
    "        # Train the model using the validation data divided by group\n",
    "        methods[ model_name_group ] = methods[model_name_group].fit(\n",
    "            data_group_list[0].scores,   # data_group_list[0] -> data_val_preds_priv or data_val_preds_unpriv\n",
    "            val.subset(data_group_list[1]).labels.ravel()\n",
    "        ) # data_group_list[1] -> priv_indices or unpriv_indices\n",
    "\n",
    "        # predict group probabilities, store in val_preds2\n",
    "        # Platt scores are given by the predictions of the posterior probabilities\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        val_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "   \n",
    "    # Evaluate the model in a range of values\n",
    "    thresh_sweep_platt = np.linspace(np.min(val_preds2.scores.ravel()),\n",
    "                                     np.max(val_preds2.scores.ravel()),\n",
    "                                     50)\n",
    "\n",
    "    # Obtain the metrics for the val set\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep_from_scores(\n",
    "            dataset_true = val,\n",
    "            dataset_preds = val_preds,\n",
    "            thresh_arr = thresh_sweep_platt\n",
    "        )\n",
    "\n",
    "    # Evaluate metrics and obtain the best thresh\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    # Test\n",
    "    #---------------\n",
    "\n",
    "    model_thresh = metrics_best_thresh_validate[model_name]['best_threshold']\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name], class_thresh = model_thresh)\n",
    "\n",
    "    ## Plat Scaling:\n",
    "    #---------------\n",
    "    \n",
    "    # 1. Divide test set using sensitive varaible's groups\n",
    "    test_preds_priv, test_preds_unpriv, priv_indices, unpriv_indices = utils.split_dataset_on_sensitive_attribute(\n",
    "        dataset = test_preds,\n",
    "        privileged_group_label = list((privileged_groups[0].values()))[0]\n",
    "    )\n",
    "    # 2. Copy test data\n",
    "    if nvar == 1:\n",
    "        test_preds2 = test_preds.copy(deepcopy = True)\n",
    "    elif nvar == 2:\n",
    "        test_single = data_test.copy(deepcopy = True)\n",
    "        test_preds2 = data_test.copy(deepcopy = True)\n",
    "        test_single.scores = np.zeros_like(test_single.labels)\n",
    "\n",
    "    # 3. Predict for each group\n",
    "    sensitive_groups_data_test = {'priv': [test_preds_priv, priv_indices],\n",
    "                                  'unpriv': [test_preds_unpriv, unpriv_indices]}\n",
    "    \n",
    "\n",
    "    for group, data_group_list in sensitive_groups_data_test.items():    \n",
    "        # We assign the correct name\n",
    "        model_name_group = '{}_{}_{}'.format(model_name, fairness_method, group)\n",
    "\n",
    "        # Predict in each group, store the result in data_val_preds2\n",
    "        # The probabilities are the Platt scores\n",
    "        scores_group = methods[model_name_group].predict_proba(data_group_list[0].scores)\n",
    "        pos_ind_group = np.where(methods[model_name_group].classes_ == data_group_list[0].favorable_label)[0][0]\n",
    "        test_preds2.scores[data_group_list[1]] = scores_group[:, pos_ind_group].reshape(-1,1)\n",
    "\n",
    "\n",
    "    if nvar == 1:    \n",
    "        # Obtain metrics\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores(\n",
    "            dataset_true = test,\n",
    "            dataset_pred = test_preds2,\n",
    "            threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold']\n",
    "        )\n",
    "\n",
    "    elif nvar == 2:\n",
    "        # Obtain metrics\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_from_scores(\n",
    "            dataset_true = test_single,\n",
    "            dataset_pred = test_preds2,\n",
    "            threshold = metrics_best_thresh_validate[model_name+fairness_method]['best_threshold']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Platt, J. (1999). Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3), 61-74."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal odds processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a classifier $\\widehat{Y}$, the equal odds procesor derives a new classifier $\\widetilde{Y}$ by using the available trade-offs that are available in the intersection of all $a$-condition ROC curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}\n",
    "    \\text{\n",
    "     min\n",
    "    }& \\quad \\mathbb{E} [ L(\\widetilde{Y}, Y)] \\\\\n",
    "    \\text{s.t} & \\quad \\gamma_a(\\widetilde{Y}) \\in D_a (\\widehat{Y  }) \\quad \\forall a \\in A\\\\ \n",
    "    & \\quad \\gamma_0(\\widetilde{Y}) = \\gamma_1(\\widetilde{Y})\\\\\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\gamma_a(\\widehat{Y})$ is the vector $( FPR_{A=a}, TPR_{a=A} )$ and $D_a(\\widehat{Y})$ is the convex hull of the $a$-condition ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PosprocEqoddsLABELS(model_name):\n",
    "    \"\"\"\n",
    "    Implement the Equald Odds post processor given prediction labels\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        model_name (str): Name of the model we want to do post processing to.\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assign the correct name\n",
    "    fairness_method = '_eqOdds' \n",
    "\n",
    "    # Copy the dataset\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the predictions of the base model\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Initialize the model and store the predictions\n",
    "    methods[model_name+fairness_method] = EqOddsPostprocessing(\n",
    "        privileged_groups = privileged_groups,\n",
    "        unprivileged_groups = unprivileged_groups, \n",
    "        seed = seed)\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name+fairness_method] = methods[model_name+fairness_method].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model in a range of thresholds\n",
    "    metrics_sweep[model_name+fairness_method] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true=val,\n",
    "        dataset_preds=val_preds,\n",
    "        model=methods[model_name+fairness_method],\n",
    "        thresh_arr=thresh_sweep,\n",
    "        scores_or_labels='labels'\n",
    "    )\n",
    "\n",
    "    # Evaluate the model for the best threshold\n",
    "    metrics_best_thresh_validate[model_name+fairness_method] = utils.describe_metrics(metrics_sweep[model_name+fairness_method])\n",
    "\n",
    "    if nvar == 1:\n",
    "\n",
    "        # We use the best threshold to obtain predicitions for test\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            model=methods[model_name+fairness_method], \n",
    "            threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "            scores_or_labels='labels'\n",
    "        )\n",
    "\n",
    "    elif nvar == 2:\n",
    "\n",
    "        test_single = data_test_single.copy(deepcopy=True)\n",
    "        # We use the best threshold to obtain predicitions for test\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            dataset_true_single = test_single,\n",
    "            model=methods[model_name+fairness_method], \n",
    "            threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "            scores_or_labels='labels'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PosprocEqoddsSCORES(model_name, quality):\n",
    "    \"\"\"\n",
    "    Implement the Equald Odds post processor given a score card.\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        model_name (str): Name of the model we want to do post processing to. \n",
    "        quality (str): \"fpr\" (false positive rate), \"fnr\" (false negative rate) or \"weighted\" (weighted combination of both).\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "     # Assign the correct name\n",
    "    fairness_method = '_eqOdds'\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy the model's predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Assign the correct name\n",
    "    model_name_metric = model_name + fairness_method + '_' + quality\n",
    "    \n",
    "    # Initialize the model \n",
    "    methods[model_name_metric] = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        cost_constraint=quality,\n",
    "        seed=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "\n",
    "    # Evaluate the model for a range of thresholds\n",
    "    metrics_sweep[model_name_metric] = utils.metrics_postprocessing_threshold_sweep(\n",
    "        dataset_true = val,\n",
    "        dataset_preds = val_preds,\n",
    "        model = methods[model_name_metric],\n",
    "        thresh_arr = thresh_sweep,\n",
    "        scores_or_labels = 'scores'\n",
    "    )\n",
    "\n",
    "    # Evaluate in best thresh\n",
    "    metrics_best_thresh_validate[model_name_metric] = utils.describe_metrics(metrics_sweep[model_name_metric])\n",
    "\n",
    "    if nvar == 1:\n",
    "\n",
    "        # Using the best thresh, evaluate in test\n",
    "        metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            model=methods[model_name_metric], \n",
    "            threshold=metrics_best_thresh_validate[model_name_metric]['best_threshold'], \n",
    "            scores_or_labels='scores'\n",
    "        )\n",
    "\n",
    "    elif nvar == 2:\n",
    "        test_single = data_test_single.copy(deepcopy=True)\n",
    "\n",
    "        # We use the best threshold to obtain predicitions for test\n",
    "        metrics_best_thresh_test[model_name+fairness_method] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=test,\n",
    "            dataset_preds=test_preds,\n",
    "            dataset_true_single = test_single,\n",
    "            model=methods[model_name+fairness_method], \n",
    "            threshold=metrics_best_thresh_validate[model_name+fairness_method]['best_threshold'], \n",
    "            scores_or_labels='labels'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reject option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The option rejection post processor defines a critical region whose close to the decision boundary whose observations are relabeled to achieve fairness. This critical region is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\{ x \\in X | max [\\mathbb{P}(\\hat{Y} = 1|x), 1 - \\mathbb{P}(\\hat{Y}| x)] < \\theta \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PosprocReject(model_name, key_metric):\n",
    "    \"\"\"\n",
    "    Implement the Option rejection post processor\n",
    "    ====================================================================================\n",
    "    Inputs:\n",
    "        model_name (str): Name of the model we want to do post processing to. \n",
    "        key_metric (str): 'spd' (Statistical parity difference), 'aod' (Average odds difference) or 'eod' (\"Equal opportunity difference\").\n",
    "        \n",
    "    Outputs:\n",
    "        None    (it  may modify the metrics_sweep, metrics_best_thresh_validate and \n",
    "                 metrics_best_thresh_test dictionaries in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assign the correct name\n",
    "    fairness_method = '_RejOpt'\n",
    "    model_name_metric = model_name + fairness_method + '_' + key_metric\n",
    "\n",
    "    # Copy the datasets\n",
    "    train, val, test = data_train.copy(deepcopy=True), data_val.copy(deepcopy=True), data_test.copy(deepcopy=True)\n",
    "\n",
    "    # Copy predictions\n",
    "    train_preds = utils.update_dataset_from_model(train, methods[model_name])\n",
    "    val_preds = utils.update_dataset_from_model(val, methods[model_name])\n",
    "    test_preds = utils.update_dataset_from_model(test, methods[model_name])\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = RejectOptionClassification(\n",
    "        unprivileged_groups=unprivileged_groups, \n",
    "        privileged_groups=privileged_groups, \n",
    "        metric_name=fair_metrics_optrej[key_metric],\n",
    "        metric_lb=-0.01,\n",
    "        metric_ub=0.01\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    methods[model_name_metric] = methods[model_name_metric].fit(train, train_preds)\n",
    "\n",
    "\n",
    "    if nvar == 1:\n",
    "        # Obtain best threshold in val\n",
    "        metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=val, \n",
    "            dataset_preds=val_preds, \n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)\n",
    "        \n",
    "        # Obtain it in test\n",
    "        metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing(\n",
    "            dataset_true=test, \n",
    "            dataset_preds=test_preds, \n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)\n",
    "        \n",
    "    elif nvar == 2:\n",
    "        val_single, test_single = data_val_single.copy(deepcopy=True), data_test_single.copy(deepcopy=True)\n",
    "        # Obtain best threshold in val\n",
    "        metrics_best_thresh_validate[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=val, \n",
    "            dataset_preds=val_preds,\n",
    "            dataset_true_single=val_single, \n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)\n",
    "        \n",
    "        # Obtain it in test\n",
    "        metrics_best_thresh_test[model_name_metric] = utils.compute_metrics_postprocessing_mult(\n",
    "            dataset_true=test, \n",
    "            dataset_preds=test_preds, \n",
    "            dataset_true_single=val_single,\n",
    "            model=methods[model_name_metric], \n",
    "            required_threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Kamiran, F., Karim, A., & Zhang, X. (2012, December). Decision theory for discrimination-aware classification. In 2012 IEEE 12th international conference on data mining (pp. 924-929). IEEE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented all the different processors. Before we start training, we define certain grids to test different settings of certain processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DI remover\n",
    "repair_level = 0.5                      \n",
    "\n",
    "\n",
    "# MetaFair classifier\n",
    "quality_constraints_meta = ['sr', 'fdr']\n",
    "tau = 0.8   \n",
    "\n",
    "# MetaFair classifier\n",
    "quality_constraints_meta = ['sr', 'fdr']\n",
    "\n",
    "# Equal odds\n",
    "quality_constraints_eqodds = [\"weighted\", 'fnr', 'fpr']\n",
    "\n",
    "# Reject option\n",
    "fair_metrics_optrej = {\n",
    "    'spd': \"Statistical parity difference\",\n",
    "    'aod': \"Average odds difference\",\n",
    "    'eod': \"Equal opportunity difference\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we define some preliminary variables (the data sets we will load, the cases we will consider,...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "# Possible data sets that we can use\n",
    "possible_datasets = ['Simulation', 'German', 'Homecredit']\n",
    "\n",
    "# Change this list if you want to use multiple \n",
    "datasets = ['Simulation']\n",
    "nvars = ['1', '2']\n",
    "\n",
    "# What operations to consider for the LPs\n",
    "operations = ['OR', 'AND', 'XOR']\n",
    "\n",
    "# ind = individual case, com = use of multistage processors\n",
    "cases = ['ind', 'com']\n",
    "\n",
    "# Functions to load the data sets\n",
    "loadDatasets = {\n",
    "    'Simulation1V': simul1V,\n",
    "    'Simulation2V': simul2V,\n",
    "    'German1V': GermanDataset1V,\n",
    "    'German2V': GermanDataset2V,\n",
    "    'Homecredit1V': Homecredit1V,\n",
    "    'Homecredit2V': Homecredit2V\n",
    "}\n",
    "\n",
    "# Dictionary that will store all the results\n",
    "resultsDict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the models and store the test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.531608; batch adversarial loss: 0.703592\n",
      "epoch 1; iter: 0; batch classifier loss: 0.265679; batch adversarial loss: 0.708745\n",
      "epoch 2; iter: 0; batch classifier loss: 0.194356; batch adversarial loss: 0.678589\n",
      "epoch 3; iter: 0; batch classifier loss: 0.148059; batch adversarial loss: 0.691195\n",
      "epoch 4; iter: 0; batch classifier loss: 0.189149; batch adversarial loss: 0.687888\n",
      "epoch 5; iter: 0; batch classifier loss: 0.256212; batch adversarial loss: 0.701131\n",
      "epoch 6; iter: 0; batch classifier loss: 0.190638; batch adversarial loss: 0.709292\n",
      "epoch 7; iter: 0; batch classifier loss: 0.053376; batch adversarial loss: 0.696257\n",
      "epoch 8; iter: 0; batch classifier loss: 0.104133; batch adversarial loss: 0.698248\n",
      "epoch 9; iter: 0; batch classifier loss: 0.177567; batch adversarial loss: 0.718294\n",
      "epoch 10; iter: 0; batch classifier loss: 0.139488; batch adversarial loss: 0.702008\n",
      "epoch 11; iter: 0; batch classifier loss: 0.076876; batch adversarial loss: 0.689923\n",
      "epoch 12; iter: 0; batch classifier loss: 0.161220; batch adversarial loss: 0.702103\n",
      "epoch 13; iter: 0; batch classifier loss: 0.162028; batch adversarial loss: 0.695399\n",
      "epoch 14; iter: 0; batch classifier loss: 0.180710; batch adversarial loss: 0.688308\n",
      "epoch 15; iter: 0; batch classifier loss: 0.172700; batch adversarial loss: 0.711295\n",
      "epoch 16; iter: 0; batch classifier loss: 0.090425; batch adversarial loss: 0.700286\n",
      "epoch 17; iter: 0; batch classifier loss: 0.157057; batch adversarial loss: 0.702170\n",
      "epoch 18; iter: 0; batch classifier loss: 0.161649; batch adversarial loss: 0.699225\n",
      "epoch 19; iter: 0; batch classifier loss: 0.262883; batch adversarial loss: 0.713907\n",
      "epoch 20; iter: 0; batch classifier loss: 0.108674; batch adversarial loss: 0.692220\n",
      "epoch 21; iter: 0; batch classifier loss: 0.093463; batch adversarial loss: 0.706562\n",
      "epoch 22; iter: 0; batch classifier loss: 0.112065; batch adversarial loss: 0.682432\n",
      "epoch 23; iter: 0; batch classifier loss: 0.033883; batch adversarial loss: 0.693349\n",
      "epoch 24; iter: 0; batch classifier loss: 0.064384; batch adversarial loss: 0.691746\n",
      "epoch 25; iter: 0; batch classifier loss: 0.086481; batch adversarial loss: 0.703230\n",
      "epoch 26; iter: 0; batch classifier loss: 0.202646; batch adversarial loss: 0.707115\n",
      "epoch 27; iter: 0; batch classifier loss: 0.082519; batch adversarial loss: 0.686347\n",
      "epoch 28; iter: 0; batch classifier loss: 0.123001; batch adversarial loss: 0.696206\n",
      "epoch 29; iter: 0; batch classifier loss: 0.188208; batch adversarial loss: 0.692479\n",
      "epoch 30; iter: 0; batch classifier loss: 0.087762; batch adversarial loss: 0.695047\n",
      "epoch 31; iter: 0; batch classifier loss: 0.091096; batch adversarial loss: 0.692268\n",
      "epoch 32; iter: 0; batch classifier loss: 0.064862; batch adversarial loss: 0.694264\n",
      "epoch 33; iter: 0; batch classifier loss: 0.150035; batch adversarial loss: 0.690061\n",
      "epoch 34; iter: 0; batch classifier loss: 0.070860; batch adversarial loss: 0.701851\n",
      "epoch 35; iter: 0; batch classifier loss: 0.066001; batch adversarial loss: 0.696309\n",
      "epoch 36; iter: 0; batch classifier loss: 0.073714; batch adversarial loss: 0.692993\n",
      "epoch 37; iter: 0; batch classifier loss: 0.144870; batch adversarial loss: 0.696416\n",
      "epoch 38; iter: 0; batch classifier loss: 0.123215; batch adversarial loss: 0.693065\n",
      "epoch 39; iter: 0; batch classifier loss: 0.100619; batch adversarial loss: 0.682802\n",
      "epoch 40; iter: 0; batch classifier loss: 0.125895; batch adversarial loss: 0.687608\n",
      "epoch 41; iter: 0; batch classifier loss: 0.138822; batch adversarial loss: 0.684528\n",
      "epoch 42; iter: 0; batch classifier loss: 0.045543; batch adversarial loss: 0.692862\n",
      "epoch 43; iter: 0; batch classifier loss: 0.163706; batch adversarial loss: 0.707216\n",
      "epoch 44; iter: 0; batch classifier loss: 0.086285; batch adversarial loss: 0.686772\n",
      "epoch 45; iter: 0; batch classifier loss: 0.262203; batch adversarial loss: 0.687852\n",
      "epoch 46; iter: 0; batch classifier loss: 0.088478; batch adversarial loss: 0.686540\n",
      "epoch 47; iter: 0; batch classifier loss: 0.168283; batch adversarial loss: 0.686092\n",
      "epoch 48; iter: 0; batch classifier loss: 0.197593; batch adversarial loss: 0.698106\n",
      "epoch 49; iter: 0; batch classifier loss: 0.100762; batch adversarial loss: 0.698949\n",
      "epoch 50; iter: 0; batch classifier loss: 0.128122; batch adversarial loss: 0.699657\n",
      "epoch 51; iter: 0; batch classifier loss: 0.133087; batch adversarial loss: 0.692290\n",
      "epoch 52; iter: 0; batch classifier loss: 0.145022; batch adversarial loss: 0.688986\n",
      "epoch 53; iter: 0; batch classifier loss: 0.142078; batch adversarial loss: 0.688523\n",
      "epoch 54; iter: 0; batch classifier loss: 0.101724; batch adversarial loss: 0.690384\n",
      "epoch 55; iter: 0; batch classifier loss: 0.068367; batch adversarial loss: 0.690122\n",
      "epoch 56; iter: 0; batch classifier loss: 0.119529; batch adversarial loss: 0.696303\n",
      "epoch 57; iter: 0; batch classifier loss: 0.116980; batch adversarial loss: 0.692505\n",
      "epoch 58; iter: 0; batch classifier loss: 0.101253; batch adversarial loss: 0.698026\n",
      "epoch 59; iter: 0; batch classifier loss: 0.128795; batch adversarial loss: 0.693414\n",
      "epoch 60; iter: 0; batch classifier loss: 0.111339; batch adversarial loss: 0.691334\n",
      "epoch 61; iter: 0; batch classifier loss: 0.158345; batch adversarial loss: 0.683841\n",
      "epoch 62; iter: 0; batch classifier loss: 0.123135; batch adversarial loss: 0.692974\n",
      "epoch 63; iter: 0; batch classifier loss: 0.095259; batch adversarial loss: 0.691582\n",
      "epoch 64; iter: 0; batch classifier loss: 0.124454; batch adversarial loss: 0.693276\n",
      "epoch 65; iter: 0; batch classifier loss: 0.093717; batch adversarial loss: 0.685892\n",
      "epoch 66; iter: 0; batch classifier loss: 0.187632; batch adversarial loss: 0.697637\n",
      "epoch 67; iter: 0; batch classifier loss: 0.061872; batch adversarial loss: 0.691374\n",
      "epoch 68; iter: 0; batch classifier loss: 0.161179; batch adversarial loss: 0.684368\n",
      "epoch 69; iter: 0; batch classifier loss: 0.104601; batch adversarial loss: 0.681130\n",
      "epoch 70; iter: 0; batch classifier loss: 0.140407; batch adversarial loss: 0.696375\n",
      "epoch 71; iter: 0; batch classifier loss: 0.175674; batch adversarial loss: 0.694276\n",
      "epoch 72; iter: 0; batch classifier loss: 0.125335; batch adversarial loss: 0.687915\n",
      "epoch 73; iter: 0; batch classifier loss: 0.178291; batch adversarial loss: 0.695140\n",
      "epoch 74; iter: 0; batch classifier loss: 0.097919; batch adversarial loss: 0.695919\n",
      "epoch 75; iter: 0; batch classifier loss: 0.135916; batch adversarial loss: 0.683674\n",
      "epoch 76; iter: 0; batch classifier loss: 0.079556; batch adversarial loss: 0.685839\n",
      "epoch 77; iter: 0; batch classifier loss: 0.094227; batch adversarial loss: 0.692621\n",
      "epoch 78; iter: 0; batch classifier loss: 0.080160; batch adversarial loss: 0.691337\n",
      "epoch 79; iter: 0; batch classifier loss: 0.061466; batch adversarial loss: 0.689277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.638924; batch adversarial loss: 0.916763\n",
      "epoch 1; iter: 0; batch classifier loss: 0.297358; batch adversarial loss: 1.066227\n",
      "epoch 2; iter: 0; batch classifier loss: 0.235809; batch adversarial loss: 1.180567\n",
      "epoch 3; iter: 0; batch classifier loss: 0.239483; batch adversarial loss: 1.236011\n",
      "epoch 4; iter: 0; batch classifier loss: 0.125097; batch adversarial loss: 1.109210\n",
      "epoch 5; iter: 0; batch classifier loss: 0.166812; batch adversarial loss: 1.176278\n",
      "epoch 6; iter: 0; batch classifier loss: 0.150616; batch adversarial loss: 1.053055\n",
      "epoch 7; iter: 0; batch classifier loss: 0.132980; batch adversarial loss: 1.043785\n",
      "epoch 8; iter: 0; batch classifier loss: 0.104908; batch adversarial loss: 1.015743\n",
      "epoch 9; iter: 0; batch classifier loss: 0.073810; batch adversarial loss: 0.906447\n",
      "epoch 10; iter: 0; batch classifier loss: 0.176091; batch adversarial loss: 0.941611\n",
      "epoch 11; iter: 0; batch classifier loss: 0.151025; batch adversarial loss: 0.911956\n",
      "epoch 12; iter: 0; batch classifier loss: 0.097009; batch adversarial loss: 0.944720\n",
      "epoch 13; iter: 0; batch classifier loss: 0.122592; batch adversarial loss: 0.881619\n",
      "epoch 14; iter: 0; batch classifier loss: 0.117640; batch adversarial loss: 0.873475\n",
      "epoch 15; iter: 0; batch classifier loss: 0.085250; batch adversarial loss: 0.942781\n",
      "epoch 16; iter: 0; batch classifier loss: 0.131129; batch adversarial loss: 0.778752\n",
      "epoch 17; iter: 0; batch classifier loss: 0.150999; batch adversarial loss: 0.846456\n",
      "epoch 18; iter: 0; batch classifier loss: 0.114562; batch adversarial loss: 0.764925\n",
      "epoch 19; iter: 0; batch classifier loss: 0.111780; batch adversarial loss: 0.838026\n",
      "epoch 20; iter: 0; batch classifier loss: 0.175858; batch adversarial loss: 0.758031\n",
      "epoch 21; iter: 0; batch classifier loss: 0.109677; batch adversarial loss: 0.748882\n",
      "epoch 22; iter: 0; batch classifier loss: 0.144844; batch adversarial loss: 0.798376\n",
      "epoch 23; iter: 0; batch classifier loss: 0.155759; batch adversarial loss: 0.765595\n",
      "epoch 24; iter: 0; batch classifier loss: 0.119921; batch adversarial loss: 0.786510\n",
      "epoch 25; iter: 0; batch classifier loss: 0.298125; batch adversarial loss: 0.721735\n",
      "epoch 26; iter: 0; batch classifier loss: 0.054738; batch adversarial loss: 0.734015\n",
      "epoch 27; iter: 0; batch classifier loss: 0.123820; batch adversarial loss: 0.689029\n",
      "epoch 28; iter: 0; batch classifier loss: 0.105658; batch adversarial loss: 0.712368\n",
      "epoch 29; iter: 0; batch classifier loss: 0.060067; batch adversarial loss: 0.685613\n",
      "epoch 30; iter: 0; batch classifier loss: 0.056251; batch adversarial loss: 0.709044\n",
      "epoch 31; iter: 0; batch classifier loss: 0.191157; batch adversarial loss: 0.723371\n",
      "epoch 32; iter: 0; batch classifier loss: 0.194457; batch adversarial loss: 0.729214\n",
      "epoch 33; iter: 0; batch classifier loss: 0.222608; batch adversarial loss: 0.706537\n",
      "epoch 34; iter: 0; batch classifier loss: 0.051150; batch adversarial loss: 0.699988\n",
      "epoch 35; iter: 0; batch classifier loss: 0.077341; batch adversarial loss: 0.706495\n",
      "epoch 36; iter: 0; batch classifier loss: 0.142139; batch adversarial loss: 0.697272\n",
      "epoch 37; iter: 0; batch classifier loss: 0.106872; batch adversarial loss: 0.705923\n",
      "epoch 38; iter: 0; batch classifier loss: 0.241134; batch adversarial loss: 0.701580\n",
      "epoch 39; iter: 0; batch classifier loss: 0.158122; batch adversarial loss: 0.693941\n",
      "epoch 40; iter: 0; batch classifier loss: 0.177592; batch adversarial loss: 0.695072\n",
      "epoch 41; iter: 0; batch classifier loss: 0.083381; batch adversarial loss: 0.704176\n",
      "epoch 42; iter: 0; batch classifier loss: 0.076542; batch adversarial loss: 0.697877\n",
      "epoch 43; iter: 0; batch classifier loss: 0.170238; batch adversarial loss: 0.690926\n",
      "epoch 44; iter: 0; batch classifier loss: 0.146096; batch adversarial loss: 0.693869\n",
      "epoch 45; iter: 0; batch classifier loss: 0.182291; batch adversarial loss: 0.690461\n",
      "epoch 46; iter: 0; batch classifier loss: 0.137182; batch adversarial loss: 0.693172\n",
      "epoch 47; iter: 0; batch classifier loss: 0.110268; batch adversarial loss: 0.691351\n",
      "epoch 48; iter: 0; batch classifier loss: 0.115332; batch adversarial loss: 0.692931\n",
      "epoch 49; iter: 0; batch classifier loss: 0.083578; batch adversarial loss: 0.690665\n",
      "epoch 50; iter: 0; batch classifier loss: 0.099921; batch adversarial loss: 0.692379\n",
      "epoch 51; iter: 0; batch classifier loss: 0.133980; batch adversarial loss: 0.691426\n",
      "epoch 52; iter: 0; batch classifier loss: 0.154258; batch adversarial loss: 0.691803\n",
      "epoch 53; iter: 0; batch classifier loss: 0.138695; batch adversarial loss: 0.693899\n",
      "epoch 54; iter: 0; batch classifier loss: 0.083693; batch adversarial loss: 0.693370\n",
      "epoch 55; iter: 0; batch classifier loss: 0.055068; batch adversarial loss: 0.693264\n",
      "epoch 56; iter: 0; batch classifier loss: 0.109365; batch adversarial loss: 0.693478\n",
      "epoch 57; iter: 0; batch classifier loss: 0.107501; batch adversarial loss: 0.693151\n",
      "epoch 58; iter: 0; batch classifier loss: 0.086777; batch adversarial loss: 0.694175\n",
      "epoch 59; iter: 0; batch classifier loss: 0.106120; batch adversarial loss: 0.693119\n",
      "epoch 60; iter: 0; batch classifier loss: 0.107510; batch adversarial loss: 0.693420\n",
      "epoch 61; iter: 0; batch classifier loss: 0.171916; batch adversarial loss: 0.692205\n",
      "epoch 62; iter: 0; batch classifier loss: 0.111881; batch adversarial loss: 0.693307\n",
      "epoch 63; iter: 0; batch classifier loss: 0.083425; batch adversarial loss: 0.693455\n",
      "epoch 64; iter: 0; batch classifier loss: 0.112791; batch adversarial loss: 0.692993\n",
      "epoch 65; iter: 0; batch classifier loss: 0.102245; batch adversarial loss: 0.693109\n",
      "epoch 66; iter: 0; batch classifier loss: 0.162037; batch adversarial loss: 0.693221\n",
      "epoch 67; iter: 0; batch classifier loss: 0.055449; batch adversarial loss: 0.694084\n",
      "epoch 68; iter: 0; batch classifier loss: 0.169125; batch adversarial loss: 0.692424\n",
      "epoch 69; iter: 0; batch classifier loss: 0.111078; batch adversarial loss: 0.691459\n",
      "epoch 70; iter: 0; batch classifier loss: 0.119174; batch adversarial loss: 0.692982\n",
      "epoch 71; iter: 0; batch classifier loss: 0.170528; batch adversarial loss: 0.695170\n",
      "epoch 72; iter: 0; batch classifier loss: 0.126405; batch adversarial loss: 0.693195\n",
      "epoch 73; iter: 0; batch classifier loss: 0.165785; batch adversarial loss: 0.692771\n",
      "epoch 74; iter: 0; batch classifier loss: 0.087008; batch adversarial loss: 0.695681\n",
      "epoch 75; iter: 0; batch classifier loss: 0.148250; batch adversarial loss: 0.693756\n",
      "epoch 76; iter: 0; batch classifier loss: 0.075560; batch adversarial loss: 0.692733\n",
      "epoch 77; iter: 0; batch classifier loss: 0.082728; batch adversarial loss: 0.694129\n",
      "epoch 78; iter: 0; batch classifier loss: 0.085168; batch adversarial loss: 0.695109\n",
      "epoch 79; iter: 0; batch classifier loss: 0.050401; batch adversarial loss: 0.693549\n",
      "epoch 0; iter: 0; batch classifier loss: 0.686029; batch adversarial loss: 0.721199\n",
      "epoch 1; iter: 0; batch classifier loss: 0.361788; batch adversarial loss: 0.755231\n",
      "epoch 2; iter: 0; batch classifier loss: 0.271414; batch adversarial loss: 0.742568\n",
      "epoch 3; iter: 0; batch classifier loss: 0.258831; batch adversarial loss: 0.814165\n",
      "epoch 4; iter: 0; batch classifier loss: 0.176826; batch adversarial loss: 0.723987\n",
      "epoch 5; iter: 0; batch classifier loss: 0.137800; batch adversarial loss: 0.755088\n",
      "epoch 6; iter: 0; batch classifier loss: 0.240792; batch adversarial loss: 0.803091\n",
      "epoch 7; iter: 0; batch classifier loss: 0.143182; batch adversarial loss: 0.740688\n",
      "epoch 8; iter: 0; batch classifier loss: 0.191702; batch adversarial loss: 0.717398\n",
      "epoch 9; iter: 0; batch classifier loss: 0.090738; batch adversarial loss: 0.689594\n",
      "epoch 10; iter: 0; batch classifier loss: 0.143180; batch adversarial loss: 0.704445\n",
      "epoch 11; iter: 0; batch classifier loss: 0.176807; batch adversarial loss: 0.685489\n",
      "epoch 12; iter: 0; batch classifier loss: 0.141074; batch adversarial loss: 0.693615\n",
      "epoch 13; iter: 0; batch classifier loss: 0.195746; batch adversarial loss: 0.713771\n",
      "epoch 14; iter: 0; batch classifier loss: 0.142255; batch adversarial loss: 0.707437\n",
      "epoch 15; iter: 0; batch classifier loss: 0.216274; batch adversarial loss: 0.683008\n",
      "epoch 16; iter: 0; batch classifier loss: 0.194511; batch adversarial loss: 0.697941\n",
      "epoch 17; iter: 0; batch classifier loss: 0.071027; batch adversarial loss: 0.694101\n",
      "epoch 18; iter: 0; batch classifier loss: 0.176350; batch adversarial loss: 0.705631\n",
      "epoch 19; iter: 0; batch classifier loss: 0.162631; batch adversarial loss: 0.693188\n",
      "epoch 20; iter: 0; batch classifier loss: 0.129066; batch adversarial loss: 0.694628\n",
      "epoch 21; iter: 0; batch classifier loss: 0.206197; batch adversarial loss: 0.691550\n",
      "epoch 22; iter: 0; batch classifier loss: 0.085889; batch adversarial loss: 0.691726\n",
      "epoch 23; iter: 0; batch classifier loss: 0.084094; batch adversarial loss: 0.691773\n",
      "epoch 24; iter: 0; batch classifier loss: 0.105336; batch adversarial loss: 0.692021\n",
      "epoch 25; iter: 0; batch classifier loss: 0.127628; batch adversarial loss: 0.692622\n",
      "epoch 26; iter: 0; batch classifier loss: 0.232248; batch adversarial loss: 0.692500\n",
      "epoch 27; iter: 0; batch classifier loss: 0.039778; batch adversarial loss: 0.692467\n",
      "epoch 28; iter: 0; batch classifier loss: 0.111284; batch adversarial loss: 0.692215\n",
      "epoch 29; iter: 0; batch classifier loss: 0.063111; batch adversarial loss: 0.692750\n",
      "epoch 30; iter: 0; batch classifier loss: 0.091307; batch adversarial loss: 0.691947\n",
      "epoch 31; iter: 0; batch classifier loss: 0.032526; batch adversarial loss: 0.692861\n",
      "epoch 32; iter: 0; batch classifier loss: 0.111732; batch adversarial loss: 0.691984\n",
      "epoch 33; iter: 0; batch classifier loss: 0.083058; batch adversarial loss: 0.693263\n",
      "epoch 34; iter: 0; batch classifier loss: 0.060485; batch adversarial loss: 0.691893\n",
      "epoch 35; iter: 0; batch classifier loss: 0.074639; batch adversarial loss: 0.696630\n",
      "epoch 36; iter: 0; batch classifier loss: 0.051273; batch adversarial loss: 0.692541\n",
      "epoch 37; iter: 0; batch classifier loss: 0.089705; batch adversarial loss: 0.693123\n",
      "epoch 38; iter: 0; batch classifier loss: 0.107169; batch adversarial loss: 0.693101\n",
      "epoch 39; iter: 0; batch classifier loss: 0.083733; batch adversarial loss: 0.690322\n",
      "epoch 40; iter: 0; batch classifier loss: 0.103554; batch adversarial loss: 0.693259\n",
      "epoch 41; iter: 0; batch classifier loss: 0.224881; batch adversarial loss: 0.693239\n",
      "epoch 42; iter: 0; batch classifier loss: 0.151201; batch adversarial loss: 0.692872\n",
      "epoch 43; iter: 0; batch classifier loss: 0.129811; batch adversarial loss: 0.690640\n",
      "epoch 44; iter: 0; batch classifier loss: 0.132101; batch adversarial loss: 0.695008\n",
      "epoch 45; iter: 0; batch classifier loss: 0.147173; batch adversarial loss: 0.692839\n",
      "epoch 46; iter: 0; batch classifier loss: 0.086468; batch adversarial loss: 0.693104\n",
      "epoch 47; iter: 0; batch classifier loss: 0.078800; batch adversarial loss: 0.692514\n",
      "epoch 48; iter: 0; batch classifier loss: 0.051914; batch adversarial loss: 0.693433\n",
      "epoch 49; iter: 0; batch classifier loss: 0.159517; batch adversarial loss: 0.694480\n",
      "epoch 50; iter: 0; batch classifier loss: 0.104290; batch adversarial loss: 0.693080\n",
      "epoch 51; iter: 0; batch classifier loss: 0.203392; batch adversarial loss: 0.692488\n",
      "epoch 52; iter: 0; batch classifier loss: 0.156357; batch adversarial loss: 0.692828\n",
      "epoch 53; iter: 0; batch classifier loss: 0.172433; batch adversarial loss: 0.690905\n",
      "epoch 54; iter: 0; batch classifier loss: 0.080745; batch adversarial loss: 0.694246\n",
      "epoch 55; iter: 0; batch classifier loss: 0.197970; batch adversarial loss: 0.692818\n",
      "epoch 56; iter: 0; batch classifier loss: 0.109033; batch adversarial loss: 0.692947\n",
      "epoch 57; iter: 0; batch classifier loss: 0.081904; batch adversarial loss: 0.692765\n",
      "epoch 58; iter: 0; batch classifier loss: 0.087554; batch adversarial loss: 0.689787\n",
      "epoch 59; iter: 0; batch classifier loss: 0.202920; batch adversarial loss: 0.691591\n",
      "epoch 60; iter: 0; batch classifier loss: 0.050806; batch adversarial loss: 0.693669\n",
      "epoch 61; iter: 0; batch classifier loss: 0.054849; batch adversarial loss: 0.692518\n",
      "epoch 62; iter: 0; batch classifier loss: 0.114341; batch adversarial loss: 0.691994\n",
      "epoch 63; iter: 0; batch classifier loss: 0.134544; batch adversarial loss: 0.695235\n",
      "epoch 64; iter: 0; batch classifier loss: 0.081193; batch adversarial loss: 0.691469\n",
      "epoch 65; iter: 0; batch classifier loss: 0.083206; batch adversarial loss: 0.692300\n",
      "epoch 66; iter: 0; batch classifier loss: 0.143469; batch adversarial loss: 0.694083\n",
      "epoch 67; iter: 0; batch classifier loss: 0.080563; batch adversarial loss: 0.694065\n",
      "epoch 68; iter: 0; batch classifier loss: 0.104460; batch adversarial loss: 0.691305\n",
      "epoch 69; iter: 0; batch classifier loss: 0.226083; batch adversarial loss: 0.693294\n",
      "epoch 70; iter: 0; batch classifier loss: 0.091379; batch adversarial loss: 0.694791\n",
      "epoch 71; iter: 0; batch classifier loss: 0.166309; batch adversarial loss: 0.692277\n",
      "epoch 72; iter: 0; batch classifier loss: 0.071859; batch adversarial loss: 0.692988\n",
      "epoch 73; iter: 0; batch classifier loss: 0.082529; batch adversarial loss: 0.695737\n",
      "epoch 74; iter: 0; batch classifier loss: 0.082217; batch adversarial loss: 0.692988\n",
      "epoch 75; iter: 0; batch classifier loss: 0.135063; batch adversarial loss: 0.692702\n",
      "epoch 76; iter: 0; batch classifier loss: 0.165825; batch adversarial loss: 0.692527\n",
      "epoch 77; iter: 0; batch classifier loss: 0.183011; batch adversarial loss: 0.695201\n",
      "epoch 78; iter: 0; batch classifier loss: 0.076584; batch adversarial loss: 0.693349\n",
      "epoch 79; iter: 0; batch classifier loss: 0.166431; batch adversarial loss: 0.693147\n",
      "epoch 0; iter: 0; batch classifier loss: 0.760855; batch adversarial loss: 0.763687\n",
      "epoch 1; iter: 0; batch classifier loss: 0.398160; batch adversarial loss: 0.788087\n",
      "epoch 2; iter: 0; batch classifier loss: 0.294916; batch adversarial loss: 0.844564\n",
      "epoch 3; iter: 0; batch classifier loss: 0.233208; batch adversarial loss: 0.708388\n",
      "epoch 4; iter: 0; batch classifier loss: 0.119016; batch adversarial loss: 0.816111\n",
      "epoch 5; iter: 0; batch classifier loss: 0.101727; batch adversarial loss: 0.751622\n",
      "epoch 6; iter: 0; batch classifier loss: 0.131701; batch adversarial loss: 0.775380\n",
      "epoch 7; iter: 0; batch classifier loss: 0.202175; batch adversarial loss: 0.697540\n",
      "epoch 8; iter: 0; batch classifier loss: 0.166860; batch adversarial loss: 0.809390\n",
      "epoch 9; iter: 0; batch classifier loss: 0.215659; batch adversarial loss: 0.783514\n",
      "epoch 10; iter: 0; batch classifier loss: 0.125239; batch adversarial loss: 0.769747\n",
      "epoch 11; iter: 0; batch classifier loss: 0.110955; batch adversarial loss: 0.764572\n",
      "epoch 12; iter: 0; batch classifier loss: 0.178336; batch adversarial loss: 0.727890\n",
      "epoch 13; iter: 0; batch classifier loss: 0.153914; batch adversarial loss: 0.679591\n",
      "epoch 14; iter: 0; batch classifier loss: 0.109211; batch adversarial loss: 0.729465\n",
      "epoch 15; iter: 0; batch classifier loss: 0.122328; batch adversarial loss: 0.723348\n",
      "epoch 16; iter: 0; batch classifier loss: 0.163654; batch adversarial loss: 0.691526\n",
      "epoch 17; iter: 0; batch classifier loss: 0.118335; batch adversarial loss: 0.707090\n",
      "epoch 18; iter: 0; batch classifier loss: 0.139594; batch adversarial loss: 0.688816\n",
      "epoch 19; iter: 0; batch classifier loss: 0.061292; batch adversarial loss: 0.706651\n",
      "epoch 20; iter: 0; batch classifier loss: 0.097786; batch adversarial loss: 0.672603\n",
      "epoch 21; iter: 0; batch classifier loss: 0.114228; batch adversarial loss: 0.702052\n",
      "epoch 22; iter: 0; batch classifier loss: 0.274919; batch adversarial loss: 0.698251\n",
      "epoch 23; iter: 0; batch classifier loss: 0.061323; batch adversarial loss: 0.696408\n",
      "epoch 24; iter: 0; batch classifier loss: 0.141220; batch adversarial loss: 0.692140\n",
      "epoch 25; iter: 0; batch classifier loss: 0.116453; batch adversarial loss: 0.693840\n",
      "epoch 26; iter: 0; batch classifier loss: 0.113780; batch adversarial loss: 0.696128\n",
      "epoch 27; iter: 0; batch classifier loss: 0.049907; batch adversarial loss: 0.695364\n",
      "epoch 28; iter: 0; batch classifier loss: 0.110989; batch adversarial loss: 0.707554\n",
      "epoch 29; iter: 0; batch classifier loss: 0.085383; batch adversarial loss: 0.684112\n",
      "epoch 30; iter: 0; batch classifier loss: 0.088567; batch adversarial loss: 0.688313\n",
      "epoch 31; iter: 0; batch classifier loss: 0.085085; batch adversarial loss: 0.690086\n",
      "epoch 32; iter: 0; batch classifier loss: 0.124068; batch adversarial loss: 0.695051\n",
      "epoch 33; iter: 0; batch classifier loss: 0.058323; batch adversarial loss: 0.695012\n",
      "epoch 34; iter: 0; batch classifier loss: 0.038438; batch adversarial loss: 0.693350\n",
      "epoch 35; iter: 0; batch classifier loss: 0.084421; batch adversarial loss: 0.693805\n",
      "epoch 36; iter: 0; batch classifier loss: 0.088477; batch adversarial loss: 0.698220\n",
      "epoch 37; iter: 0; batch classifier loss: 0.116485; batch adversarial loss: 0.694754\n",
      "epoch 38; iter: 0; batch classifier loss: 0.086126; batch adversarial loss: 0.697208\n",
      "epoch 39; iter: 0; batch classifier loss: 0.129037; batch adversarial loss: 0.691497\n",
      "epoch 40; iter: 0; batch classifier loss: 0.085194; batch adversarial loss: 0.693923\n",
      "epoch 41; iter: 0; batch classifier loss: 0.134054; batch adversarial loss: 0.688400\n",
      "epoch 42; iter: 0; batch classifier loss: 0.137811; batch adversarial loss: 0.694832\n",
      "epoch 43; iter: 0; batch classifier loss: 0.124065; batch adversarial loss: 0.686918\n",
      "epoch 44; iter: 0; batch classifier loss: 0.178184; batch adversarial loss: 0.684984\n",
      "epoch 45; iter: 0; batch classifier loss: 0.114227; batch adversarial loss: 0.699312\n",
      "epoch 46; iter: 0; batch classifier loss: 0.056643; batch adversarial loss: 0.691917\n",
      "epoch 47; iter: 0; batch classifier loss: 0.194222; batch adversarial loss: 0.693080\n",
      "epoch 48; iter: 0; batch classifier loss: 0.080519; batch adversarial loss: 0.690301\n",
      "epoch 49; iter: 0; batch classifier loss: 0.142773; batch adversarial loss: 0.692893\n",
      "epoch 50; iter: 0; batch classifier loss: 0.183652; batch adversarial loss: 0.693491\n",
      "epoch 51; iter: 0; batch classifier loss: 0.149086; batch adversarial loss: 0.694566\n",
      "epoch 52; iter: 0; batch classifier loss: 0.085695; batch adversarial loss: 0.696444\n",
      "epoch 53; iter: 0; batch classifier loss: 0.192046; batch adversarial loss: 0.696327\n",
      "epoch 54; iter: 0; batch classifier loss: 0.034952; batch adversarial loss: 0.693395\n",
      "epoch 55; iter: 0; batch classifier loss: 0.088071; batch adversarial loss: 0.695555\n",
      "epoch 56; iter: 0; batch classifier loss: 0.237071; batch adversarial loss: 0.699209\n",
      "epoch 57; iter: 0; batch classifier loss: 0.125201; batch adversarial loss: 0.692997\n",
      "epoch 58; iter: 0; batch classifier loss: 0.060564; batch adversarial loss: 0.692972\n",
      "epoch 59; iter: 0; batch classifier loss: 0.082053; batch adversarial loss: 0.691793\n",
      "epoch 60; iter: 0; batch classifier loss: 0.235835; batch adversarial loss: 0.689316\n",
      "epoch 61; iter: 0; batch classifier loss: 0.115801; batch adversarial loss: 0.686032\n",
      "epoch 62; iter: 0; batch classifier loss: 0.184673; batch adversarial loss: 0.694116\n",
      "epoch 63; iter: 0; batch classifier loss: 0.136152; batch adversarial loss: 0.691805\n",
      "epoch 64; iter: 0; batch classifier loss: 0.159230; batch adversarial loss: 0.698009\n",
      "epoch 65; iter: 0; batch classifier loss: 0.099772; batch adversarial loss: 0.686829\n",
      "epoch 66; iter: 0; batch classifier loss: 0.247532; batch adversarial loss: 0.689558\n",
      "epoch 67; iter: 0; batch classifier loss: 0.114511; batch adversarial loss: 0.691731\n",
      "epoch 68; iter: 0; batch classifier loss: 0.114336; batch adversarial loss: 0.691313\n",
      "epoch 69; iter: 0; batch classifier loss: 0.115296; batch adversarial loss: 0.694996\n",
      "epoch 70; iter: 0; batch classifier loss: 0.061525; batch adversarial loss: 0.692839\n",
      "epoch 71; iter: 0; batch classifier loss: 0.118710; batch adversarial loss: 0.695941\n",
      "epoch 72; iter: 0; batch classifier loss: 0.140123; batch adversarial loss: 0.691399\n",
      "epoch 73; iter: 0; batch classifier loss: 0.143576; batch adversarial loss: 0.693579\n",
      "epoch 74; iter: 0; batch classifier loss: 0.171829; batch adversarial loss: 0.690501\n",
      "epoch 75; iter: 0; batch classifier loss: 0.063585; batch adversarial loss: 0.693040\n",
      "epoch 76; iter: 0; batch classifier loss: 0.034381; batch adversarial loss: 0.691815\n",
      "epoch 77; iter: 0; batch classifier loss: 0.165783; batch adversarial loss: 0.690238\n",
      "epoch 78; iter: 0; batch classifier loss: 0.203325; batch adversarial loss: 0.684490\n",
      "epoch 79; iter: 0; batch classifier loss: 0.150956; batch adversarial loss: 0.686006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.558319; batch adversarial loss: 0.730538\n",
      "epoch 1; iter: 0; batch classifier loss: 0.502788; batch adversarial loss: 0.747523\n",
      "epoch 2; iter: 0; batch classifier loss: 0.405915; batch adversarial loss: 0.704983\n",
      "epoch 3; iter: 0; batch classifier loss: 0.352665; batch adversarial loss: 0.693389\n",
      "epoch 4; iter: 0; batch classifier loss: 0.312186; batch adversarial loss: 0.684298\n",
      "epoch 5; iter: 0; batch classifier loss: 0.303904; batch adversarial loss: 0.675297\n",
      "epoch 6; iter: 0; batch classifier loss: 0.191669; batch adversarial loss: 0.652846\n",
      "epoch 7; iter: 0; batch classifier loss: 0.031290; batch adversarial loss: 0.642044\n",
      "epoch 8; iter: 0; batch classifier loss: 0.089396; batch adversarial loss: 0.625044\n",
      "epoch 9; iter: 0; batch classifier loss: 0.173872; batch adversarial loss: 0.647012\n",
      "epoch 10; iter: 0; batch classifier loss: 0.133183; batch adversarial loss: 0.608414\n",
      "epoch 11; iter: 0; batch classifier loss: 0.068811; batch adversarial loss: 0.578438\n",
      "epoch 12; iter: 0; batch classifier loss: 0.150392; batch adversarial loss: 0.569212\n",
      "epoch 13; iter: 0; batch classifier loss: 0.159446; batch adversarial loss: 0.611606\n",
      "epoch 14; iter: 0; batch classifier loss: 0.180077; batch adversarial loss: 0.625763\n",
      "epoch 15; iter: 0; batch classifier loss: 0.163816; batch adversarial loss: 0.510041\n",
      "epoch 16; iter: 0; batch classifier loss: 0.078595; batch adversarial loss: 0.555426\n",
      "epoch 17; iter: 0; batch classifier loss: 0.167831; batch adversarial loss: 0.635352\n",
      "epoch 18; iter: 0; batch classifier loss: 0.159687; batch adversarial loss: 0.579986\n",
      "epoch 19; iter: 0; batch classifier loss: 0.273859; batch adversarial loss: 0.630158\n",
      "epoch 20; iter: 0; batch classifier loss: 0.118269; batch adversarial loss: 0.596115\n",
      "epoch 21; iter: 0; batch classifier loss: 0.081402; batch adversarial loss: 0.566310\n",
      "epoch 22; iter: 0; batch classifier loss: 0.113319; batch adversarial loss: 0.582139\n",
      "epoch 23; iter: 0; batch classifier loss: 0.032109; batch adversarial loss: 0.594296\n",
      "epoch 24; iter: 0; batch classifier loss: 0.061880; batch adversarial loss: 0.534054\n",
      "epoch 25; iter: 0; batch classifier loss: 0.090276; batch adversarial loss: 0.623353\n",
      "epoch 26; iter: 0; batch classifier loss: 0.189204; batch adversarial loss: 0.586893\n",
      "epoch 27; iter: 0; batch classifier loss: 0.082851; batch adversarial loss: 0.576018\n",
      "epoch 28; iter: 0; batch classifier loss: 0.126811; batch adversarial loss: 0.550234\n",
      "epoch 29; iter: 0; batch classifier loss: 0.193719; batch adversarial loss: 0.609604\n",
      "epoch 30; iter: 0; batch classifier loss: 0.088209; batch adversarial loss: 0.552430\n",
      "epoch 31; iter: 0; batch classifier loss: 0.092839; batch adversarial loss: 0.600124\n",
      "epoch 32; iter: 0; batch classifier loss: 0.069941; batch adversarial loss: 0.505190\n",
      "epoch 33; iter: 0; batch classifier loss: 0.160481; batch adversarial loss: 0.579683\n",
      "epoch 34; iter: 0; batch classifier loss: 0.064466; batch adversarial loss: 0.585307\n",
      "epoch 35; iter: 0; batch classifier loss: 0.057076; batch adversarial loss: 0.569589\n",
      "epoch 36; iter: 0; batch classifier loss: 0.064689; batch adversarial loss: 0.601603\n",
      "epoch 37; iter: 0; batch classifier loss: 0.142541; batch adversarial loss: 0.474806\n",
      "epoch 38; iter: 0; batch classifier loss: 0.113039; batch adversarial loss: 0.525548\n",
      "epoch 39; iter: 0; batch classifier loss: 0.099941; batch adversarial loss: 0.525545\n",
      "epoch 40; iter: 0; batch classifier loss: 0.134672; batch adversarial loss: 0.598973\n",
      "epoch 41; iter: 0; batch classifier loss: 0.140353; batch adversarial loss: 0.582479\n",
      "epoch 42; iter: 0; batch classifier loss: 0.029931; batch adversarial loss: 0.570870\n",
      "epoch 43; iter: 0; batch classifier loss: 0.124594; batch adversarial loss: 0.557497\n",
      "epoch 44; iter: 0; batch classifier loss: 0.075546; batch adversarial loss: 0.568401\n",
      "epoch 45; iter: 0; batch classifier loss: 0.275344; batch adversarial loss: 0.574825\n",
      "epoch 46; iter: 0; batch classifier loss: 0.083581; batch adversarial loss: 0.543331\n",
      "epoch 47; iter: 0; batch classifier loss: 0.175963; batch adversarial loss: 0.611902\n",
      "epoch 48; iter: 0; batch classifier loss: 0.195059; batch adversarial loss: 0.601485\n",
      "epoch 49; iter: 0; batch classifier loss: 0.095152; batch adversarial loss: 0.518228\n",
      "epoch 50; iter: 0; batch classifier loss: 0.116782; batch adversarial loss: 0.522997\n",
      "epoch 51; iter: 0; batch classifier loss: 0.132643; batch adversarial loss: 0.549513\n",
      "epoch 52; iter: 0; batch classifier loss: 0.154960; batch adversarial loss: 0.523161\n",
      "epoch 53; iter: 0; batch classifier loss: 0.135868; batch adversarial loss: 0.606085\n",
      "epoch 54; iter: 0; batch classifier loss: 0.092714; batch adversarial loss: 0.667026\n",
      "epoch 55; iter: 0; batch classifier loss: 0.063143; batch adversarial loss: 0.634227\n",
      "epoch 56; iter: 0; batch classifier loss: 0.109431; batch adversarial loss: 0.623680\n",
      "epoch 57; iter: 0; batch classifier loss: 0.110049; batch adversarial loss: 0.598450\n",
      "epoch 58; iter: 0; batch classifier loss: 0.094338; batch adversarial loss: 0.523689\n",
      "epoch 59; iter: 0; batch classifier loss: 0.114042; batch adversarial loss: 0.598410\n",
      "epoch 60; iter: 0; batch classifier loss: 0.112573; batch adversarial loss: 0.547868\n",
      "epoch 61; iter: 0; batch classifier loss: 0.166419; batch adversarial loss: 0.565744\n",
      "epoch 62; iter: 0; batch classifier loss: 0.108266; batch adversarial loss: 0.567850\n",
      "epoch 63; iter: 0; batch classifier loss: 0.082428; batch adversarial loss: 0.548657\n",
      "epoch 64; iter: 0; batch classifier loss: 0.112507; batch adversarial loss: 0.589722\n",
      "epoch 65; iter: 0; batch classifier loss: 0.089868; batch adversarial loss: 0.542635\n",
      "epoch 66; iter: 0; batch classifier loss: 0.162839; batch adversarial loss: 0.622719\n",
      "epoch 67; iter: 0; batch classifier loss: 0.052961; batch adversarial loss: 0.578290\n",
      "epoch 68; iter: 0; batch classifier loss: 0.164555; batch adversarial loss: 0.545520\n",
      "epoch 69; iter: 0; batch classifier loss: 0.112131; batch adversarial loss: 0.568006\n",
      "epoch 70; iter: 0; batch classifier loss: 0.113344; batch adversarial loss: 0.585624\n",
      "epoch 71; iter: 0; batch classifier loss: 0.172075; batch adversarial loss: 0.520115\n",
      "epoch 72; iter: 0; batch classifier loss: 0.124744; batch adversarial loss: 0.524996\n",
      "epoch 73; iter: 0; batch classifier loss: 0.166750; batch adversarial loss: 0.638734\n",
      "epoch 74; iter: 0; batch classifier loss: 0.081852; batch adversarial loss: 0.518343\n",
      "epoch 75; iter: 0; batch classifier loss: 0.148843; batch adversarial loss: 0.507405\n",
      "epoch 76; iter: 0; batch classifier loss: 0.081474; batch adversarial loss: 0.560714\n",
      "epoch 77; iter: 0; batch classifier loss: 0.075840; batch adversarial loss: 0.543795\n",
      "epoch 78; iter: 0; batch classifier loss: 0.081255; batch adversarial loss: 0.492898\n",
      "epoch 79; iter: 0; batch classifier loss: 0.052775; batch adversarial loss: 0.493811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.626914; batch adversarial loss: 0.571415\n",
      "epoch 1; iter: 0; batch classifier loss: 0.246338; batch adversarial loss: 0.733814\n",
      "epoch 2; iter: 0; batch classifier loss: 0.168918; batch adversarial loss: 0.521423\n",
      "epoch 3; iter: 0; batch classifier loss: 0.168152; batch adversarial loss: 0.587210\n",
      "epoch 4; iter: 0; batch classifier loss: 0.124597; batch adversarial loss: 0.486316\n",
      "epoch 5; iter: 0; batch classifier loss: 0.122198; batch adversarial loss: 0.568393\n",
      "epoch 6; iter: 0; batch classifier loss: 0.083187; batch adversarial loss: 0.590624\n",
      "epoch 7; iter: 0; batch classifier loss: 0.183943; batch adversarial loss: 0.594723\n",
      "epoch 8; iter: 0; batch classifier loss: 0.176538; batch adversarial loss: 0.463562\n",
      "epoch 9; iter: 0; batch classifier loss: 0.131094; batch adversarial loss: 0.539921\n",
      "epoch 10; iter: 0; batch classifier loss: 0.187511; batch adversarial loss: 0.535159\n",
      "epoch 11; iter: 0; batch classifier loss: 0.033519; batch adversarial loss: 0.457072\n",
      "epoch 12; iter: 0; batch classifier loss: 0.207751; batch adversarial loss: 0.556344\n",
      "epoch 13; iter: 0; batch classifier loss: 0.232002; batch adversarial loss: 0.486274\n",
      "epoch 14; iter: 0; batch classifier loss: 0.069523; batch adversarial loss: 0.561671\n",
      "epoch 15; iter: 0; batch classifier loss: 0.065267; batch adversarial loss: 0.572139\n",
      "epoch 16; iter: 0; batch classifier loss: 0.156348; batch adversarial loss: 0.607573\n",
      "epoch 17; iter: 0; batch classifier loss: 0.201822; batch adversarial loss: 0.643640\n",
      "epoch 18; iter: 0; batch classifier loss: 0.039241; batch adversarial loss: 0.595404\n",
      "epoch 19; iter: 0; batch classifier loss: 0.096699; batch adversarial loss: 0.626616\n",
      "epoch 20; iter: 0; batch classifier loss: 0.180458; batch adversarial loss: 0.599488\n",
      "epoch 21; iter: 0; batch classifier loss: 0.058705; batch adversarial loss: 0.525316\n",
      "epoch 22; iter: 0; batch classifier loss: 0.111039; batch adversarial loss: 0.564665\n",
      "epoch 23; iter: 0; batch classifier loss: 0.175077; batch adversarial loss: 0.569020\n",
      "epoch 24; iter: 0; batch classifier loss: 0.089925; batch adversarial loss: 0.606807\n",
      "epoch 25; iter: 0; batch classifier loss: 0.060510; batch adversarial loss: 0.634198\n",
      "epoch 26; iter: 0; batch classifier loss: 0.114771; batch adversarial loss: 0.569238\n",
      "epoch 27; iter: 0; batch classifier loss: 0.057043; batch adversarial loss: 0.526775\n",
      "epoch 28; iter: 0; batch classifier loss: 0.149299; batch adversarial loss: 0.575343\n",
      "epoch 29; iter: 0; batch classifier loss: 0.082657; batch adversarial loss: 0.570449\n",
      "epoch 30; iter: 0; batch classifier loss: 0.063043; batch adversarial loss: 0.491634\n",
      "epoch 31; iter: 0; batch classifier loss: 0.112786; batch adversarial loss: 0.546853\n",
      "epoch 32; iter: 0; batch classifier loss: 0.054883; batch adversarial loss: 0.578219\n",
      "epoch 33; iter: 0; batch classifier loss: 0.106213; batch adversarial loss: 0.628637\n",
      "epoch 34; iter: 0; batch classifier loss: 0.124332; batch adversarial loss: 0.573203\n",
      "epoch 35; iter: 0; batch classifier loss: 0.086253; batch adversarial loss: 0.555078\n",
      "epoch 36; iter: 0; batch classifier loss: 0.055874; batch adversarial loss: 0.519475\n",
      "epoch 37; iter: 0; batch classifier loss: 0.108967; batch adversarial loss: 0.513186\n",
      "epoch 38; iter: 0; batch classifier loss: 0.109729; batch adversarial loss: 0.517126\n",
      "epoch 39; iter: 0; batch classifier loss: 0.108504; batch adversarial loss: 0.525176\n",
      "epoch 40; iter: 0; batch classifier loss: 0.148142; batch adversarial loss: 0.562227\n",
      "epoch 41; iter: 0; batch classifier loss: 0.139046; batch adversarial loss: 0.578198\n",
      "epoch 42; iter: 0; batch classifier loss: 0.030323; batch adversarial loss: 0.585384\n",
      "epoch 43; iter: 0; batch classifier loss: 0.028950; batch adversarial loss: 0.561138\n",
      "epoch 44; iter: 0; batch classifier loss: 0.227210; batch adversarial loss: 0.552089\n",
      "epoch 45; iter: 0; batch classifier loss: 0.084746; batch adversarial loss: 0.561660\n",
      "epoch 46; iter: 0; batch classifier loss: 0.064616; batch adversarial loss: 0.604655\n",
      "epoch 47; iter: 0; batch classifier loss: 0.135366; batch adversarial loss: 0.544683\n",
      "epoch 48; iter: 0; batch classifier loss: 0.258190; batch adversarial loss: 0.487478\n",
      "epoch 49; iter: 0; batch classifier loss: 0.031790; batch adversarial loss: 0.406684\n",
      "epoch 50; iter: 0; batch classifier loss: 0.116313; batch adversarial loss: 0.514614\n",
      "epoch 51; iter: 0; batch classifier loss: 0.156104; batch adversarial loss: 0.589563\n",
      "epoch 52; iter: 0; batch classifier loss: 0.158837; batch adversarial loss: 0.514647\n",
      "epoch 53; iter: 0; batch classifier loss: 0.106468; batch adversarial loss: 0.588454\n",
      "epoch 54; iter: 0; batch classifier loss: 0.085377; batch adversarial loss: 0.553601\n",
      "epoch 55; iter: 0; batch classifier loss: 0.109411; batch adversarial loss: 0.519993\n",
      "epoch 56; iter: 0; batch classifier loss: 0.127709; batch adversarial loss: 0.637453\n",
      "epoch 57; iter: 0; batch classifier loss: 0.123047; batch adversarial loss: 0.537533\n",
      "epoch 58; iter: 0; batch classifier loss: 0.079338; batch adversarial loss: 0.597356\n",
      "epoch 59; iter: 0; batch classifier loss: 0.141370; batch adversarial loss: 0.587201\n",
      "epoch 60; iter: 0; batch classifier loss: 0.105819; batch adversarial loss: 0.596000\n",
      "epoch 61; iter: 0; batch classifier loss: 0.089366; batch adversarial loss: 0.537161\n",
      "epoch 62; iter: 0; batch classifier loss: 0.055120; batch adversarial loss: 0.545672\n",
      "epoch 63; iter: 0; batch classifier loss: 0.165196; batch adversarial loss: 0.572343\n",
      "epoch 64; iter: 0; batch classifier loss: 0.025107; batch adversarial loss: 0.553495\n",
      "epoch 65; iter: 0; batch classifier loss: 0.151730; batch adversarial loss: 0.511567\n",
      "epoch 66; iter: 0; batch classifier loss: 0.186631; batch adversarial loss: 0.564626\n",
      "epoch 67; iter: 0; batch classifier loss: 0.147239; batch adversarial loss: 0.545752\n",
      "epoch 68; iter: 0; batch classifier loss: 0.161367; batch adversarial loss: 0.589363\n",
      "epoch 69; iter: 0; batch classifier loss: 0.029850; batch adversarial loss: 0.587456\n",
      "epoch 70; iter: 0; batch classifier loss: 0.028213; batch adversarial loss: 0.622316\n",
      "epoch 71; iter: 0; batch classifier loss: 0.193059; batch adversarial loss: 0.534606\n",
      "epoch 72; iter: 0; batch classifier loss: 0.052739; batch adversarial loss: 0.605416\n",
      "epoch 73; iter: 0; batch classifier loss: 0.130275; batch adversarial loss: 0.614935\n",
      "epoch 74; iter: 0; batch classifier loss: 0.138636; batch adversarial loss: 0.518762\n",
      "epoch 75; iter: 0; batch classifier loss: 0.056695; batch adversarial loss: 0.519994\n",
      "epoch 76; iter: 0; batch classifier loss: 0.111098; batch adversarial loss: 0.527361\n",
      "epoch 77; iter: 0; batch classifier loss: 0.165193; batch adversarial loss: 0.519950\n",
      "epoch 78; iter: 0; batch classifier loss: 0.067991; batch adversarial loss: 0.503046\n",
      "epoch 79; iter: 0; batch classifier loss: 0.100664; batch adversarial loss: 0.460667\n",
      "epoch 0; iter: 0; batch classifier loss: 0.549412; batch adversarial loss: 0.517532\n",
      "epoch 1; iter: 0; batch classifier loss: 0.207223; batch adversarial loss: 0.581683\n",
      "epoch 2; iter: 0; batch classifier loss: 0.183875; batch adversarial loss: 0.587603\n",
      "epoch 3; iter: 0; batch classifier loss: 0.066459; batch adversarial loss: 0.575079\n",
      "epoch 4; iter: 0; batch classifier loss: 0.103835; batch adversarial loss: 0.585951\n",
      "epoch 5; iter: 0; batch classifier loss: 0.120112; batch adversarial loss: 0.540509\n",
      "epoch 6; iter: 0; batch classifier loss: 0.177426; batch adversarial loss: 0.507114\n",
      "epoch 7; iter: 0; batch classifier loss: 0.057870; batch adversarial loss: 0.709134\n",
      "epoch 8; iter: 0; batch classifier loss: 0.097386; batch adversarial loss: 0.565048\n",
      "epoch 9; iter: 0; batch classifier loss: 0.187720; batch adversarial loss: 0.503871\n",
      "epoch 10; iter: 0; batch classifier loss: 0.108867; batch adversarial loss: 0.554868\n",
      "epoch 11; iter: 0; batch classifier loss: 0.148526; batch adversarial loss: 0.518975\n",
      "epoch 12; iter: 0; batch classifier loss: 0.088160; batch adversarial loss: 0.593418\n",
      "epoch 13; iter: 0; batch classifier loss: 0.272673; batch adversarial loss: 0.534902\n",
      "epoch 14; iter: 0; batch classifier loss: 0.138765; batch adversarial loss: 0.556883\n",
      "epoch 15; iter: 0; batch classifier loss: 0.062985; batch adversarial loss: 0.570835\n",
      "epoch 16; iter: 0; batch classifier loss: 0.065607; batch adversarial loss: 0.548556\n",
      "epoch 17; iter: 0; batch classifier loss: 0.065578; batch adversarial loss: 0.583067\n",
      "epoch 18; iter: 0; batch classifier loss: 0.035368; batch adversarial loss: 0.567577\n",
      "epoch 19; iter: 0; batch classifier loss: 0.134274; batch adversarial loss: 0.579172\n",
      "epoch 20; iter: 0; batch classifier loss: 0.100777; batch adversarial loss: 0.506418\n",
      "epoch 21; iter: 0; batch classifier loss: 0.086304; batch adversarial loss: 0.608386\n",
      "epoch 22; iter: 0; batch classifier loss: 0.120728; batch adversarial loss: 0.532407\n",
      "epoch 23; iter: 0; batch classifier loss: 0.118989; batch adversarial loss: 0.515602\n",
      "epoch 24; iter: 0; batch classifier loss: 0.030331; batch adversarial loss: 0.559396\n",
      "epoch 25; iter: 0; batch classifier loss: 0.142430; batch adversarial loss: 0.497822\n",
      "epoch 26; iter: 0; batch classifier loss: 0.055818; batch adversarial loss: 0.575530\n",
      "epoch 27; iter: 0; batch classifier loss: 0.110309; batch adversarial loss: 0.611288\n",
      "epoch 28; iter: 0; batch classifier loss: 0.086832; batch adversarial loss: 0.611639\n",
      "epoch 29; iter: 0; batch classifier loss: 0.051722; batch adversarial loss: 0.492458\n",
      "epoch 30; iter: 0; batch classifier loss: 0.090092; batch adversarial loss: 0.568101\n",
      "epoch 31; iter: 0; batch classifier loss: 0.077852; batch adversarial loss: 0.638413\n",
      "epoch 32; iter: 0; batch classifier loss: 0.126954; batch adversarial loss: 0.601915\n",
      "epoch 33; iter: 0; batch classifier loss: 0.131418; batch adversarial loss: 0.595236\n",
      "epoch 34; iter: 0; batch classifier loss: 0.081712; batch adversarial loss: 0.639608\n",
      "epoch 35; iter: 0; batch classifier loss: 0.115280; batch adversarial loss: 0.544716\n",
      "epoch 36; iter: 0; batch classifier loss: 0.205115; batch adversarial loss: 0.582379\n",
      "epoch 37; iter: 0; batch classifier loss: 0.122272; batch adversarial loss: 0.535567\n",
      "epoch 38; iter: 0; batch classifier loss: 0.220629; batch adversarial loss: 0.600470\n",
      "epoch 39; iter: 0; batch classifier loss: 0.098307; batch adversarial loss: 0.559660\n",
      "epoch 40; iter: 0; batch classifier loss: 0.057689; batch adversarial loss: 0.611882\n",
      "epoch 41; iter: 0; batch classifier loss: 0.194885; batch adversarial loss: 0.507666\n",
      "epoch 42; iter: 0; batch classifier loss: 0.215625; batch adversarial loss: 0.598472\n",
      "epoch 43; iter: 0; batch classifier loss: 0.164451; batch adversarial loss: 0.586763\n",
      "epoch 44; iter: 0; batch classifier loss: 0.056685; batch adversarial loss: 0.587689\n",
      "epoch 45; iter: 0; batch classifier loss: 0.052109; batch adversarial loss: 0.553582\n",
      "epoch 46; iter: 0; batch classifier loss: 0.196895; batch adversarial loss: 0.518381\n",
      "epoch 47; iter: 0; batch classifier loss: 0.172879; batch adversarial loss: 0.513512\n",
      "epoch 48; iter: 0; batch classifier loss: 0.056971; batch adversarial loss: 0.535865\n",
      "epoch 49; iter: 0; batch classifier loss: 0.176318; batch adversarial loss: 0.597643\n",
      "epoch 50; iter: 0; batch classifier loss: 0.087137; batch adversarial loss: 0.570439\n",
      "epoch 51; iter: 0; batch classifier loss: 0.194019; batch adversarial loss: 0.612742\n",
      "epoch 52; iter: 0; batch classifier loss: 0.166476; batch adversarial loss: 0.596819\n",
      "epoch 53; iter: 0; batch classifier loss: 0.305006; batch adversarial loss: 0.547669\n",
      "epoch 54; iter: 0; batch classifier loss: 0.074422; batch adversarial loss: 0.563150\n",
      "epoch 55; iter: 0; batch classifier loss: 0.127793; batch adversarial loss: 0.545834\n",
      "epoch 56; iter: 0; batch classifier loss: 0.105686; batch adversarial loss: 0.544269\n",
      "epoch 57; iter: 0; batch classifier loss: 0.099424; batch adversarial loss: 0.605832\n",
      "epoch 58; iter: 0; batch classifier loss: 0.142538; batch adversarial loss: 0.536106\n",
      "epoch 59; iter: 0; batch classifier loss: 0.112754; batch adversarial loss: 0.536361\n",
      "epoch 60; iter: 0; batch classifier loss: 0.125205; batch adversarial loss: 0.527478\n",
      "epoch 61; iter: 0; batch classifier loss: 0.184362; batch adversarial loss: 0.604293\n",
      "epoch 62; iter: 0; batch classifier loss: 0.073649; batch adversarial loss: 0.545436\n",
      "epoch 63; iter: 0; batch classifier loss: 0.095820; batch adversarial loss: 0.522023\n",
      "epoch 64; iter: 0; batch classifier loss: 0.084026; batch adversarial loss: 0.546136\n",
      "epoch 65; iter: 0; batch classifier loss: 0.228285; batch adversarial loss: 0.523718\n",
      "epoch 66; iter: 0; batch classifier loss: 0.117833; batch adversarial loss: 0.600208\n",
      "epoch 67; iter: 0; batch classifier loss: 0.147563; batch adversarial loss: 0.570683\n",
      "epoch 68; iter: 0; batch classifier loss: 0.137780; batch adversarial loss: 0.555578\n",
      "epoch 69; iter: 0; batch classifier loss: 0.140000; batch adversarial loss: 0.582538\n",
      "epoch 70; iter: 0; batch classifier loss: 0.094956; batch adversarial loss: 0.520938\n",
      "epoch 71; iter: 0; batch classifier loss: 0.060618; batch adversarial loss: 0.572564\n",
      "epoch 72; iter: 0; batch classifier loss: 0.121557; batch adversarial loss: 0.537256\n",
      "epoch 73; iter: 0; batch classifier loss: 0.076420; batch adversarial loss: 0.498084\n",
      "epoch 74; iter: 0; batch classifier loss: 0.091562; batch adversarial loss: 0.581387\n",
      "epoch 75; iter: 0; batch classifier loss: 0.182564; batch adversarial loss: 0.583570\n",
      "epoch 76; iter: 0; batch classifier loss: 0.103717; batch adversarial loss: 0.635830\n",
      "epoch 77; iter: 0; batch classifier loss: 0.125652; batch adversarial loss: 0.586663\n",
      "epoch 78; iter: 0; batch classifier loss: 0.158254; batch adversarial loss: 0.602943\n",
      "epoch 79; iter: 0; batch classifier loss: 0.178960; batch adversarial loss: 0.628306\n",
      "epoch 0; iter: 0; batch classifier loss: 0.714564; batch adversarial loss: 0.655639\n",
      "epoch 1; iter: 0; batch classifier loss: 0.660037; batch adversarial loss: 0.654621\n",
      "epoch 2; iter: 0; batch classifier loss: 0.748848; batch adversarial loss: 0.703962\n",
      "epoch 3; iter: 0; batch classifier loss: 0.758567; batch adversarial loss: 0.698106\n",
      "epoch 4; iter: 0; batch classifier loss: 0.815256; batch adversarial loss: 0.679958\n",
      "epoch 5; iter: 0; batch classifier loss: 0.780451; batch adversarial loss: 0.701923\n",
      "epoch 6; iter: 0; batch classifier loss: 0.967288; batch adversarial loss: 0.661935\n",
      "epoch 7; iter: 0; batch classifier loss: 0.900676; batch adversarial loss: 0.697936\n",
      "epoch 8; iter: 0; batch classifier loss: 0.979715; batch adversarial loss: 0.658883\n",
      "epoch 9; iter: 0; batch classifier loss: 1.003562; batch adversarial loss: 0.643134\n",
      "epoch 10; iter: 0; batch classifier loss: 0.964011; batch adversarial loss: 0.635205\n",
      "epoch 11; iter: 0; batch classifier loss: 0.978774; batch adversarial loss: 0.639052\n",
      "epoch 12; iter: 0; batch classifier loss: 0.852654; batch adversarial loss: 0.662460\n",
      "epoch 13; iter: 0; batch classifier loss: 0.468413; batch adversarial loss: 0.614045\n",
      "epoch 14; iter: 0; batch classifier loss: 0.395435; batch adversarial loss: 0.613397\n",
      "epoch 15; iter: 0; batch classifier loss: 0.364593; batch adversarial loss: 0.616766\n",
      "epoch 16; iter: 0; batch classifier loss: 0.271008; batch adversarial loss: 0.617586\n",
      "epoch 17; iter: 0; batch classifier loss: 0.378330; batch adversarial loss: 0.615892\n",
      "epoch 18; iter: 0; batch classifier loss: 0.347185; batch adversarial loss: 0.559987\n",
      "epoch 19; iter: 0; batch classifier loss: 0.426506; batch adversarial loss: 0.573623\n",
      "epoch 20; iter: 0; batch classifier loss: 0.376670; batch adversarial loss: 0.602940\n",
      "epoch 21; iter: 0; batch classifier loss: 0.461945; batch adversarial loss: 0.620866\n",
      "epoch 22; iter: 0; batch classifier loss: 0.394869; batch adversarial loss: 0.641210\n",
      "epoch 23; iter: 0; batch classifier loss: 0.444893; batch adversarial loss: 0.583215\n",
      "epoch 24; iter: 0; batch classifier loss: 0.511696; batch adversarial loss: 0.571318\n",
      "epoch 25; iter: 0; batch classifier loss: 0.448671; batch adversarial loss: 0.610763\n",
      "epoch 26; iter: 0; batch classifier loss: 0.559065; batch adversarial loss: 0.541939\n",
      "epoch 27; iter: 0; batch classifier loss: 0.498676; batch adversarial loss: 0.615510\n",
      "epoch 28; iter: 0; batch classifier loss: 0.588251; batch adversarial loss: 0.546720\n",
      "epoch 29; iter: 0; batch classifier loss: 0.527825; batch adversarial loss: 0.620229\n",
      "epoch 30; iter: 0; batch classifier loss: 0.616721; batch adversarial loss: 0.558194\n",
      "epoch 31; iter: 0; batch classifier loss: 0.311170; batch adversarial loss: 0.611846\n",
      "epoch 32; iter: 0; batch classifier loss: 0.157145; batch adversarial loss: 0.551198\n",
      "epoch 33; iter: 0; batch classifier loss: 0.092912; batch adversarial loss: 0.543044\n",
      "epoch 34; iter: 0; batch classifier loss: 0.075700; batch adversarial loss: 0.569897\n",
      "epoch 35; iter: 0; batch classifier loss: 0.109027; batch adversarial loss: 0.546821\n",
      "epoch 36; iter: 0; batch classifier loss: 0.120722; batch adversarial loss: 0.508951\n",
      "epoch 37; iter: 0; batch classifier loss: 0.142143; batch adversarial loss: 0.550291\n",
      "epoch 38; iter: 0; batch classifier loss: 0.116183; batch adversarial loss: 0.526267\n",
      "epoch 39; iter: 0; batch classifier loss: 0.160342; batch adversarial loss: 0.564410\n",
      "epoch 40; iter: 0; batch classifier loss: 0.121845; batch adversarial loss: 0.547909\n",
      "epoch 41; iter: 0; batch classifier loss: 0.179822; batch adversarial loss: 0.499401\n",
      "epoch 42; iter: 0; batch classifier loss: 0.179056; batch adversarial loss: 0.515380\n",
      "epoch 43; iter: 0; batch classifier loss: 0.135470; batch adversarial loss: 0.645007\n",
      "epoch 44; iter: 0; batch classifier loss: 0.171735; batch adversarial loss: 0.554998\n",
      "epoch 45; iter: 0; batch classifier loss: 0.159180; batch adversarial loss: 0.563600\n",
      "epoch 46; iter: 0; batch classifier loss: 0.080376; batch adversarial loss: 0.588587\n",
      "epoch 47; iter: 0; batch classifier loss: 0.218346; batch adversarial loss: 0.537844\n",
      "epoch 48; iter: 0; batch classifier loss: 0.108502; batch adversarial loss: 0.588209\n",
      "epoch 49; iter: 0; batch classifier loss: 0.150530; batch adversarial loss: 0.521245\n",
      "epoch 50; iter: 0; batch classifier loss: 0.207981; batch adversarial loss: 0.579713\n",
      "epoch 51; iter: 0; batch classifier loss: 0.162787; batch adversarial loss: 0.563634\n",
      "epoch 52; iter: 0; batch classifier loss: 0.089822; batch adversarial loss: 0.554313\n",
      "epoch 53; iter: 0; batch classifier loss: 0.205784; batch adversarial loss: 0.520323\n",
      "epoch 54; iter: 0; batch classifier loss: 0.047829; batch adversarial loss: 0.546201\n",
      "epoch 55; iter: 0; batch classifier loss: 0.091103; batch adversarial loss: 0.545904\n",
      "epoch 56; iter: 0; batch classifier loss: 0.261954; batch adversarial loss: 0.469562\n",
      "epoch 57; iter: 0; batch classifier loss: 0.107083; batch adversarial loss: 0.638926\n",
      "epoch 58; iter: 0; batch classifier loss: 0.063154; batch adversarial loss: 0.545676\n",
      "epoch 59; iter: 0; batch classifier loss: 0.093057; batch adversarial loss: 0.553983\n",
      "epoch 60; iter: 0; batch classifier loss: 0.235805; batch adversarial loss: 0.545361\n",
      "epoch 61; iter: 0; batch classifier loss: 0.121782; batch adversarial loss: 0.579263\n",
      "epoch 62; iter: 0; batch classifier loss: 0.213281; batch adversarial loss: 0.511435\n",
      "epoch 63; iter: 0; batch classifier loss: 0.148451; batch adversarial loss: 0.511786\n",
      "epoch 64; iter: 0; batch classifier loss: 0.168492; batch adversarial loss: 0.511647\n",
      "epoch 65; iter: 0; batch classifier loss: 0.107435; batch adversarial loss: 0.587830\n",
      "epoch 66; iter: 0; batch classifier loss: 0.263093; batch adversarial loss: 0.579655\n",
      "epoch 67; iter: 0; batch classifier loss: 0.103898; batch adversarial loss: 0.553718\n",
      "epoch 68; iter: 0; batch classifier loss: 0.132536; batch adversarial loss: 0.537079\n",
      "epoch 69; iter: 0; batch classifier loss: 0.114911; batch adversarial loss: 0.503018\n",
      "epoch 70; iter: 0; batch classifier loss: 0.055431; batch adversarial loss: 0.630273\n",
      "epoch 71; iter: 0; batch classifier loss: 0.111204; batch adversarial loss: 0.528839\n",
      "epoch 72; iter: 0; batch classifier loss: 0.142695; batch adversarial loss: 0.553899\n",
      "epoch 73; iter: 0; batch classifier loss: 0.141943; batch adversarial loss: 0.562404\n",
      "epoch 74; iter: 0; batch classifier loss: 0.156962; batch adversarial loss: 0.613377\n",
      "epoch 75; iter: 0; batch classifier loss: 0.060280; batch adversarial loss: 0.545614\n",
      "epoch 76; iter: 0; batch classifier loss: 0.032308; batch adversarial loss: 0.528311\n",
      "epoch 77; iter: 0; batch classifier loss: 0.158133; batch adversarial loss: 0.553895\n",
      "epoch 78; iter: 0; batch classifier loss: 0.243713; batch adversarial loss: 0.579382\n",
      "epoch 79; iter: 0; batch classifier loss: 0.172963; batch adversarial loss: 0.511082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.572303; batch adversarial loss: 0.657578\n",
      "epoch 1; iter: 0; batch classifier loss: 0.330898; batch adversarial loss: 0.630192\n",
      "epoch 2; iter: 0; batch classifier loss: 0.290148; batch adversarial loss: 0.582813\n",
      "epoch 3; iter: 0; batch classifier loss: 0.268654; batch adversarial loss: 0.591296\n",
      "epoch 4; iter: 0; batch classifier loss: 0.301352; batch adversarial loss: 0.597786\n",
      "epoch 5; iter: 0; batch classifier loss: 0.333045; batch adversarial loss: 0.615646\n",
      "epoch 6; iter: 0; batch classifier loss: 0.252648; batch adversarial loss: 0.611990\n",
      "epoch 7; iter: 0; batch classifier loss: 0.110809; batch adversarial loss: 0.661256\n",
      "epoch 8; iter: 0; batch classifier loss: 0.131561; batch adversarial loss: 0.591828\n",
      "epoch 9; iter: 0; batch classifier loss: 0.166060; batch adversarial loss: 0.574238\n",
      "epoch 10; iter: 0; batch classifier loss: 0.187843; batch adversarial loss: 0.585167\n",
      "epoch 11; iter: 0; batch classifier loss: 0.129924; batch adversarial loss: 0.600928\n",
      "epoch 12; iter: 0; batch classifier loss: 0.220311; batch adversarial loss: 0.607094\n",
      "epoch 13; iter: 0; batch classifier loss: 0.234209; batch adversarial loss: 0.567388\n",
      "epoch 14; iter: 0; batch classifier loss: 0.276097; batch adversarial loss: 0.516812\n",
      "epoch 15; iter: 0; batch classifier loss: 0.246213; batch adversarial loss: 0.596500\n",
      "epoch 16; iter: 0; batch classifier loss: 0.148384; batch adversarial loss: 0.576576\n",
      "epoch 17; iter: 0; batch classifier loss: 0.180965; batch adversarial loss: 0.499867\n",
      "epoch 18; iter: 0; batch classifier loss: 0.200412; batch adversarial loss: 0.602576\n",
      "epoch 19; iter: 0; batch classifier loss: 0.285933; batch adversarial loss: 0.649111\n",
      "epoch 20; iter: 0; batch classifier loss: 0.152582; batch adversarial loss: 0.583107\n",
      "epoch 21; iter: 0; batch classifier loss: 0.121146; batch adversarial loss: 0.586853\n",
      "epoch 22; iter: 0; batch classifier loss: 0.191659; batch adversarial loss: 0.630203\n",
      "epoch 23; iter: 0; batch classifier loss: 0.067212; batch adversarial loss: 0.549555\n",
      "epoch 24; iter: 0; batch classifier loss: 0.116459; batch adversarial loss: 0.567336\n",
      "epoch 25; iter: 0; batch classifier loss: 0.104372; batch adversarial loss: 0.531132\n",
      "epoch 26; iter: 0; batch classifier loss: 0.189641; batch adversarial loss: 0.626791\n",
      "epoch 27; iter: 0; batch classifier loss: 0.117385; batch adversarial loss: 0.576960\n",
      "epoch 28; iter: 0; batch classifier loss: 0.123655; batch adversarial loss: 0.654928\n",
      "epoch 29; iter: 0; batch classifier loss: 0.254132; batch adversarial loss: 0.577822\n",
      "epoch 30; iter: 0; batch classifier loss: 0.093949; batch adversarial loss: 0.509959\n",
      "epoch 31; iter: 0; batch classifier loss: 0.087885; batch adversarial loss: 0.517893\n",
      "epoch 32; iter: 0; batch classifier loss: 0.074574; batch adversarial loss: 0.660323\n",
      "epoch 33; iter: 0; batch classifier loss: 0.166548; batch adversarial loss: 0.486781\n",
      "epoch 34; iter: 0; batch classifier loss: 0.078184; batch adversarial loss: 0.568214\n",
      "epoch 35; iter: 0; batch classifier loss: 0.068703; batch adversarial loss: 0.549076\n",
      "epoch 36; iter: 0; batch classifier loss: 0.066065; batch adversarial loss: 0.494682\n",
      "epoch 37; iter: 0; batch classifier loss: 0.149954; batch adversarial loss: 0.576594\n",
      "epoch 38; iter: 0; batch classifier loss: 0.135593; batch adversarial loss: 0.638646\n",
      "epoch 39; iter: 0; batch classifier loss: 0.125360; batch adversarial loss: 0.562112\n",
      "epoch 40; iter: 0; batch classifier loss: 0.133209; batch adversarial loss: 0.582587\n",
      "epoch 41; iter: 0; batch classifier loss: 0.171771; batch adversarial loss: 0.530238\n",
      "epoch 42; iter: 0; batch classifier loss: 0.043975; batch adversarial loss: 0.528539\n",
      "epoch 43; iter: 0; batch classifier loss: 0.150032; batch adversarial loss: 0.596689\n",
      "epoch 44; iter: 0; batch classifier loss: 0.090383; batch adversarial loss: 0.479889\n",
      "epoch 45; iter: 0; batch classifier loss: 0.291815; batch adversarial loss: 0.574791\n",
      "epoch 46; iter: 0; batch classifier loss: 0.099965; batch adversarial loss: 0.516541\n",
      "epoch 47; iter: 0; batch classifier loss: 0.190300; batch adversarial loss: 0.566173\n",
      "epoch 48; iter: 0; batch classifier loss: 0.196039; batch adversarial loss: 0.547632\n",
      "epoch 49; iter: 0; batch classifier loss: 0.106289; batch adversarial loss: 0.605637\n",
      "epoch 50; iter: 0; batch classifier loss: 0.121138; batch adversarial loss: 0.564101\n",
      "epoch 51; iter: 0; batch classifier loss: 0.141723; batch adversarial loss: 0.629753\n",
      "epoch 52; iter: 0; batch classifier loss: 0.171443; batch adversarial loss: 0.551826\n",
      "epoch 53; iter: 0; batch classifier loss: 0.140794; batch adversarial loss: 0.494543\n",
      "epoch 54; iter: 0; batch classifier loss: 0.094945; batch adversarial loss: 0.506311\n",
      "epoch 55; iter: 0; batch classifier loss: 0.065153; batch adversarial loss: 0.556780\n",
      "epoch 56; iter: 0; batch classifier loss: 0.106498; batch adversarial loss: 0.518198\n",
      "epoch 57; iter: 0; batch classifier loss: 0.108991; batch adversarial loss: 0.572112\n",
      "epoch 58; iter: 0; batch classifier loss: 0.092478; batch adversarial loss: 0.586443\n",
      "epoch 59; iter: 0; batch classifier loss: 0.116977; batch adversarial loss: 0.499284\n",
      "epoch 60; iter: 0; batch classifier loss: 0.118554; batch adversarial loss: 0.561887\n",
      "epoch 61; iter: 0; batch classifier loss: 0.199911; batch adversarial loss: 0.486968\n",
      "epoch 62; iter: 0; batch classifier loss: 0.116795; batch adversarial loss: 0.517792\n",
      "epoch 63; iter: 0; batch classifier loss: 0.079168; batch adversarial loss: 0.560149\n",
      "epoch 64; iter: 0; batch classifier loss: 0.110358; batch adversarial loss: 0.517943\n",
      "epoch 65; iter: 0; batch classifier loss: 0.095449; batch adversarial loss: 0.573343\n",
      "epoch 66; iter: 0; batch classifier loss: 0.152209; batch adversarial loss: 0.549577\n",
      "epoch 67; iter: 0; batch classifier loss: 0.053640; batch adversarial loss: 0.504050\n",
      "epoch 68; iter: 0; batch classifier loss: 0.164522; batch adversarial loss: 0.554972\n",
      "epoch 69; iter: 0; batch classifier loss: 0.116441; batch adversarial loss: 0.517861\n",
      "epoch 70; iter: 0; batch classifier loss: 0.117464; batch adversarial loss: 0.515490\n",
      "epoch 71; iter: 0; batch classifier loss: 0.182592; batch adversarial loss: 0.612019\n",
      "epoch 72; iter: 0; batch classifier loss: 0.122407; batch adversarial loss: 0.533197\n",
      "epoch 73; iter: 0; batch classifier loss: 0.168874; batch adversarial loss: 0.548849\n",
      "epoch 74; iter: 0; batch classifier loss: 0.089356; batch adversarial loss: 0.611490\n",
      "epoch 75; iter: 0; batch classifier loss: 0.162457; batch adversarial loss: 0.571598\n",
      "epoch 76; iter: 0; batch classifier loss: 0.079055; batch adversarial loss: 0.562816\n",
      "epoch 77; iter: 0; batch classifier loss: 0.079021; batch adversarial loss: 0.632364\n",
      "epoch 78; iter: 0; batch classifier loss: 0.082180; batch adversarial loss: 0.569296\n",
      "epoch 79; iter: 0; batch classifier loss: 0.056592; batch adversarial loss: 0.591455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.639972; batch adversarial loss: 1.132146\n",
      "epoch 1; iter: 0; batch classifier loss: 0.272889; batch adversarial loss: 1.428081\n",
      "epoch 2; iter: 0; batch classifier loss: 0.170081; batch adversarial loss: 1.490721\n",
      "epoch 3; iter: 0; batch classifier loss: 0.169925; batch adversarial loss: 1.559849\n",
      "epoch 4; iter: 0; batch classifier loss: 0.112039; batch adversarial loss: 1.480471\n",
      "epoch 5; iter: 0; batch classifier loss: 0.103736; batch adversarial loss: 1.432502\n",
      "epoch 6; iter: 0; batch classifier loss: 0.073891; batch adversarial loss: 1.488530\n",
      "epoch 7; iter: 0; batch classifier loss: 0.168483; batch adversarial loss: 1.386825\n",
      "epoch 8; iter: 0; batch classifier loss: 0.168687; batch adversarial loss: 1.231662\n",
      "epoch 9; iter: 0; batch classifier loss: 0.121383; batch adversarial loss: 1.305665\n",
      "epoch 10; iter: 0; batch classifier loss: 0.181596; batch adversarial loss: 1.239091\n",
      "epoch 11; iter: 0; batch classifier loss: 0.033256; batch adversarial loss: 1.143036\n",
      "epoch 12; iter: 0; batch classifier loss: 0.197968; batch adversarial loss: 1.188326\n",
      "epoch 13; iter: 0; batch classifier loss: 0.230685; batch adversarial loss: 1.012786\n",
      "epoch 14; iter: 0; batch classifier loss: 0.062109; batch adversarial loss: 1.043377\n",
      "epoch 15; iter: 0; batch classifier loss: 0.064832; batch adversarial loss: 1.106933\n",
      "epoch 16; iter: 0; batch classifier loss: 0.153242; batch adversarial loss: 1.000267\n",
      "epoch 17; iter: 0; batch classifier loss: 0.190641; batch adversarial loss: 0.981475\n",
      "epoch 18; iter: 0; batch classifier loss: 0.036911; batch adversarial loss: 0.977221\n",
      "epoch 19; iter: 0; batch classifier loss: 0.101403; batch adversarial loss: 0.938395\n",
      "epoch 20; iter: 0; batch classifier loss: 0.177213; batch adversarial loss: 0.909643\n",
      "epoch 21; iter: 0; batch classifier loss: 0.058635; batch adversarial loss: 0.888590\n",
      "epoch 22; iter: 0; batch classifier loss: 0.111483; batch adversarial loss: 0.845936\n",
      "epoch 23; iter: 0; batch classifier loss: 0.175579; batch adversarial loss: 0.795126\n",
      "epoch 24; iter: 0; batch classifier loss: 0.086541; batch adversarial loss: 0.809834\n",
      "epoch 25; iter: 0; batch classifier loss: 0.063551; batch adversarial loss: 0.774575\n",
      "epoch 26; iter: 0; batch classifier loss: 0.115264; batch adversarial loss: 0.762517\n",
      "epoch 27; iter: 0; batch classifier loss: 0.055778; batch adversarial loss: 0.754854\n",
      "epoch 28; iter: 0; batch classifier loss: 0.146003; batch adversarial loss: 0.729568\n",
      "epoch 29; iter: 0; batch classifier loss: 0.082707; batch adversarial loss: 0.729187\n",
      "epoch 30; iter: 0; batch classifier loss: 0.065949; batch adversarial loss: 0.704744\n",
      "epoch 31; iter: 0; batch classifier loss: 0.111659; batch adversarial loss: 0.696155\n",
      "epoch 32; iter: 0; batch classifier loss: 0.060072; batch adversarial loss: 0.682622\n",
      "epoch 33; iter: 0; batch classifier loss: 0.107001; batch adversarial loss: 0.664320\n",
      "epoch 34; iter: 0; batch classifier loss: 0.123127; batch adversarial loss: 0.665005\n",
      "epoch 35; iter: 0; batch classifier loss: 0.084539; batch adversarial loss: 0.654948\n",
      "epoch 36; iter: 0; batch classifier loss: 0.059008; batch adversarial loss: 0.647182\n",
      "epoch 37; iter: 0; batch classifier loss: 0.108358; batch adversarial loss: 0.645786\n",
      "epoch 38; iter: 0; batch classifier loss: 0.113165; batch adversarial loss: 0.628442\n",
      "epoch 39; iter: 0; batch classifier loss: 0.112592; batch adversarial loss: 0.614785\n",
      "epoch 40; iter: 0; batch classifier loss: 0.147853; batch adversarial loss: 0.614485\n",
      "epoch 41; iter: 0; batch classifier loss: 0.135137; batch adversarial loss: 0.601584\n",
      "epoch 42; iter: 0; batch classifier loss: 0.035884; batch adversarial loss: 0.601026\n",
      "epoch 43; iter: 0; batch classifier loss: 0.033503; batch adversarial loss: 0.616644\n",
      "epoch 44; iter: 0; batch classifier loss: 0.219547; batch adversarial loss: 0.588018\n",
      "epoch 45; iter: 0; batch classifier loss: 0.090765; batch adversarial loss: 0.604601\n",
      "epoch 46; iter: 0; batch classifier loss: 0.070856; batch adversarial loss: 0.588949\n",
      "epoch 47; iter: 0; batch classifier loss: 0.134527; batch adversarial loss: 0.577758\n",
      "epoch 48; iter: 0; batch classifier loss: 0.261522; batch adversarial loss: 0.585962\n",
      "epoch 49; iter: 0; batch classifier loss: 0.032935; batch adversarial loss: 0.614919\n",
      "epoch 50; iter: 0; batch classifier loss: 0.118178; batch adversarial loss: 0.584409\n",
      "epoch 51; iter: 0; batch classifier loss: 0.150641; batch adversarial loss: 0.591427\n",
      "epoch 52; iter: 0; batch classifier loss: 0.159862; batch adversarial loss: 0.620341\n",
      "epoch 53; iter: 0; batch classifier loss: 0.110995; batch adversarial loss: 0.545632\n",
      "epoch 54; iter: 0; batch classifier loss: 0.086508; batch adversarial loss: 0.595415\n",
      "epoch 55; iter: 0; batch classifier loss: 0.110105; batch adversarial loss: 0.563991\n",
      "epoch 56; iter: 0; batch classifier loss: 0.123669; batch adversarial loss: 0.571697\n",
      "epoch 57; iter: 0; batch classifier loss: 0.130681; batch adversarial loss: 0.556390\n",
      "epoch 58; iter: 0; batch classifier loss: 0.076507; batch adversarial loss: 0.564943\n",
      "epoch 59; iter: 0; batch classifier loss: 0.131994; batch adversarial loss: 0.578671\n",
      "epoch 60; iter: 0; batch classifier loss: 0.106043; batch adversarial loss: 0.561412\n",
      "epoch 61; iter: 0; batch classifier loss: 0.100007; batch adversarial loss: 0.564100\n",
      "epoch 62; iter: 0; batch classifier loss: 0.080523; batch adversarial loss: 0.587515\n",
      "epoch 63; iter: 0; batch classifier loss: 0.165690; batch adversarial loss: 0.521967\n",
      "epoch 64; iter: 0; batch classifier loss: 0.037112; batch adversarial loss: 0.510233\n",
      "epoch 65; iter: 0; batch classifier loss: 0.141084; batch adversarial loss: 0.579305\n",
      "epoch 66; iter: 0; batch classifier loss: 0.189385; batch adversarial loss: 0.639707\n",
      "epoch 67; iter: 0; batch classifier loss: 0.159891; batch adversarial loss: 0.502822\n",
      "epoch 68; iter: 0; batch classifier loss: 0.145776; batch adversarial loss: 0.595947\n",
      "epoch 69; iter: 0; batch classifier loss: 0.038943; batch adversarial loss: 0.571749\n",
      "epoch 70; iter: 0; batch classifier loss: 0.039083; batch adversarial loss: 0.525945\n",
      "epoch 71; iter: 0; batch classifier loss: 0.203798; batch adversarial loss: 0.567122\n",
      "epoch 72; iter: 0; batch classifier loss: 0.059836; batch adversarial loss: 0.562797\n",
      "epoch 73; iter: 0; batch classifier loss: 0.126165; batch adversarial loss: 0.566168\n",
      "epoch 74; iter: 0; batch classifier loss: 0.140202; batch adversarial loss: 0.618480\n",
      "epoch 75; iter: 0; batch classifier loss: 0.059309; batch adversarial loss: 0.586223\n",
      "epoch 76; iter: 0; batch classifier loss: 0.112722; batch adversarial loss: 0.513033\n",
      "epoch 77; iter: 0; batch classifier loss: 0.178068; batch adversarial loss: 0.505961\n",
      "epoch 78; iter: 0; batch classifier loss: 0.067208; batch adversarial loss: 0.539591\n",
      "epoch 79; iter: 0; batch classifier loss: 0.098542; batch adversarial loss: 0.622665\n",
      "epoch 0; iter: 0; batch classifier loss: 0.543298; batch adversarial loss: 0.979866\n",
      "epoch 1; iter: 0; batch classifier loss: 0.299289; batch adversarial loss: 1.151073\n",
      "epoch 2; iter: 0; batch classifier loss: 0.229908; batch adversarial loss: 1.202869\n",
      "epoch 3; iter: 0; batch classifier loss: 0.121849; batch adversarial loss: 1.084069\n",
      "epoch 4; iter: 0; batch classifier loss: 0.135988; batch adversarial loss: 1.084921\n",
      "epoch 5; iter: 0; batch classifier loss: 0.124859; batch adversarial loss: 1.101661\n",
      "epoch 6; iter: 0; batch classifier loss: 0.197404; batch adversarial loss: 0.994099\n",
      "epoch 7; iter: 0; batch classifier loss: 0.061377; batch adversarial loss: 1.009976\n",
      "epoch 8; iter: 0; batch classifier loss: 0.106288; batch adversarial loss: 0.927690\n",
      "epoch 9; iter: 0; batch classifier loss: 0.192235; batch adversarial loss: 0.903560\n",
      "epoch 10; iter: 0; batch classifier loss: 0.105520; batch adversarial loss: 0.916855\n",
      "epoch 11; iter: 0; batch classifier loss: 0.149773; batch adversarial loss: 0.834808\n",
      "epoch 12; iter: 0; batch classifier loss: 0.093837; batch adversarial loss: 0.847714\n",
      "epoch 13; iter: 0; batch classifier loss: 0.255765; batch adversarial loss: 0.825467\n",
      "epoch 14; iter: 0; batch classifier loss: 0.142055; batch adversarial loss: 0.805726\n",
      "epoch 15; iter: 0; batch classifier loss: 0.067028; batch adversarial loss: 0.784585\n",
      "epoch 16; iter: 0; batch classifier loss: 0.068595; batch adversarial loss: 0.764519\n",
      "epoch 17; iter: 0; batch classifier loss: 0.067547; batch adversarial loss: 0.743609\n",
      "epoch 18; iter: 0; batch classifier loss: 0.038250; batch adversarial loss: 0.719804\n",
      "epoch 19; iter: 0; batch classifier loss: 0.142504; batch adversarial loss: 0.706984\n",
      "epoch 20; iter: 0; batch classifier loss: 0.102862; batch adversarial loss: 0.695465\n",
      "epoch 21; iter: 0; batch classifier loss: 0.081291; batch adversarial loss: 0.680044\n",
      "epoch 22; iter: 0; batch classifier loss: 0.125564; batch adversarial loss: 0.662682\n",
      "epoch 23; iter: 0; batch classifier loss: 0.123967; batch adversarial loss: 0.661201\n",
      "epoch 24; iter: 0; batch classifier loss: 0.031505; batch adversarial loss: 0.645044\n",
      "epoch 25; iter: 0; batch classifier loss: 0.145961; batch adversarial loss: 0.619766\n",
      "epoch 26; iter: 0; batch classifier loss: 0.056333; batch adversarial loss: 0.629446\n",
      "epoch 27; iter: 0; batch classifier loss: 0.110683; batch adversarial loss: 0.637466\n",
      "epoch 28; iter: 0; batch classifier loss: 0.088539; batch adversarial loss: 0.631791\n",
      "epoch 29; iter: 0; batch classifier loss: 0.050800; batch adversarial loss: 0.605442\n",
      "epoch 30; iter: 0; batch classifier loss: 0.091197; batch adversarial loss: 0.601263\n",
      "epoch 31; iter: 0; batch classifier loss: 0.074162; batch adversarial loss: 0.549085\n",
      "epoch 32; iter: 0; batch classifier loss: 0.126673; batch adversarial loss: 0.592515\n",
      "epoch 33; iter: 0; batch classifier loss: 0.130666; batch adversarial loss: 0.577165\n",
      "epoch 34; iter: 0; batch classifier loss: 0.085887; batch adversarial loss: 0.587045\n",
      "epoch 35; iter: 0; batch classifier loss: 0.105477; batch adversarial loss: 0.576226\n",
      "epoch 36; iter: 0; batch classifier loss: 0.205366; batch adversarial loss: 0.613673\n",
      "epoch 37; iter: 0; batch classifier loss: 0.126803; batch adversarial loss: 0.601819\n",
      "epoch 38; iter: 0; batch classifier loss: 0.226288; batch adversarial loss: 0.604104\n",
      "epoch 39; iter: 0; batch classifier loss: 0.101307; batch adversarial loss: 0.532099\n",
      "epoch 40; iter: 0; batch classifier loss: 0.054353; batch adversarial loss: 0.553619\n",
      "epoch 41; iter: 0; batch classifier loss: 0.182397; batch adversarial loss: 0.534721\n",
      "epoch 42; iter: 0; batch classifier loss: 0.210084; batch adversarial loss: 0.504640\n",
      "epoch 43; iter: 0; batch classifier loss: 0.171321; batch adversarial loss: 0.573142\n",
      "epoch 44; iter: 0; batch classifier loss: 0.065505; batch adversarial loss: 0.563549\n",
      "epoch 45; iter: 0; batch classifier loss: 0.066994; batch adversarial loss: 0.618515\n",
      "epoch 46; iter: 0; batch classifier loss: 0.203286; batch adversarial loss: 0.536863\n",
      "epoch 47; iter: 0; batch classifier loss: 0.155798; batch adversarial loss: 0.525722\n",
      "epoch 48; iter: 0; batch classifier loss: 0.073956; batch adversarial loss: 0.590528\n",
      "epoch 49; iter: 0; batch classifier loss: 0.187626; batch adversarial loss: 0.516198\n",
      "epoch 50; iter: 0; batch classifier loss: 0.105776; batch adversarial loss: 0.591402\n",
      "epoch 51; iter: 0; batch classifier loss: 0.192741; batch adversarial loss: 0.559116\n",
      "epoch 52; iter: 0; batch classifier loss: 0.170780; batch adversarial loss: 0.492812\n",
      "epoch 53; iter: 0; batch classifier loss: 0.281305; batch adversarial loss: 0.551592\n",
      "epoch 54; iter: 0; batch classifier loss: 0.105057; batch adversarial loss: 0.556277\n",
      "epoch 55; iter: 0; batch classifier loss: 0.134068; batch adversarial loss: 0.534862\n",
      "epoch 56; iter: 0; batch classifier loss: 0.129970; batch adversarial loss: 0.556749\n",
      "epoch 57; iter: 0; batch classifier loss: 0.125400; batch adversarial loss: 0.527376\n",
      "epoch 58; iter: 0; batch classifier loss: 0.139026; batch adversarial loss: 0.541459\n",
      "epoch 59; iter: 0; batch classifier loss: 0.121224; batch adversarial loss: 0.486935\n",
      "epoch 60; iter: 0; batch classifier loss: 0.122310; batch adversarial loss: 0.587082\n",
      "epoch 61; iter: 0; batch classifier loss: 0.175128; batch adversarial loss: 0.506782\n",
      "epoch 62; iter: 0; batch classifier loss: 0.077652; batch adversarial loss: 0.534629\n",
      "epoch 63; iter: 0; batch classifier loss: 0.103379; batch adversarial loss: 0.520941\n",
      "epoch 64; iter: 0; batch classifier loss: 0.097454; batch adversarial loss: 0.554873\n",
      "epoch 65; iter: 0; batch classifier loss: 0.226286; batch adversarial loss: 0.548449\n",
      "epoch 66; iter: 0; batch classifier loss: 0.130168; batch adversarial loss: 0.572560\n",
      "epoch 67; iter: 0; batch classifier loss: 0.157774; batch adversarial loss: 0.532989\n",
      "epoch 68; iter: 0; batch classifier loss: 0.151161; batch adversarial loss: 0.530599\n",
      "epoch 69; iter: 0; batch classifier loss: 0.137875; batch adversarial loss: 0.544579\n",
      "epoch 70; iter: 0; batch classifier loss: 0.090537; batch adversarial loss: 0.579521\n",
      "epoch 71; iter: 0; batch classifier loss: 0.065509; batch adversarial loss: 0.588364\n",
      "epoch 72; iter: 0; batch classifier loss: 0.118039; batch adversarial loss: 0.567436\n",
      "epoch 73; iter: 0; batch classifier loss: 0.074420; batch adversarial loss: 0.684860\n",
      "epoch 74; iter: 0; batch classifier loss: 0.092672; batch adversarial loss: 0.589954\n",
      "epoch 75; iter: 0; batch classifier loss: 0.191008; batch adversarial loss: 0.548735\n",
      "epoch 76; iter: 0; batch classifier loss: 0.086066; batch adversarial loss: 0.532784\n",
      "epoch 77; iter: 0; batch classifier loss: 0.102668; batch adversarial loss: 0.641933\n",
      "epoch 78; iter: 0; batch classifier loss: 0.141580; batch adversarial loss: 0.523744\n",
      "epoch 79; iter: 0; batch classifier loss: 0.116492; batch adversarial loss: 0.531286\n",
      "epoch 0; iter: 0; batch classifier loss: 0.728900; batch adversarial loss: 0.787722\n",
      "epoch 1; iter: 0; batch classifier loss: 0.491058; batch adversarial loss: 0.790809\n",
      "epoch 2; iter: 0; batch classifier loss: 0.388437; batch adversarial loss: 0.768156\n",
      "epoch 3; iter: 0; batch classifier loss: 0.306943; batch adversarial loss: 0.757832\n",
      "epoch 4; iter: 0; batch classifier loss: 0.318852; batch adversarial loss: 0.742625\n",
      "epoch 5; iter: 0; batch classifier loss: 0.261430; batch adversarial loss: 0.723105\n",
      "epoch 6; iter: 0; batch classifier loss: 0.227593; batch adversarial loss: 0.700696\n",
      "epoch 7; iter: 0; batch classifier loss: 0.230867; batch adversarial loss: 0.682817\n",
      "epoch 8; iter: 0; batch classifier loss: 0.171266; batch adversarial loss: 0.668225\n",
      "epoch 9; iter: 0; batch classifier loss: 0.219676; batch adversarial loss: 0.665390\n",
      "epoch 10; iter: 0; batch classifier loss: 0.214366; batch adversarial loss: 0.655975\n",
      "epoch 11; iter: 0; batch classifier loss: 0.133155; batch adversarial loss: 0.628821\n",
      "epoch 12; iter: 0; batch classifier loss: 0.341541; batch adversarial loss: 0.603173\n",
      "epoch 13; iter: 0; batch classifier loss: 0.154177; batch adversarial loss: 0.608592\n",
      "epoch 14; iter: 0; batch classifier loss: 0.250699; batch adversarial loss: 0.593562\n",
      "epoch 15; iter: 0; batch classifier loss: 0.169012; batch adversarial loss: 0.594961\n",
      "epoch 16; iter: 0; batch classifier loss: 0.159429; batch adversarial loss: 0.592147\n",
      "epoch 17; iter: 0; batch classifier loss: 0.209134; batch adversarial loss: 0.598861\n",
      "epoch 18; iter: 0; batch classifier loss: 0.238343; batch adversarial loss: 0.607905\n",
      "epoch 19; iter: 0; batch classifier loss: 0.186096; batch adversarial loss: 0.607277\n",
      "epoch 20; iter: 0; batch classifier loss: 0.177397; batch adversarial loss: 0.598564\n",
      "epoch 21; iter: 0; batch classifier loss: 0.289815; batch adversarial loss: 0.565219\n",
      "epoch 22; iter: 0; batch classifier loss: 0.206511; batch adversarial loss: 0.547120\n",
      "epoch 23; iter: 0; batch classifier loss: 0.174398; batch adversarial loss: 0.596514\n",
      "epoch 24; iter: 0; batch classifier loss: 0.296736; batch adversarial loss: 0.604859\n",
      "epoch 25; iter: 0; batch classifier loss: 0.223925; batch adversarial loss: 0.546409\n",
      "epoch 26; iter: 0; batch classifier loss: 0.240994; batch adversarial loss: 0.549384\n",
      "epoch 27; iter: 0; batch classifier loss: 0.258386; batch adversarial loss: 0.572525\n",
      "epoch 28; iter: 0; batch classifier loss: 0.234425; batch adversarial loss: 0.635036\n",
      "epoch 29; iter: 0; batch classifier loss: 0.316312; batch adversarial loss: 0.608268\n",
      "epoch 30; iter: 0; batch classifier loss: 0.304971; batch adversarial loss: 0.549240\n",
      "epoch 31; iter: 0; batch classifier loss: 0.410506; batch adversarial loss: 0.537049\n",
      "epoch 32; iter: 0; batch classifier loss: 0.310634; batch adversarial loss: 0.610977\n",
      "epoch 33; iter: 0; batch classifier loss: 0.316435; batch adversarial loss: 0.594813\n",
      "epoch 34; iter: 0; batch classifier loss: 0.353366; batch adversarial loss: 0.659410\n",
      "epoch 35; iter: 0; batch classifier loss: 0.396437; batch adversarial loss: 0.577039\n",
      "epoch 36; iter: 0; batch classifier loss: 0.321048; batch adversarial loss: 0.601430\n",
      "epoch 37; iter: 0; batch classifier loss: 0.434876; batch adversarial loss: 0.607874\n",
      "epoch 38; iter: 0; batch classifier loss: 0.430771; batch adversarial loss: 0.586721\n",
      "epoch 39; iter: 0; batch classifier loss: 0.605848; batch adversarial loss: 0.559367\n",
      "epoch 40; iter: 0; batch classifier loss: 0.638673; batch adversarial loss: 0.562596\n",
      "epoch 41; iter: 0; batch classifier loss: 0.742575; batch adversarial loss: 0.582508\n",
      "epoch 42; iter: 0; batch classifier loss: 0.700742; batch adversarial loss: 0.594506\n",
      "epoch 43; iter: 0; batch classifier loss: 0.838684; batch adversarial loss: 0.555862\n",
      "epoch 44; iter: 0; batch classifier loss: 0.851306; batch adversarial loss: 0.570402\n",
      "epoch 45; iter: 0; batch classifier loss: 0.744055; batch adversarial loss: 0.562818\n",
      "epoch 46; iter: 0; batch classifier loss: 0.680733; batch adversarial loss: 0.632959\n",
      "epoch 47; iter: 0; batch classifier loss: 0.805558; batch adversarial loss: 0.561891\n",
      "epoch 48; iter: 0; batch classifier loss: 0.796194; batch adversarial loss: 0.547763\n",
      "epoch 49; iter: 0; batch classifier loss: 0.787541; batch adversarial loss: 0.549660\n",
      "epoch 50; iter: 0; batch classifier loss: 0.320518; batch adversarial loss: 0.575727\n",
      "epoch 51; iter: 0; batch classifier loss: 0.170712; batch adversarial loss: 0.545024\n",
      "epoch 52; iter: 0; batch classifier loss: 0.142684; batch adversarial loss: 0.602231\n",
      "epoch 53; iter: 0; batch classifier loss: 0.261146; batch adversarial loss: 0.620853\n",
      "epoch 54; iter: 0; batch classifier loss: 0.087910; batch adversarial loss: 0.588546\n",
      "epoch 55; iter: 0; batch classifier loss: 0.136034; batch adversarial loss: 0.610332\n",
      "epoch 56; iter: 0; batch classifier loss: 0.263334; batch adversarial loss: 0.601512\n",
      "epoch 57; iter: 0; batch classifier loss: 0.163086; batch adversarial loss: 0.566991\n",
      "epoch 58; iter: 0; batch classifier loss: 0.109366; batch adversarial loss: 0.588544\n",
      "epoch 59; iter: 0; batch classifier loss: 0.153039; batch adversarial loss: 0.542834\n",
      "epoch 60; iter: 0; batch classifier loss: 0.275765; batch adversarial loss: 0.534718\n",
      "epoch 61; iter: 0; batch classifier loss: 0.201864; batch adversarial loss: 0.529348\n",
      "epoch 62; iter: 0; batch classifier loss: 0.257875; batch adversarial loss: 0.580084\n",
      "epoch 63; iter: 0; batch classifier loss: 0.193594; batch adversarial loss: 0.587393\n",
      "epoch 64; iter: 0; batch classifier loss: 0.194754; batch adversarial loss: 0.566147\n",
      "epoch 65; iter: 0; batch classifier loss: 0.180067; batch adversarial loss: 0.489584\n",
      "epoch 66; iter: 0; batch classifier loss: 0.293475; batch adversarial loss: 0.556994\n",
      "epoch 67; iter: 0; batch classifier loss: 0.166116; batch adversarial loss: 0.589290\n",
      "epoch 68; iter: 0; batch classifier loss: 0.200187; batch adversarial loss: 0.587922\n",
      "epoch 69; iter: 0; batch classifier loss: 0.183216; batch adversarial loss: 0.556836\n",
      "epoch 70; iter: 0; batch classifier loss: 0.122732; batch adversarial loss: 0.514135\n",
      "epoch 71; iter: 0; batch classifier loss: 0.107651; batch adversarial loss: 0.573890\n",
      "epoch 72; iter: 0; batch classifier loss: 0.227021; batch adversarial loss: 0.580130\n",
      "epoch 73; iter: 0; batch classifier loss: 0.184286; batch adversarial loss: 0.573151\n",
      "epoch 74; iter: 0; batch classifier loss: 0.255164; batch adversarial loss: 0.583277\n",
      "epoch 75; iter: 0; batch classifier loss: 0.106421; batch adversarial loss: 0.556213\n",
      "epoch 76; iter: 0; batch classifier loss: 0.103707; batch adversarial loss: 0.564389\n",
      "epoch 77; iter: 0; batch classifier loss: 0.232817; batch adversarial loss: 0.591144\n",
      "epoch 78; iter: 0; batch classifier loss: 0.330874; batch adversarial loss: 0.560171\n",
      "epoch 79; iter: 0; batch classifier loss: 0.267809; batch adversarial loss: 0.561884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\datasets\\standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.565150; batch adversarial loss: 0.700943\n",
      "epoch 1; iter: 0; batch classifier loss: 0.296341; batch adversarial loss: 0.736336\n",
      "epoch 2; iter: 0; batch classifier loss: 0.233607; batch adversarial loss: 0.731639\n",
      "epoch 3; iter: 0; batch classifier loss: 0.183365; batch adversarial loss: 0.712331\n",
      "epoch 4; iter: 0; batch classifier loss: 0.228031; batch adversarial loss: 0.716811\n",
      "epoch 5; iter: 0; batch classifier loss: 0.285345; batch adversarial loss: 0.712790\n",
      "epoch 6; iter: 0; batch classifier loss: 0.205463; batch adversarial loss: 0.708050\n",
      "epoch 7; iter: 0; batch classifier loss: 0.050259; batch adversarial loss: 0.686458\n",
      "epoch 8; iter: 0; batch classifier loss: 0.116392; batch adversarial loss: 0.709288\n",
      "epoch 9; iter: 0; batch classifier loss: 0.186821; batch adversarial loss: 0.720534\n",
      "epoch 10; iter: 0; batch classifier loss: 0.141454; batch adversarial loss: 0.700822\n",
      "epoch 11; iter: 0; batch classifier loss: 0.086308; batch adversarial loss: 0.700690\n",
      "epoch 12; iter: 0; batch classifier loss: 0.167289; batch adversarial loss: 0.712321\n",
      "epoch 13; iter: 0; batch classifier loss: 0.177222; batch adversarial loss: 0.729716\n",
      "epoch 14; iter: 0; batch classifier loss: 0.185042; batch adversarial loss: 0.704441\n",
      "epoch 15; iter: 0; batch classifier loss: 0.177293; batch adversarial loss: 0.697049\n",
      "epoch 16; iter: 0; batch classifier loss: 0.082437; batch adversarial loss: 0.686148\n",
      "epoch 17; iter: 0; batch classifier loss: 0.167861; batch adversarial loss: 0.717599\n",
      "epoch 18; iter: 0; batch classifier loss: 0.163079; batch adversarial loss: 0.699500\n",
      "epoch 19; iter: 0; batch classifier loss: 0.269181; batch adversarial loss: 0.732158\n",
      "epoch 20; iter: 0; batch classifier loss: 0.119666; batch adversarial loss: 0.702389\n",
      "epoch 21; iter: 0; batch classifier loss: 0.079571; batch adversarial loss: 0.695991\n",
      "epoch 22; iter: 0; batch classifier loss: 0.120594; batch adversarial loss: 0.701924\n",
      "epoch 23; iter: 0; batch classifier loss: 0.033315; batch adversarial loss: 0.693328\n",
      "epoch 24; iter: 0; batch classifier loss: 0.064649; batch adversarial loss: 0.699232\n",
      "epoch 25; iter: 0; batch classifier loss: 0.086026; batch adversarial loss: 0.705031\n",
      "epoch 26; iter: 0; batch classifier loss: 0.191144; batch adversarial loss: 0.700602\n",
      "epoch 27; iter: 0; batch classifier loss: 0.093059; batch adversarial loss: 0.686778\n",
      "epoch 28; iter: 0; batch classifier loss: 0.135522; batch adversarial loss: 0.689528\n",
      "epoch 29; iter: 0; batch classifier loss: 0.205341; batch adversarial loss: 0.708441\n",
      "epoch 30; iter: 0; batch classifier loss: 0.082720; batch adversarial loss: 0.695285\n",
      "epoch 31; iter: 0; batch classifier loss: 0.104482; batch adversarial loss: 0.687050\n",
      "epoch 32; iter: 0; batch classifier loss: 0.076702; batch adversarial loss: 0.690341\n",
      "epoch 33; iter: 0; batch classifier loss: 0.166995; batch adversarial loss: 0.694163\n",
      "epoch 34; iter: 0; batch classifier loss: 0.055983; batch adversarial loss: 0.698203\n",
      "epoch 35; iter: 0; batch classifier loss: 0.062353; batch adversarial loss: 0.690275\n",
      "epoch 36; iter: 0; batch classifier loss: 0.056377; batch adversarial loss: 0.697642\n",
      "epoch 37; iter: 0; batch classifier loss: 0.156403; batch adversarial loss: 0.696137\n",
      "epoch 38; iter: 0; batch classifier loss: 0.129127; batch adversarial loss: 0.692371\n",
      "epoch 39; iter: 0; batch classifier loss: 0.112278; batch adversarial loss: 0.706013\n",
      "epoch 40; iter: 0; batch classifier loss: 0.129050; batch adversarial loss: 0.699272\n",
      "epoch 41; iter: 0; batch classifier loss: 0.150210; batch adversarial loss: 0.702985\n",
      "epoch 42; iter: 0; batch classifier loss: 0.033275; batch adversarial loss: 0.693212\n",
      "epoch 43; iter: 0; batch classifier loss: 0.151318; batch adversarial loss: 0.683007\n",
      "epoch 44; iter: 0; batch classifier loss: 0.083673; batch adversarial loss: 0.693865\n",
      "epoch 45; iter: 0; batch classifier loss: 0.283869; batch adversarial loss: 0.708579\n",
      "epoch 46; iter: 0; batch classifier loss: 0.090891; batch adversarial loss: 0.694203\n",
      "epoch 47; iter: 0; batch classifier loss: 0.192491; batch adversarial loss: 0.690840\n",
      "epoch 48; iter: 0; batch classifier loss: 0.198289; batch adversarial loss: 0.702550\n",
      "epoch 49; iter: 0; batch classifier loss: 0.100111; batch adversarial loss: 0.694245\n",
      "epoch 50; iter: 0; batch classifier loss: 0.112297; batch adversarial loss: 0.697197\n",
      "epoch 51; iter: 0; batch classifier loss: 0.146177; batch adversarial loss: 0.695585\n",
      "epoch 52; iter: 0; batch classifier loss: 0.174529; batch adversarial loss: 0.687145\n",
      "epoch 53; iter: 0; batch classifier loss: 0.142130; batch adversarial loss: 0.700021\n",
      "epoch 54; iter: 0; batch classifier loss: 0.089308; batch adversarial loss: 0.694348\n",
      "epoch 55; iter: 0; batch classifier loss: 0.056585; batch adversarial loss: 0.696167\n",
      "epoch 56; iter: 0; batch classifier loss: 0.106912; batch adversarial loss: 0.691927\n",
      "epoch 57; iter: 0; batch classifier loss: 0.109898; batch adversarial loss: 0.697178\n",
      "epoch 58; iter: 0; batch classifier loss: 0.082645; batch adversarial loss: 0.693669\n",
      "epoch 59; iter: 0; batch classifier loss: 0.109601; batch adversarial loss: 0.691656\n",
      "epoch 60; iter: 0; batch classifier loss: 0.112618; batch adversarial loss: 0.691815\n",
      "epoch 61; iter: 0; batch classifier loss: 0.180129; batch adversarial loss: 0.687540\n",
      "epoch 62; iter: 0; batch classifier loss: 0.116072; batch adversarial loss: 0.691694\n",
      "epoch 63; iter: 0; batch classifier loss: 0.075368; batch adversarial loss: 0.697752\n",
      "epoch 64; iter: 0; batch classifier loss: 0.111287; batch adversarial loss: 0.695901\n",
      "epoch 65; iter: 0; batch classifier loss: 0.099859; batch adversarial loss: 0.695939\n",
      "epoch 66; iter: 0; batch classifier loss: 0.144895; batch adversarial loss: 0.700463\n",
      "epoch 67; iter: 0; batch classifier loss: 0.055161; batch adversarial loss: 0.695074\n",
      "epoch 68; iter: 0; batch classifier loss: 0.165176; batch adversarial loss: 0.703412\n",
      "epoch 69; iter: 0; batch classifier loss: 0.121540; batch adversarial loss: 0.691875\n",
      "epoch 70; iter: 0; batch classifier loss: 0.105710; batch adversarial loss: 0.695141\n",
      "epoch 71; iter: 0; batch classifier loss: 0.173667; batch adversarial loss: 0.692436\n",
      "epoch 72; iter: 0; batch classifier loss: 0.131171; batch adversarial loss: 0.696550\n",
      "epoch 73; iter: 0; batch classifier loss: 0.182538; batch adversarial loss: 0.699794\n",
      "epoch 74; iter: 0; batch classifier loss: 0.081519; batch adversarial loss: 0.690369\n",
      "epoch 75; iter: 0; batch classifier loss: 0.167698; batch adversarial loss: 0.695061\n",
      "epoch 76; iter: 0; batch classifier loss: 0.080809; batch adversarial loss: 0.696446\n",
      "epoch 77; iter: 0; batch classifier loss: 0.079490; batch adversarial loss: 0.694156\n",
      "epoch 78; iter: 0; batch classifier loss: 0.082447; batch adversarial loss: 0.692465\n",
      "epoch 79; iter: 0; batch classifier loss: 0.048460; batch adversarial loss: 0.691380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.762041; batch adversarial loss: 0.714410\n",
      "epoch 1; iter: 0; batch classifier loss: 0.378644; batch adversarial loss: 0.680456\n",
      "epoch 2; iter: 0; batch classifier loss: 0.296924; batch adversarial loss: 0.680615\n",
      "epoch 3; iter: 0; batch classifier loss: 0.299953; batch adversarial loss: 0.690147\n",
      "epoch 4; iter: 0; batch classifier loss: 0.324131; batch adversarial loss: 0.694380\n",
      "epoch 5; iter: 0; batch classifier loss: 0.261154; batch adversarial loss: 0.684049\n",
      "epoch 6; iter: 0; batch classifier loss: 0.305467; batch adversarial loss: 0.698037\n",
      "epoch 7; iter: 0; batch classifier loss: 0.271237; batch adversarial loss: 0.680803\n",
      "epoch 8; iter: 0; batch classifier loss: 0.254618; batch adversarial loss: 0.674444\n",
      "epoch 9; iter: 0; batch classifier loss: 0.112344; batch adversarial loss: 0.687929\n",
      "epoch 10; iter: 0; batch classifier loss: 0.202310; batch adversarial loss: 0.687931\n",
      "epoch 11; iter: 0; batch classifier loss: 0.190984; batch adversarial loss: 0.689137\n",
      "epoch 12; iter: 0; batch classifier loss: 0.101849; batch adversarial loss: 0.685237\n",
      "epoch 13; iter: 0; batch classifier loss: 0.147523; batch adversarial loss: 0.687259\n",
      "epoch 14; iter: 0; batch classifier loss: 0.184027; batch adversarial loss: 0.692629\n",
      "epoch 15; iter: 0; batch classifier loss: 0.112919; batch adversarial loss: 0.692538\n",
      "epoch 16; iter: 0; batch classifier loss: 0.186683; batch adversarial loss: 0.688962\n",
      "epoch 17; iter: 0; batch classifier loss: 0.184586; batch adversarial loss: 0.689348\n",
      "epoch 18; iter: 0; batch classifier loss: 0.062347; batch adversarial loss: 0.690759\n",
      "epoch 19; iter: 0; batch classifier loss: 0.127298; batch adversarial loss: 0.691354\n",
      "epoch 20; iter: 0; batch classifier loss: 0.207683; batch adversarial loss: 0.689819\n",
      "epoch 21; iter: 0; batch classifier loss: 0.088998; batch adversarial loss: 0.691692\n",
      "epoch 22; iter: 0; batch classifier loss: 0.105770; batch adversarial loss: 0.691713\n",
      "epoch 23; iter: 0; batch classifier loss: 0.171886; batch adversarial loss: 0.691723\n",
      "epoch 24; iter: 0; batch classifier loss: 0.081693; batch adversarial loss: 0.692394\n",
      "epoch 25; iter: 0; batch classifier loss: 0.073821; batch adversarial loss: 0.692529\n",
      "epoch 26; iter: 0; batch classifier loss: 0.121476; batch adversarial loss: 0.692243\n",
      "epoch 27; iter: 0; batch classifier loss: 0.052686; batch adversarial loss: 0.692216\n",
      "epoch 28; iter: 0; batch classifier loss: 0.131551; batch adversarial loss: 0.692908\n",
      "epoch 29; iter: 0; batch classifier loss: 0.085057; batch adversarial loss: 0.692876\n",
      "epoch 30; iter: 0; batch classifier loss: 0.065925; batch adversarial loss: 0.692157\n",
      "epoch 31; iter: 0; batch classifier loss: 0.107416; batch adversarial loss: 0.692562\n",
      "epoch 32; iter: 0; batch classifier loss: 0.060408; batch adversarial loss: 0.692731\n",
      "epoch 33; iter: 0; batch classifier loss: 0.099784; batch adversarial loss: 0.693060\n",
      "epoch 34; iter: 0; batch classifier loss: 0.110453; batch adversarial loss: 0.692739\n",
      "epoch 35; iter: 0; batch classifier loss: 0.076353; batch adversarial loss: 0.692957\n",
      "epoch 36; iter: 0; batch classifier loss: 0.052584; batch adversarial loss: 0.692478\n",
      "epoch 37; iter: 0; batch classifier loss: 0.107790; batch adversarial loss: 0.692631\n",
      "epoch 38; iter: 0; batch classifier loss: 0.111315; batch adversarial loss: 0.692563\n",
      "epoch 39; iter: 0; batch classifier loss: 0.115665; batch adversarial loss: 0.691739\n",
      "epoch 40; iter: 0; batch classifier loss: 0.165034; batch adversarial loss: 0.691868\n",
      "epoch 41; iter: 0; batch classifier loss: 0.134080; batch adversarial loss: 0.693300\n",
      "epoch 42; iter: 0; batch classifier loss: 0.031825; batch adversarial loss: 0.692935\n",
      "epoch 43; iter: 0; batch classifier loss: 0.029992; batch adversarial loss: 0.693330\n",
      "epoch 44; iter: 0; batch classifier loss: 0.204771; batch adversarial loss: 0.692776\n",
      "epoch 45; iter: 0; batch classifier loss: 0.086289; batch adversarial loss: 0.693094\n",
      "epoch 46; iter: 0; batch classifier loss: 0.061149; batch adversarial loss: 0.693683\n",
      "epoch 47; iter: 0; batch classifier loss: 0.129365; batch adversarial loss: 0.692497\n",
      "epoch 48; iter: 0; batch classifier loss: 0.262500; batch adversarial loss: 0.690072\n",
      "epoch 49; iter: 0; batch classifier loss: 0.031033; batch adversarial loss: 0.691241\n",
      "epoch 50; iter: 0; batch classifier loss: 0.118487; batch adversarial loss: 0.692406\n",
      "epoch 51; iter: 0; batch classifier loss: 0.148944; batch adversarial loss: 0.694524\n",
      "epoch 52; iter: 0; batch classifier loss: 0.163060; batch adversarial loss: 0.693395\n",
      "epoch 53; iter: 0; batch classifier loss: 0.105443; batch adversarial loss: 0.692919\n",
      "epoch 54; iter: 0; batch classifier loss: 0.079359; batch adversarial loss: 0.693256\n",
      "epoch 55; iter: 0; batch classifier loss: 0.111077; batch adversarial loss: 0.693088\n",
      "epoch 56; iter: 0; batch classifier loss: 0.133183; batch adversarial loss: 0.693361\n",
      "epoch 57; iter: 0; batch classifier loss: 0.117740; batch adversarial loss: 0.693506\n",
      "epoch 58; iter: 0; batch classifier loss: 0.079160; batch adversarial loss: 0.692391\n",
      "epoch 59; iter: 0; batch classifier loss: 0.137139; batch adversarial loss: 0.691668\n",
      "epoch 60; iter: 0; batch classifier loss: 0.099553; batch adversarial loss: 0.692149\n",
      "epoch 61; iter: 0; batch classifier loss: 0.091341; batch adversarial loss: 0.692605\n",
      "epoch 62; iter: 0; batch classifier loss: 0.054296; batch adversarial loss: 0.693501\n",
      "epoch 63; iter: 0; batch classifier loss: 0.153020; batch adversarial loss: 0.691597\n",
      "epoch 64; iter: 0; batch classifier loss: 0.026015; batch adversarial loss: 0.691747\n",
      "epoch 65; iter: 0; batch classifier loss: 0.159540; batch adversarial loss: 0.690791\n",
      "epoch 66; iter: 0; batch classifier loss: 0.184136; batch adversarial loss: 0.694640\n",
      "epoch 67; iter: 0; batch classifier loss: 0.151742; batch adversarial loss: 0.692193\n",
      "epoch 68; iter: 0; batch classifier loss: 0.166744; batch adversarial loss: 0.691437\n",
      "epoch 69; iter: 0; batch classifier loss: 0.029438; batch adversarial loss: 0.693210\n",
      "epoch 70; iter: 0; batch classifier loss: 0.026891; batch adversarial loss: 0.693325\n",
      "epoch 71; iter: 0; batch classifier loss: 0.189010; batch adversarial loss: 0.693212\n",
      "epoch 72; iter: 0; batch classifier loss: 0.054620; batch adversarial loss: 0.694369\n",
      "epoch 73; iter: 0; batch classifier loss: 0.142674; batch adversarial loss: 0.693817\n",
      "epoch 74; iter: 0; batch classifier loss: 0.120495; batch adversarial loss: 0.693000\n",
      "epoch 75; iter: 0; batch classifier loss: 0.057314; batch adversarial loss: 0.693150\n",
      "epoch 76; iter: 0; batch classifier loss: 0.105746; batch adversarial loss: 0.692384\n",
      "epoch 77; iter: 0; batch classifier loss: 0.164835; batch adversarial loss: 0.691810\n",
      "epoch 78; iter: 0; batch classifier loss: 0.076707; batch adversarial loss: 0.691278\n",
      "epoch 79; iter: 0; batch classifier loss: 0.102589; batch adversarial loss: 0.692487\n",
      "epoch 0; iter: 0; batch classifier loss: 0.546099; batch adversarial loss: 0.828056\n",
      "epoch 1; iter: 0; batch classifier loss: 0.301048; batch adversarial loss: 0.839085\n",
      "epoch 2; iter: 0; batch classifier loss: 0.250187; batch adversarial loss: 0.809900\n",
      "epoch 3; iter: 0; batch classifier loss: 0.134764; batch adversarial loss: 0.860322\n",
      "epoch 4; iter: 0; batch classifier loss: 0.120296; batch adversarial loss: 0.850056\n",
      "epoch 5; iter: 0; batch classifier loss: 0.137429; batch adversarial loss: 0.750039\n",
      "epoch 6; iter: 0; batch classifier loss: 0.186987; batch adversarial loss: 0.775094\n",
      "epoch 7; iter: 0; batch classifier loss: 0.070394; batch adversarial loss: 0.859214\n",
      "epoch 8; iter: 0; batch classifier loss: 0.099581; batch adversarial loss: 0.796497\n",
      "epoch 9; iter: 0; batch classifier loss: 0.199214; batch adversarial loss: 0.749236\n",
      "epoch 10; iter: 0; batch classifier loss: 0.118121; batch adversarial loss: 0.718291\n",
      "epoch 11; iter: 0; batch classifier loss: 0.131697; batch adversarial loss: 0.751610\n",
      "epoch 12; iter: 0; batch classifier loss: 0.085851; batch adversarial loss: 0.740220\n",
      "epoch 13; iter: 0; batch classifier loss: 0.277010; batch adversarial loss: 0.714580\n",
      "epoch 14; iter: 0; batch classifier loss: 0.132398; batch adversarial loss: 0.702520\n",
      "epoch 15; iter: 0; batch classifier loss: 0.063751; batch adversarial loss: 0.704646\n",
      "epoch 16; iter: 0; batch classifier loss: 0.071654; batch adversarial loss: 0.688832\n",
      "epoch 17; iter: 0; batch classifier loss: 0.070150; batch adversarial loss: 0.700149\n",
      "epoch 18; iter: 0; batch classifier loss: 0.037631; batch adversarial loss: 0.704437\n",
      "epoch 19; iter: 0; batch classifier loss: 0.130753; batch adversarial loss: 0.707638\n",
      "epoch 20; iter: 0; batch classifier loss: 0.105114; batch adversarial loss: 0.683079\n",
      "epoch 21; iter: 0; batch classifier loss: 0.089575; batch adversarial loss: 0.693900\n",
      "epoch 22; iter: 0; batch classifier loss: 0.116677; batch adversarial loss: 0.701962\n",
      "epoch 23; iter: 0; batch classifier loss: 0.125940; batch adversarial loss: 0.691087\n",
      "epoch 24; iter: 0; batch classifier loss: 0.033388; batch adversarial loss: 0.693381\n",
      "epoch 25; iter: 0; batch classifier loss: 0.136796; batch adversarial loss: 0.686280\n",
      "epoch 26; iter: 0; batch classifier loss: 0.057966; batch adversarial loss: 0.692883\n",
      "epoch 27; iter: 0; batch classifier loss: 0.114995; batch adversarial loss: 0.704014\n",
      "epoch 28; iter: 0; batch classifier loss: 0.092962; batch adversarial loss: 0.699593\n",
      "epoch 29; iter: 0; batch classifier loss: 0.051525; batch adversarial loss: 0.686530\n",
      "epoch 30; iter: 0; batch classifier loss: 0.093039; batch adversarial loss: 0.689718\n",
      "epoch 31; iter: 0; batch classifier loss: 0.072680; batch adversarial loss: 0.694985\n",
      "epoch 32; iter: 0; batch classifier loss: 0.125689; batch adversarial loss: 0.693594\n",
      "epoch 33; iter: 0; batch classifier loss: 0.134823; batch adversarial loss: 0.697289\n",
      "epoch 34; iter: 0; batch classifier loss: 0.080397; batch adversarial loss: 0.695615\n",
      "epoch 35; iter: 0; batch classifier loss: 0.116972; batch adversarial loss: 0.693970\n",
      "epoch 36; iter: 0; batch classifier loss: 0.200258; batch adversarial loss: 0.689831\n",
      "epoch 37; iter: 0; batch classifier loss: 0.122234; batch adversarial loss: 0.694952\n",
      "epoch 38; iter: 0; batch classifier loss: 0.227393; batch adversarial loss: 0.694687\n",
      "epoch 39; iter: 0; batch classifier loss: 0.099054; batch adversarial loss: 0.693513\n",
      "epoch 40; iter: 0; batch classifier loss: 0.059958; batch adversarial loss: 0.694998\n",
      "epoch 41; iter: 0; batch classifier loss: 0.197920; batch adversarial loss: 0.694703\n",
      "epoch 42; iter: 0; batch classifier loss: 0.223521; batch adversarial loss: 0.696431\n",
      "epoch 43; iter: 0; batch classifier loss: 0.161893; batch adversarial loss: 0.692880\n",
      "epoch 44; iter: 0; batch classifier loss: 0.055708; batch adversarial loss: 0.692477\n",
      "epoch 45; iter: 0; batch classifier loss: 0.054477; batch adversarial loss: 0.695126\n",
      "epoch 46; iter: 0; batch classifier loss: 0.198906; batch adversarial loss: 0.690148\n",
      "epoch 47; iter: 0; batch classifier loss: 0.179285; batch adversarial loss: 0.697776\n",
      "epoch 48; iter: 0; batch classifier loss: 0.060931; batch adversarial loss: 0.694308\n",
      "epoch 49; iter: 0; batch classifier loss: 0.175025; batch adversarial loss: 0.694166\n",
      "epoch 50; iter: 0; batch classifier loss: 0.087267; batch adversarial loss: 0.693860\n",
      "epoch 51; iter: 0; batch classifier loss: 0.201520; batch adversarial loss: 0.696175\n",
      "epoch 52; iter: 0; batch classifier loss: 0.170263; batch adversarial loss: 0.693735\n",
      "epoch 53; iter: 0; batch classifier loss: 0.304508; batch adversarial loss: 0.696943\n",
      "epoch 54; iter: 0; batch classifier loss: 0.073808; batch adversarial loss: 0.693007\n",
      "epoch 55; iter: 0; batch classifier loss: 0.131008; batch adversarial loss: 0.694353\n",
      "epoch 56; iter: 0; batch classifier loss: 0.101416; batch adversarial loss: 0.690482\n",
      "epoch 57; iter: 0; batch classifier loss: 0.098591; batch adversarial loss: 0.693972\n",
      "epoch 58; iter: 0; batch classifier loss: 0.144667; batch adversarial loss: 0.692630\n",
      "epoch 59; iter: 0; batch classifier loss: 0.112531; batch adversarial loss: 0.691111\n",
      "epoch 60; iter: 0; batch classifier loss: 0.129249; batch adversarial loss: 0.694683\n",
      "epoch 61; iter: 0; batch classifier loss: 0.194956; batch adversarial loss: 0.694529\n",
      "epoch 62; iter: 0; batch classifier loss: 0.073511; batch adversarial loss: 0.692759\n",
      "epoch 63; iter: 0; batch classifier loss: 0.091944; batch adversarial loss: 0.692168\n",
      "epoch 64; iter: 0; batch classifier loss: 0.078468; batch adversarial loss: 0.691823\n",
      "epoch 65; iter: 0; batch classifier loss: 0.227631; batch adversarial loss: 0.695218\n",
      "epoch 66; iter: 0; batch classifier loss: 0.109436; batch adversarial loss: 0.694620\n",
      "epoch 67; iter: 0; batch classifier loss: 0.152977; batch adversarial loss: 0.692665\n",
      "epoch 68; iter: 0; batch classifier loss: 0.133766; batch adversarial loss: 0.691818\n",
      "epoch 69; iter: 0; batch classifier loss: 0.134535; batch adversarial loss: 0.694262\n",
      "epoch 70; iter: 0; batch classifier loss: 0.103224; batch adversarial loss: 0.694432\n",
      "epoch 71; iter: 0; batch classifier loss: 0.049023; batch adversarial loss: 0.693401\n",
      "epoch 72; iter: 0; batch classifier loss: 0.129162; batch adversarial loss: 0.693157\n",
      "epoch 73; iter: 0; batch classifier loss: 0.049017; batch adversarial loss: 0.694271\n",
      "epoch 74; iter: 0; batch classifier loss: 0.077371; batch adversarial loss: 0.693245\n",
      "epoch 75; iter: 0; batch classifier loss: 0.167999; batch adversarial loss: 0.692963\n",
      "epoch 76; iter: 0; batch classifier loss: 0.082316; batch adversarial loss: 0.694275\n",
      "epoch 77; iter: 0; batch classifier loss: 0.080308; batch adversarial loss: 0.694090\n",
      "epoch 78; iter: 0; batch classifier loss: 0.150613; batch adversarial loss: 0.692961\n",
      "epoch 79; iter: 0; batch classifier loss: 0.122356; batch adversarial loss: 0.693792\n",
      "epoch 0; iter: 0; batch classifier loss: 0.724440; batch adversarial loss: 0.697869\n",
      "epoch 1; iter: 0; batch classifier loss: 0.356001; batch adversarial loss: 0.674626\n",
      "epoch 2; iter: 0; batch classifier loss: 0.242501; batch adversarial loss: 0.700446\n",
      "epoch 3; iter: 0; batch classifier loss: 0.208793; batch adversarial loss: 0.690971\n",
      "epoch 4; iter: 0; batch classifier loss: 0.263766; batch adversarial loss: 0.675450\n",
      "epoch 5; iter: 0; batch classifier loss: 0.212627; batch adversarial loss: 0.687076\n",
      "epoch 6; iter: 0; batch classifier loss: 0.151581; batch adversarial loss: 0.684193\n",
      "epoch 7; iter: 0; batch classifier loss: 0.201130; batch adversarial loss: 0.688444\n",
      "epoch 8; iter: 0; batch classifier loss: 0.155538; batch adversarial loss: 0.684622\n",
      "epoch 9; iter: 0; batch classifier loss: 0.181694; batch adversarial loss: 0.693918\n",
      "epoch 10; iter: 0; batch classifier loss: 0.162721; batch adversarial loss: 0.693094\n",
      "epoch 11; iter: 0; batch classifier loss: 0.093657; batch adversarial loss: 0.688769\n",
      "epoch 12; iter: 0; batch classifier loss: 0.286893; batch adversarial loss: 0.691190\n",
      "epoch 13; iter: 0; batch classifier loss: 0.143033; batch adversarial loss: 0.687931\n",
      "epoch 14; iter: 0; batch classifier loss: 0.199871; batch adversarial loss: 0.687290\n",
      "epoch 15; iter: 0; batch classifier loss: 0.139728; batch adversarial loss: 0.690357\n",
      "epoch 16; iter: 0; batch classifier loss: 0.154777; batch adversarial loss: 0.693114\n",
      "epoch 17; iter: 0; batch classifier loss: 0.166513; batch adversarial loss: 0.691281\n",
      "epoch 18; iter: 0; batch classifier loss: 0.209403; batch adversarial loss: 0.690693\n",
      "epoch 19; iter: 0; batch classifier loss: 0.158458; batch adversarial loss: 0.691089\n",
      "epoch 20; iter: 0; batch classifier loss: 0.134042; batch adversarial loss: 0.693277\n",
      "epoch 21; iter: 0; batch classifier loss: 0.232615; batch adversarial loss: 0.690402\n",
      "epoch 22; iter: 0; batch classifier loss: 0.138252; batch adversarial loss: 0.691019\n",
      "epoch 23; iter: 0; batch classifier loss: 0.111574; batch adversarial loss: 0.691440\n",
      "epoch 24; iter: 0; batch classifier loss: 0.242313; batch adversarial loss: 0.691381\n",
      "epoch 25; iter: 0; batch classifier loss: 0.152808; batch adversarial loss: 0.690889\n",
      "epoch 26; iter: 0; batch classifier loss: 0.158160; batch adversarial loss: 0.686185\n",
      "epoch 27; iter: 0; batch classifier loss: 0.131080; batch adversarial loss: 0.691899\n",
      "epoch 28; iter: 0; batch classifier loss: 0.129651; batch adversarial loss: 0.691429\n",
      "epoch 29; iter: 0; batch classifier loss: 0.099212; batch adversarial loss: 0.693024\n",
      "epoch 30; iter: 0; batch classifier loss: 0.107991; batch adversarial loss: 0.687147\n",
      "epoch 31; iter: 0; batch classifier loss: 0.114306; batch adversarial loss: 0.689789\n",
      "epoch 32; iter: 0; batch classifier loss: 0.125992; batch adversarial loss: 0.691272\n",
      "epoch 33; iter: 0; batch classifier loss: 0.067285; batch adversarial loss: 0.689270\n",
      "epoch 34; iter: 0; batch classifier loss: 0.054604; batch adversarial loss: 0.693406\n",
      "epoch 35; iter: 0; batch classifier loss: 0.101520; batch adversarial loss: 0.690058\n",
      "epoch 36; iter: 0; batch classifier loss: 0.093218; batch adversarial loss: 0.688895\n",
      "epoch 37; iter: 0; batch classifier loss: 0.120663; batch adversarial loss: 0.690980\n",
      "epoch 38; iter: 0; batch classifier loss: 0.075411; batch adversarial loss: 0.687807\n",
      "epoch 39; iter: 0; batch classifier loss: 0.116944; batch adversarial loss: 0.687392\n",
      "epoch 40; iter: 0; batch classifier loss: 0.076490; batch adversarial loss: 0.686029\n",
      "epoch 41; iter: 0; batch classifier loss: 0.124348; batch adversarial loss: 0.684391\n",
      "epoch 42; iter: 0; batch classifier loss: 0.131465; batch adversarial loss: 0.685796\n",
      "epoch 43; iter: 0; batch classifier loss: 0.131177; batch adversarial loss: 0.690271\n",
      "epoch 44; iter: 0; batch classifier loss: 0.202869; batch adversarial loss: 0.687941\n",
      "epoch 45; iter: 0; batch classifier loss: 0.121096; batch adversarial loss: 0.688460\n",
      "epoch 46; iter: 0; batch classifier loss: 0.059798; batch adversarial loss: 0.692387\n",
      "epoch 47; iter: 0; batch classifier loss: 0.189131; batch adversarial loss: 0.685990\n",
      "epoch 48; iter: 0; batch classifier loss: 0.076468; batch adversarial loss: 0.687995\n",
      "epoch 49; iter: 0; batch classifier loss: 0.135426; batch adversarial loss: 0.688535\n",
      "epoch 50; iter: 0; batch classifier loss: 0.181998; batch adversarial loss: 0.690200\n",
      "epoch 51; iter: 0; batch classifier loss: 0.128915; batch adversarial loss: 0.687762\n",
      "epoch 52; iter: 0; batch classifier loss: 0.078664; batch adversarial loss: 0.691986\n",
      "epoch 53; iter: 0; batch classifier loss: 0.191369; batch adversarial loss: 0.689512\n",
      "epoch 54; iter: 0; batch classifier loss: 0.032037; batch adversarial loss: 0.691297\n",
      "epoch 55; iter: 0; batch classifier loss: 0.089106; batch adversarial loss: 0.692512\n",
      "epoch 56; iter: 0; batch classifier loss: 0.198786; batch adversarial loss: 0.685812\n",
      "epoch 57; iter: 0; batch classifier loss: 0.130411; batch adversarial loss: 0.694170\n",
      "epoch 58; iter: 0; batch classifier loss: 0.058686; batch adversarial loss: 0.691524\n",
      "epoch 59; iter: 0; batch classifier loss: 0.097499; batch adversarial loss: 0.690765\n",
      "epoch 60; iter: 0; batch classifier loss: 0.256592; batch adversarial loss: 0.691371\n",
      "epoch 61; iter: 0; batch classifier loss: 0.119900; batch adversarial loss: 0.688289\n",
      "epoch 62; iter: 0; batch classifier loss: 0.203922; batch adversarial loss: 0.690809\n",
      "epoch 63; iter: 0; batch classifier loss: 0.139276; batch adversarial loss: 0.690361\n",
      "epoch 64; iter: 0; batch classifier loss: 0.171344; batch adversarial loss: 0.691700\n",
      "epoch 65; iter: 0; batch classifier loss: 0.107508; batch adversarial loss: 0.690162\n",
      "epoch 66; iter: 0; batch classifier loss: 0.253165; batch adversarial loss: 0.690146\n",
      "epoch 67; iter: 0; batch classifier loss: 0.107360; batch adversarial loss: 0.692557\n",
      "epoch 68; iter: 0; batch classifier loss: 0.115208; batch adversarial loss: 0.689327\n",
      "epoch 69; iter: 0; batch classifier loss: 0.104787; batch adversarial loss: 0.688113\n",
      "epoch 70; iter: 0; batch classifier loss: 0.059331; batch adversarial loss: 0.692282\n",
      "epoch 71; iter: 0; batch classifier loss: 0.092073; batch adversarial loss: 0.690046\n",
      "epoch 72; iter: 0; batch classifier loss: 0.133367; batch adversarial loss: 0.690906\n",
      "epoch 73; iter: 0; batch classifier loss: 0.126129; batch adversarial loss: 0.692023\n",
      "epoch 74; iter: 0; batch classifier loss: 0.178036; batch adversarial loss: 0.694306\n",
      "epoch 75; iter: 0; batch classifier loss: 0.055767; batch adversarial loss: 0.692290\n",
      "epoch 76; iter: 0; batch classifier loss: 0.027000; batch adversarial loss: 0.690745\n",
      "epoch 77; iter: 0; batch classifier loss: 0.163058; batch adversarial loss: 0.692538\n",
      "epoch 78; iter: 0; batch classifier loss: 0.227826; batch adversarial loss: 0.689214\n",
      "epoch 79; iter: 0; batch classifier loss: 0.169529; batch adversarial loss: 0.689713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
      "c:\\Users\\Arturo\\miniconda3\\envs\\condaFair\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\eq_odds_postprocessing.py:190: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n"
     ]
    }
   ],
   "source": [
    "for data in datasets:\n",
    "    for nvar in nvars:\n",
    "        # Select name of the data set\n",
    "        dataset = data + nvar + 'V'\n",
    "        \n",
    "        # Univariate case \n",
    "        if nvar == '1':\n",
    "            # Arguments for the iteration\n",
    "            argumentsLoadData = {\n",
    "                'seed': seed\n",
    "            }\n",
    "            nvar = 1\n",
    "\n",
    "            # Load data\n",
    "            data_train, data_val, data_test, \\\n",
    "            sensitive_attribute, privileged_groups, \\\n",
    "            unprivileged_groups = loadDatasets[dataset](**argumentsLoadData)\n",
    "\n",
    "            for case in cases:\n",
    "                # No multistage processor\n",
    "                if case == 'ind': \n",
    "\n",
    "                    # Obtain benchmarks\n",
    "                    modelsNames, modelsTrain, modelsArgs = ObtainPrelDataSingle()\n",
    "                    modelsBenchmark = modelsNames\n",
    "\n",
    "\n",
    "                    # Initialize dicts\n",
    "                    methods = dict()\n",
    "\n",
    "                    # Range of thresholds to evaluate our models\n",
    "                    thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                    metrics_sweep = dict()\n",
    "\n",
    "                    # Store results from validation and test\n",
    "                    metrics_best_thresh_validate = dict()\n",
    "                    metrics_best_thresh_test = dict()\n",
    "\n",
    "                    # Benchmarks\n",
    "                    BenchmarkLogistic()\n",
    "                    BenchmarkXGB()\n",
    "                    \n",
    "                    # Pre processing\n",
    "                    for model in modelsNames:\n",
    "                        PreprocRW(model, do_results = True)\n",
    "                        PreprocDI(repair_level, model, do_results = True)\n",
    "                    \n",
    "                    # In processing\n",
    "                    for quality in quality_constraints_meta:\n",
    "                        InprocMeta(quality, tau = 0.8, do_results = True)\n",
    "                    InprocPI(eta = 50.0, do_results = True)\n",
    "                    \n",
    "                    tf.compat.v1.reset_default_graph()\n",
    "                    sess = tf.compat.v1.Session()\n",
    "                    InprocAdvs(do_results = True)\n",
    "                    sess.close()\n",
    "                    \n",
    "                    # Post processing\n",
    "                    for model in modelsNames:\n",
    "                        PosprocPlatt(model)\n",
    "                        PosprocEqoddsLABELS(model)\n",
    "                        for quality in quality_constraints_eqodds:\n",
    "                            PosprocEqoddsSCORES(model, quality)\n",
    "                        for key_metric in fair_metrics_optrej:\n",
    "                            PosprocReject(model, key_metric)\n",
    "\n",
    "                    # Name of the training instance\n",
    "                    file = dataset + '_' + case + '_' + str(i)\n",
    "                    \n",
    "                    # Store the results in a dictionary\n",
    "                    resultsDict[file] = dict()\n",
    "                    resultsDict[file]['methods'] = methods\n",
    "                    resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                    resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "                    # Use pickle to save the results\n",
    "                    with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                        pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                        pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                # Multistage processor\n",
    "                elif case == 'com':\n",
    "                    # Obtain benchmarks and in proncessing models\n",
    "                    modelsNames, modelsBenchmark, modelsPost, \\\n",
    "                    modelsTrain, modelsArgs = ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups)\n",
    "\n",
    "                    # Initialize dicts\n",
    "                    methods = dict()\n",
    "\n",
    "                    # Range of thresholds to evaluate our models\n",
    "                    thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                    metrics_sweep = dict()\n",
    "\n",
    "                    # Store results from validation and test\n",
    "                    metrics_best_thresh_validate = dict()\n",
    "                    metrics_best_thresh_test = dict()\n",
    "\n",
    "                    # Benchmarks\n",
    "                    BenchmarkLogistic()\n",
    "                    BenchmarkXGB()\n",
    "\n",
    "                    # Pre processing + In processing\n",
    "                    for model in modelsNames:\n",
    "                        if model == 'adversarial':\n",
    "                            tf.compat.v1.reset_default_graph()\n",
    "                            sess = tf.compat.v1.Session()\n",
    "                        PreprocRW(model, do_results = True)\n",
    "                        if model == 'adversarial':\n",
    "                            sess.close()\n",
    "                            tf.compat.v1.reset_default_graph()\n",
    "                            sess = tf.compat.v1.Session()\n",
    "                        PreprocDI(repair_level, model, do_results = True)\n",
    "                        \n",
    "                        if model == 'adversarial':\n",
    "                            sess.close()\n",
    "\n",
    "                    # Pre/In processing + Post processing\n",
    "                    for quality in quality_constraints_meta:\n",
    "                        InprocMeta(quality, tau = 0.8, do_results = True)\n",
    "                    InprocPI(eta = 50.0, do_results = True)\n",
    "\n",
    "                    tf.compat.v1.reset_default_graph()\n",
    "                    sess = tf.compat.v1.Session()\n",
    "                    InprocAdvs(do_results = True)\n",
    "\n",
    "                    for model in modelsPost:\n",
    "                        PosprocPlatt(model)\n",
    "                        PosprocEqoddsLABELS(model)\n",
    "                        for quality in quality_constraints_eqodds:\n",
    "                            PosprocEqoddsSCORES(model, quality)\n",
    "                        for key_metric in fair_metrics_optrej:\n",
    "                            PosprocReject(model, key_metric)\n",
    "                            \n",
    "                    sess.close()\n",
    "                    \n",
    "                    # Name of the training instance\n",
    "                    file = dataset + '_' + case + '_' + str(i)\n",
    "\n",
    "                    # Store the results in a dictionary\n",
    "                    resultsDict[file] = dict()\n",
    "                    resultsDict[file]['methods'] = methods\n",
    "                    resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                    resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "                    \n",
    "                    # Save them with pickle\n",
    "                    with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                        pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                        pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "        # Multivariate case\n",
    "        elif nvar == '2':\n",
    "            for operation in operations:\n",
    "                    # The arguments of the function that loads the data are now different\n",
    "                    argumentsLoadData = {\n",
    "                        'seed': seed,\n",
    "                        'operation': operation\n",
    "                    }\n",
    "                    nvar = 2\n",
    "\n",
    "                    resultsDict[dataset + '_' + operation] = dict()\n",
    "\n",
    "                    # Load the data\n",
    "                    data_train, data_val, data_test, \\\n",
    "                    sensitive_attribute, privileged_groups, unprivileged_groups, \\\n",
    "                    data_val_single, data_test_single = loadDatasets[dataset](**argumentsLoadData)\n",
    "        \n",
    "                    for case in cases:\n",
    "                        if case == 'ind': \n",
    "\n",
    "                            # Initialize dicts\n",
    "                            methods = dict()\n",
    "\n",
    "                            # Obtain benchmarks\n",
    "                            modelsNames, modelsTrain, modelsArgs = ObtainPrelDataSingle()\n",
    "                            modelsBenchmark = modelsNames\n",
    "\n",
    "                            # Range of thresholds to evaluate our models\n",
    "                            thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                            metrics_sweep = dict()\n",
    "\n",
    "                            # Store results from validation and test\n",
    "                            metrics_best_thresh_validate = dict()\n",
    "                            metrics_best_thresh_test = dict()\n",
    "\n",
    "                            # Benchmarks\n",
    "                            BenchmarkLogistic()\n",
    "                            BenchmarkXGB()\n",
    "                            \n",
    "                            # Pre processing\n",
    "                            for model in modelsNames:\n",
    "                                PreprocRW(model, do_results = True)\n",
    "                                PreprocDI(repair_level, model, do_results = True)\n",
    "                            \n",
    "                            # In processing\n",
    "                            for quality in quality_constraints_meta:\n",
    "                                InprocMeta(quality, tau = 0.8, do_results = True)\n",
    "                            InprocPI(eta = 50.0, do_results = True)\n",
    "                            \n",
    "                            tf.compat.v1.reset_default_graph()\n",
    "                            sess = tf.compat.v1.Session()\n",
    "                            InprocAdvs(do_results = True)\n",
    "                            sess.close()\n",
    "                            \n",
    "                            # Post processing\n",
    "                            for model in modelsNames:\n",
    "                                PosprocPlatt(model)\n",
    "                                PosprocEqoddsLABELS(model)\n",
    "                                for quality in quality_constraints_eqodds:\n",
    "                                    PosprocEqoddsSCORES(model, quality)\n",
    "                                for key_metric in fair_metrics_optrej:\n",
    "                                    PosprocReject(model, key_metric)\n",
    "\n",
    "                            file =  dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "\n",
    "                            # Store results in a dictionary\n",
    "                            resultsDict[file] = dict()\n",
    "                            resultsDict[file]['methods'] = methods\n",
    "                            resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                            resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "\n",
    "                            # Save results with pickle\n",
    "                            with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                                pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                            with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                                pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "                        # Multistage processors\n",
    "                        elif case == 'com':\n",
    "                            # Obtain benchmarks and in proncessing models\n",
    "                            modelsNames, modelsBenchmark, modelsPost, \\\n",
    "                            modelsTrain, modelsArgs = ObtainPrelDataMultiple(sensitive_attribute, privileged_groups, unprivileged_groups)\n",
    "\n",
    "                            # Initialize dicts\n",
    "                            methods = dict()\n",
    "\n",
    "                            # Range of thresholds to evaluate our models\n",
    "                            thresh_sweep = np.linspace(0.01, 1.0, 50)\n",
    "                            metrics_sweep = dict()\n",
    "\n",
    "                            # Store results from validation and test\n",
    "                            metrics_best_thresh_validate = dict()\n",
    "                            metrics_best_thresh_test = dict()\n",
    "\n",
    "                            # Benchmarks\n",
    "                            BenchmarkLogistic()\n",
    "                            BenchmarkXGB()\n",
    "\n",
    "                            # Pre processing + In processing\n",
    "                            for model in modelsNames:\n",
    "                                if model == 'adversarial':\n",
    "                                    tf.compat.v1.reset_default_graph()\n",
    "                                    sess = tf.compat.v1.Session()\n",
    "                                PreprocRW(model, do_results = True)\n",
    "                                if model == 'adversarial':\n",
    "                                    sess.close()\n",
    "                                    tf.compat.v1.reset_default_graph()\n",
    "                                    sess = tf.compat.v1.Session()\n",
    "                                PreprocDI(repair_level, model, do_results = True)\n",
    "                                \n",
    "                                if model == 'adversarial':\n",
    "                                    sess.close()\n",
    "\n",
    "                            # Pre/In processing + Post processing\n",
    "                            for quality in quality_constraints_meta:\n",
    "                                InprocMeta(quality, tau = 0.8, do_results = True)\n",
    "                            InprocPI(eta = 50.0, do_results = True)\n",
    "\n",
    "                            tf.compat.v1.reset_default_graph()\n",
    "                            sess = tf.compat.v1.Session()\n",
    "                            InprocAdvs(do_results = True)\n",
    "\n",
    "                            for model in modelsPost:\n",
    "                                PosprocPlatt(model)\n",
    "                                PosprocEqoddsLABELS(model)\n",
    "                                for quality in quality_constraints_eqodds:\n",
    "                                    PosprocEqoddsSCORES(model, quality)\n",
    "                                for key_metric in fair_metrics_optrej:\n",
    "                                    PosprocReject(model, key_metric)\n",
    "                                    \n",
    "                            sess.close()\n",
    "\n",
    "                            file = dataset + '_' + operation + '_' + case + '_' + str(i)\n",
    "\n",
    "                            # Store results in a dictionary                            \n",
    "                            resultsDict[file] = dict()\n",
    "                            resultsDict[file]['methods'] = methods\n",
    "                            resultsDict[file]['best_thresh_test'] = pd.DataFrame(metrics_best_thresh_test).T\n",
    "                            resultsDict[file]['metrics_sweep'] = metrics_sweep\n",
    "                        \n",
    "                            # Save results with pickle\n",
    "                            with open('results/best/' + data + '/' + file + '_best.pickle', 'wb') as handle:\n",
    "                                pickle.dump(resultsDict[file]['best_thresh_test'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                            with open('results/sweep/' + data + '/' + file + '_sweep.pickle', 'wb') as handle:\n",
    "                                pickle.dump(resultsDict[file]['metrics_sweep'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to analyze this data, which will be the goal of the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaFair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
